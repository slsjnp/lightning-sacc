Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers
Sixiao Zheng1* Jiachen Lu1 Hengshuang Zhao2 Xiatian Zhu3 Zekun Luo4 Yabiao Wang4 Yanwei Fu1 Jianfeng Feng1 Tao Xiang3, 5 Philip H.S. Torr2 Li Zhang1† 1Fudan University 2University of Oxford 3University of Surrey 4Tencent Youtu Lab 5Facebook AI https://fudan-zvg.github.io/SETR

arXiv:2012.15840v1 [cs.CV] 31 Dec 2020

Abstract
Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive ﬁelds. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive ﬁeld, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Speciﬁcally, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the ﬁrst (44.42% mIoU) position in the highly competitive ADE20K test server leaderboard.
1. Introduction
Since the seminal work of [37], existing semantic segmentation models have been dominated by those based on fully convolutional network (FCN). A standard FCN segmentation model has an encoder-decoder architecture: the encoder is for feature representation learning, while the decoder for pixel-level classiﬁcation of the feature representa-
*Work done while Sixiao Zheng was interning at Tencent Youtu Lab. †Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan University.

tions yielded by the encoder. Among the two, feature representation learning (i.e., the encoder) is arguably the most important model component [8, 29, 57, 60]. The encoder, like most other CNNs designed for image understanding, consists of stacked convolution layers. Due to concerns on computational cost, the resolution of feature maps is reduced progressively and the encoder is hence able to learn more abstract/semantic visual concepts with a gradually increased receptive ﬁeld. Such a design is popular due to two favourable merits, namely translation equivariance and locality. The former respects well the nature of imaging process [58] which underpins the model generalization ability to unseen image data. Whereas the latter controls the model complexity by sharing parameters across space. However, it also raises a fundamental limitation that learning long-range dependency information, critical for semantic segmentation in unconstrained scene images [2,50], becomes challenging due to still limited receptive ﬁelds.
To overcome this aforementioned limitation, a number of approaches have been introduced recently. One approach is to directly manipulate the convolution operation. This includes large kernel sizes [41], atrous convolutions [8, 23], and image/feature pyramids [60]. The other approach is to integrate attention modules into the FCN architecture. Such a module aims to model global interactions of all pixels in feature map [48]. When applied to semantic segmentation [26,30], a common design is to combine the attention module to the FCN architecture with attention layers sitting on the top. Taking either approach, the standard encoderdecoder FCN model architecture remains unchanged. More recently, attempts have been made to get rid of convolutions altogether and deploy attention-alone models [47] instead. However, even without convolution they do not change the nature of the FCN model structure: an encoder downsamples the spatial resolution of the input, developing lowerresolution feature mappings useful for discriminating semantic classes, and the decoder upsamples the feature rep-

1

resentations into a full-resolution segmentation map.
In this paper, we aim to provide a rethinking to the semantic segmentation model design and contribute an alternative. In particular, we propose to replace the stacked convolution layers based encoder with gradually reduced spatial resolution with a pure transformer [45], resulting in a new segmentation model termed SEgmentation TRansformer (SETR). This transformer-alone encoder treats an input image as a sequence of image patches represented by learned patch embedding, and transforms the sequence with global self-attention modeling for discriminative feature representation learning. Concretely, we ﬁrst decompose an image into a grid of ﬁxed-sized patches, forming a sequence of patches. With a linear embedding layer applied to the ﬂatten pixel vectors of every patch, we then obtain a sequence of feature embedding vectors as the input to a transformer. Given the learned features from the encoder transformer, a decoder is then used to recover the original image resolution. Crucially there is no downsampling in spatial resolution but global context modeling at every layer of the encoder transformer, thus offering a completely new perspective to the semantic segmentation problem.
This pure transformer design is inspired by its tremendous success in natural language processing [13, 14, 45, 49, 51]. More recently, a pure vision transformer or ViT [17] has shown to be effective for image classiﬁcation tasks. It thus provides direct evidence that the traditional stacked convolution layer (i.e., CNN) design can be challenged and image features do not necessarily need to be learned progressively from local to global context by reducing spatial resolution. However, extending a pure transformer from image classiﬁcation to a spatial location sensitive tasks of semantic segmentation is non-trivial. We show empirically that our SETR not only offers a new perspective in model design, but also achieves new state-of-the-art on a number of benchmarks.
The following contributions are made in this paper: (1) We reformulate the image semantic segmentation problem from a sequence-to-sequence learning perspective, offering an alternative to the dominating encoder-decoder FCN model design. (2) As an instantiation, we exploit the transformer framework to implement our fully attentive feature representation encoder by sequentializing images. (3) To extensively examine the self-attentive feature presentations, we further introduce three different decoder designs with varying complexities. Extensive experiments show that our SETR models can learn superior feature representations as compared to different FCNs with and without attention modules, yielding new state of the art on ADE20K (50.28%), Pascal Context (55.83%) and competitive results on Cityscapes. Particularly, our entry is ranked the 1st (44.42% mIoU) place in the highly competitive ADE20K test server leaderboard.

2. Related work
Semantic segmentation Semantic image segmentation has been signiﬁcantly boosted with the development of deep neural networks. By removing fully connected layers, the fully convolutional networks (FCN) [37] is able to achieve pixel-wise predictions. While the predictions of FCN are relatively coarse, several CRF/MRF [6, 36, 62] based approaches are developed to help reﬁne the coarse predictions. To address the inherent tension between semantics and location [37], coarse and ﬁne layers need to be aggregated for both the encoder and decoder. This leads to different variants of the encoder-decoder structures [2, 39, 43] for multilevel feature fusion.
Many recent efforts have been focused on addressing the limited receptive ﬁeld/context modeling problem in FCN. To enlarge the receptive ﬁeld, DeepLab [7] and Dilation [53] introduce the dilated convolution. Alternatively, context modeling is the focus of PSPNet [60] and DeepLabV2 [9]. The former proposes the PPM module to obtain different region’s contextual information while the latter develops ASPP module that adopts pyramid dilated convolutions with different dilation rates. Decomposed large kernels [41] are also utilized for context capturing. Recently, attention based models are popular for capturing long range context information. PSANet [61] develops the pointwise spatial attention module for dynamically capturing the long range context. DANet [18] embeds both spatial attention and channel attention. CCNet [27] alternatively focuses on economizing the heavy computation budget introduced by full spatial attention. DGMN [57] builds a dynamic graph message passing network for scene modeling and it can signiﬁcantly reduce the computational complexity. Note that all these approaches are still based on FCNs where the feature encoding and extraction part are based on classical ConvNets like VGG [44] and ResNet [21]. In this work, we alternatively rethink the semantic segmentation task in a different perspective.
Transformer Transformer and self-attention models have revolutionized machine translation and natural language processing [13,14,45,49,51]. Recently, there are also some explorations for the usage of transformer structures in image recognition. Non-local networks [48] appends transformer style attention onto the convolutional backbone. AANet [3] mixes convolution and self-attention for backbone training. LRNet [25] and stand-alone networks [42] explore local self-attention to avoid the heavy computation brought by global self-attention. SAN [59] explores two types of self-attention modules. Axial-Attention [47] decomposes the global spatial attention into two separate axial attentions such that the computation is largely reduced. Apart from these pure transformer based model, there are also CNN-transformer hybrid ones. DETR [5] and the fol-

2

MLP Layer Norm

MLDPecHoedaerd

Multi-Head Attention
Layer Norm
Patch Embedding
+
Position Embedding

...

Transformer Layer 24x
Transformer Layer

+

+ ... +

+

Linear Projection
...

reshape

conv→2x

conv→2x

conv→2x

conv→2x

reshape-conv Z24
Z18
Z12
Z6

(b)
conv-conv-4x

conv-4x

(a)

(c)

Figure 1. Schematic illustration of the proposed SEgmentation TRansformer (SETR) (a). We ﬁrst split an image into ﬁxed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. To perform pixel-wise segmentation, we introduce different decoder designs: (b) progressive upsampling (resulting in a variant called SETRPUP); and (c) multi-level feature aggregation (a variant called SETR-MLA).

lowing deformable version utilize transformer for object detection where transformer is appended inside the detection head. STTR [33] and LSTR [35] adopt transformer for disparity estimation and lane shape prediction respectively. Most recently, ViT [17] is the ﬁrst work to show that a pure transformer based image classiﬁcation model can achieve the state-of-the-art. It provides direct inspiration to us to exploit a pure transformer based encoder design in a semantic segmentation model.
The most related work is [47] which also leverages attention for image segmentation. However, there are several key differences. First, though convolution is completely removed in [47] as in our SETR, their model still follows the conventional FCN design in that spatial resolution of feature maps are reduced progressively. In contrast, our sequenceto-sequence prediction model keeps the same spatial resolution throughout and thus represents a step-change in model design. Second, to maximize the scalability on modern hardware accelerators and facilitate easy-to-use, we stick to the standard self-attention design. Instead [47] adopts a specially designed axial-attention [22] which is less scalable to standard computing facilities. Our model is also superior in segmentation accuracy (see Section 4).
3. Method
3.1. FCN-based semantic segmentation
In order to contrast with our new model design, let us ﬁrst revisit the conventional FCN [37] for image semantic segmentation. A FCN encoder consists of a stack of sequentially connected convolutional layers. The ﬁrst layer takes

as input the image, denoted as H ×W ×3 with H ×W specifying the image size in pixels. The input of subsequent layer i is a three-dimensional tensor sized h × w × d, where h and w are spatial dimensions of feature maps, and d is the feature/channel dimension. Locations of the tensor in a higher layer are computed based on the locations of tensors of all lower layers they are connected to via layer-by-layer convolutions, which are deﬁned as their receptive ﬁelds. Due to the locality nature of convolution operation, the receptive ﬁeld increases linearly along the depth of layers, conditional on the kernel sizes (typically 3 × 3). As a result, only higher layers with big receptive ﬁelds can model long-range dependencies in this FCN architecture. However, it is shown that the beneﬁts of adding more layers would diminish rapidly once reaching certain depths [21]. Having limited receptive ﬁelds for context modeling is thus an intrinsic limitation of the vanilla FCN architecture.
Recently, a number of state-of-the-art methods [26, 56, 57] suggest that combing FCN with attention mechanism is a more effective strategy for learning long-range contextual information. These methods limit the attention learning to higher layers with smaller input sizes alone due to its quadratic complexity w.r.t. the pixel number of feature tensors. This means that dependency learning on lower-level feature tensors are lacking, leading to sub-optimal representation learning. To overcome this limitation, we propose a pure self-attention based encoder, named as SEgmentation TRansformers (SETR).

3

3.2. Segmentation transformers (SETR)

Image to sequence SETR follows the same input-output

structure as in NLP for transformation between 1D se-

quences. There thus exists a mismatch between 2D image

and 1D sequence. Concretely, the Transformer, as depicted

in Figure 1(a), accepts a 1D sequence of feature embeddings Z ∈ RL×C as input, L is the length of sequence, C is the
hidden channel size. Image sequentialization is thus needed to convert the an input image x ∈ RH×W ×3 into Z.

A straightforward way for image sequentialization is to

ﬂatten the image pixel values into a 1D vector with size of 3HW . For a typical image sized at 480(H) × 480(W ) × 3,

the resulting vector will have a length of 691,200. Given

the quadratic model complexity of Transformer, it is not

possible that such high-dimensional vectors can be handled

in both space and time. Therefore tokenizing every single

pixel as input to our transformer is out of the question.

In view of the fact that a typical encoder designed for

semantic segmentation would downsample a 2D image x ∈

RH×W ×3 into a feature map xf

∈

R , H 16

×

W 16

×C

we

thus

decide to set the transformer input sequence length L as

H 16

×

W 16

=

HW 256

.

This way, the

output sequence

of

the trans-

former can be simply reshaped to the target feature map xf .

To

obtain

the

HW 256

-long

input

sequence,

we

divide

an

image

x

∈

RH×W ×3

into

a

grid

of

H 16

×

W 16

patches

uni-

formly, and then ﬂatten this grid into a sequence. By

further mapping each vectorized patch p into a latent C-

dimensional embedding space using a linear projection function f : p −→ e ∈ RC, we obtain a 1D sequence of patch embeddings for an image x. To encode the patch spa-
cial information, we learn a speciﬁc embedding pi for every location i which is added to ei to form the ﬁnal sequence input E = {e1 + p1, e2 + p2, · · · , eL + pL}. This way, spatial information is kept despite the orderless self-attention

nature of transformers.

Transformer Given the 1D embedding sequence E as input, a pure transformer based encoder is employed to learn feature representations. This means each transformer layer has a global receptive ﬁeld, solving the limited receptive ﬁeld problem of existing FCN encoder once and for all. The transformer encoder consists of Le layers of multi-head self-attention (MSA) and Multilayer Perceptron (MLP) blocks [46] (Figure 1(a)). At each layer l, the input to self-attention is in a triplet of (query, key, value) computed from the input Zl−1 ∈ RL×C as:
query = Zl−1WQ, key = Zl−1WK , value = Zl−1WV , (1)
where WQ/WK /WV ∈ RC×d are the learnable parameters of three linear projection layers and d is the dimension of (query, key, value). Self-attention (SA) is then formu-

lated as:
SA(Zl−1) = Zl−1 + softmax( Zl−1WQ√(ZWK ) )(Zl−1WV ). d (2)
MSA is an extension with m independent SA operations and project their concatenated outputs: M SA(Zl−1) = [SA1(Zl−1); SA2(Zl−1); · · · ; SAm(Zl−1)]WO, where WO ∈ Rmd×C . d is typically set to C/m. The output of MSA is then transformed by a MLP block with residual skip as the layer output as:
Zl = M SA(Zl−1) + M LP (M SA(Zl−1)) ∈ RL×C . (3)
Note, layer norm is applied before MSA and MLP blocks which is omitted for simplicity. We denote {Z1, Z2, · · · , ZLe } as the features of transformer layers.

3.3. Decoder designs

To evaluate the effectiveness of SETR’s encoder feature

representations Z, we introduce three different decoder de-

signs to perform pixel-level segmentation. As the goal of

the decoder is to generate the segmentation results in the

original 2D image space (H × W ), we need to reshape the

encoder’s features (that are used in decoder), Z, from a 2D

shape

of

HW 256

×C

to

a

standard

3D

feature

map

H 16

×

W 16

×C.

Next, we brieﬂy describe the three decoders.

(1) Naive upsampling (Naive) This naive decoder ﬁrst projects the transformer feature ZLe to the dimension of category number (e.g., 19 for experiments on Cityscapes). For this we adopt a simple 2-layer network with architecture: 1 × 1 conv + sync batch norm (w/ ReLU) + 1 × 1 conv. After that, we simply bilinearly upsample the output to the full image resolution, followed by a classiﬁcation layer with pixel-wise cross-entropy loss. When this decoder is used, we denote our model as SETR-Naive.

(2) Progressive UPsampling (PUP) Instead of one-step

upscaling which may introduce noisy predictions, we con-

sider a progressive upsampling strategy that alternates conv

layers and upsampling operations. To maximally mitigate

the adversarial effect, we restrict upsampling to 2×. Hence,

a total of 4 operations are needed for reaching full resolution

from

Z Le

with

size

H 16

×

W 16

.

More

details

of

this

process

are

given in Figure 1(b). When using this decoder, we denote

our model as SETR-PUP.

(3) Multi-Level feature Aggregation (MLA) The third

design is characterized by multi-level feature aggregation

(Figure 1(c)) in similar spirit of feature pyramid network

[28, 34]. However, our decoder is different fundamentally because the feature representations Zl of every SETR’s

layer share the same resolution without a pyramid shape.

Speciﬁcally, we take as input the feature representations

{Z m }

(m

∈

{

Le M

,

2

Le M

,

·

·

·

,M

Le M

})

from

M

layers

uni-

formly

distributed

across

the

layers

with

step

Le M

to

the

de-

coder. M streams are then deployed, with each focusing on

4

Method SETR-Naive SETR-MLA SETR-PUP

40k 77.37 76.65 78.39

80k 77.90 77.24 79.34

Method Semantic FPN (res-50) [40] Semantic FPN (res-101) [40] SETR-MLA

mIoU 74.52 75.80 76.65

Pre-train Random ImageNet-21k ImageNet-21k+1k

mIoU 42.27 78.39 78.34

(a) Different variants of SETR on (b) Compare to Semantic FPN [28]: (c) Pre-training on different data

various training schedules.

SETR-MLA with iteration 80k.

with SETR-PUP.

Method

40k

SETR-Hybrid (1k-random) 74.48

SETR-Hybrid (21k-random) 75.03

SETR-Hybrid (1k-21k)

76.76

SETR-Hybrid (21k-21k) 76.90

SETR-Naive-S

75.54

SETR-MLA-S

75.60

SETR-PUP-S

76.71

80k
77.36 77.57 76.57 77.61 76.25 76.87 78.02

Method
FCN [40] FCN [40] FCN SETR-Naive SETR-MLA SETR-PUP

Pre-train ImageNet-1k ImageNet-1k ImageNet-21k ImageNet-21k ImageNet-21k ImageNet-21k

Backbone ADE20K Cityscapes
ResNet-50 36.10 71.47 ResNet-101 39.91 73.93 ResNet-101 42.17 76.38
T-Large 48.18 77.37 T-Large 48.64 76.65 T-Large 48.58 78.39

(d) Compare to SETR-Hybrid and scaling (e) Compare to FCN with different pre-training with single-scale

study of different variants.

inference on the ADE20K val and Cityscapes val set.

Table 1. Ablation studies. All methods are evaluated using mean IoU (%), single scale test protocol. Unless otherwise speciﬁed, all models are trained on Cityscapes train ﬁne set with 40,000 iterations and batch size 8, and evaluated on the Cityscapes validation set.

one speciﬁc selected layer. In each stream, we ﬁrst reshape

the encoder’s feature Zl

from a 2D shape of

HW 256

×C

to a

3D

feature

map

H 16

×

W 16

×

C.

A

3-layer

(kernel

size

1

×

1,

3 × 3, and 3 × 3) network is applied with the feature chan-

nels halved at the ﬁrst and third layers respectively, and the

spatial resolution upscaled 4× by bilinear operation after

the third layer. To enhance the interactions across differ-

ent streams, we introduce a top-down aggregation design

via element-wise addition after the ﬁrst layer. An additional

3 × 3 conv is applied after element-wise additioned feature.

After the third layer we obtain the fused feature from all the

streams via channel-wise concatenation which is then bilin-

ear upsampled 4× to the full resolution. When using this

decoder, we denote our model as SETR-MLA.

4. Experiments
4.1. Experimental setup
We conduct experiments on three widely-used semantic segmentation benchmark datasets.
Cityscapes [12] densely annotates 19 object categories in images with urban scenes. It contains 5000 ﬁnely annotated images, split into 2975, 500 and 1525 for training, validation and testing respectively. The images are all captured at a high resolution of 2048 × 1024. In addition, it provides 19,998 coarse annotated images for model training.
ADE20K [63] is a challenging scene parsing benchmark with 150 ﬁne-grained semantic concepts. It contains 20210, 2000 and 3352 images for training, validation and testing.

Model

T-layers

Hidden size Att head

T-Small

12

768

12

T-Large

24

1024

16

Table 2. Conﬁguration of Transformer variants.

PASCAL Context [38] provides pixel-wise semantic labels for whole scene (both “thing” and “stuff” classes), and contains 4998 and 5105 images for training and validation respectively. Following previous works, we evaluate on the most frequent 59 classes and the background class (60 classes in total).

Implementation details Following the default setting (e.g., data augmentation and training schedule) of public codebase mmsegmentation [40], (i) we apply random resize with ratio between 0.5 and 2, random cropping (768, 512 and 480 for Cityscapes, ADE20K and Pascal Context respectively) and random horizontal ﬂipping during training for all the experiments; (ii) We set the total iteration to 160,000 and 80,000 for the experiments on ADE20K and Pascal Context, and report the performances for both case with batch size 8 and 16 respectively. For Cityscapes, we set batch size to 8 with a number of training schedule reported in Table 1, 5 and 6 for fair comparison. We adopt a polynomial learning rate decay schedule [60] and employ SGD as the optimizer. Momentum and weight decay are set to 0.9 and 0 respectively for all the experiments on the three datasets. We set initial learning rate 0.001 on ADE20K and Pascal Context, and 0.01 on Cityscapes.

Auxiliary loss As [60] we also ﬁnd the auxiliary segmentation loss helps the model training. Each auxiliary loss head follows a 2-layer network. We add

5

Figure 2. Qualitative results on ADE20K: SETR (right column) vs. dilated FCN baseline (left column) in each pair. Best viewed in color and zoom in.

Method
FCN (16, 160k, SS) [40] FCN (16, 160k, MS) [40] EncNet [55] PSPNet [60] DMNet [19] CCNet [26] Strip pooling [24] APCNet [20] OCNet [54] SETR-MLA (8, 160k, SS) SETR-MLA (8, 160k, MS) SETR-PUP (16, 160k, SS) SETR-PUP (16, 160k, MS) SETR-MLA (16, 160k, SS) SETR-MLA (16, 160k, MS)

Backbone
ResNet-101 ResNet-101 ResNet-101 ResNet-269 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101
T-Large T-Large T-Large T-Large T-Large T-Large

mIoU
39.91 41.40 44.65 44.94 45.50 45.22 45.60 45.38 45.45 48.27 50.03 48.58 50.09 48.64 50.28

Pixel Acc.
79.52 80.65 81.69 81.69
82.09 82.52 83.41 82.90 83.58 82.64 83.46

Table 3. State-of-the-art comparison on the ADE20K dataset. Performances of different model variants and batch size (e.g., 8 or 16) are reported. SS: Single-scale inference. MS: Multi-scale inference.

auxiliary losses at different Transformer layers: SETRNaive (Z10, Z15, Z20), SETR-PUP (Z10, Z15, Z20, Z24), SETR-MLA (Z6, Z12, Z18, Z24). Both auxiliary loss and main loss heads are applied concurrently.
Multi-scale test We use the default settings of mmsegmentation [40]. Speciﬁcally, the input image is ﬁrst scaled to a uniform size. Multi-scale scaling and random horizontal ﬂip are then performed on the image with scaling factor (0.5, 0.75, 1.0, 1.25, 1.5, 1.75). Sliding window is adopted for test (e.g., 480 × 480 for Pascal Context). If the shorter side is smaller than the size of the sliding window, the image is scaled with its shorter side to the size of the sliding window (e.g., 480) while keeping the aspect ratio. Synchronized BN is used in decoder and auxiliary loss heads. For training simplicity, we do not adopt the widely-used tricks such as OHEM [54] loss in model training.
Baselines We adopt dilated FCN [37] and Semantic FPN [28] as baselines with their results taken from [40]. Our models and the baselines are trained and tested in the same settings for fair comparison. In addition, state-of-theart models are also compared. Note that the dilated FCN is with output stride 8 and we use output stride 16 in all our models due to GPU memory constrain.

Figure 3. Qualitative results on Pascal Context: SETR (right column) vs. dilated FCN baseline (left column) in each pair. Best viewed in color and zoom in.

Method

Backbone

mIoU

FCN (16, 80k, SS) [40]

ResNet-101

44.47

FCN (16, 80k, MS) [40] ResNet-101

45.74

PSPNet [60]

ResNet-101

47.80

DANet [18]

ResNet-101

52.60

EMANet [32]

ResNet-101

53.10

SVCNet [15]

ResNet-101

53.20

ACNet [16]

ResNet-101

54.10

GFFNet [31]

ResNet-101

54.20

APCNet [20]

ResNet-101

54.70

SETR-MLA (8, 80k, SS)

T-Large

54.39

SETR-MLA (8, 80k, MS)

T-Large

55.39

SETR-PUP (16, 80k, SS) T-Large

54.40

SETR-PUP (16, 80k, MS) T-Large

55.27

SETR-MLA (16, 80k, SS) T-Large

54.87

SETR-MLA (16, 80k, MS) T-Large

55.83

Table 4. State-of-the-art comparison on the Pascal Context

dataset. Performances of different model variants and batch sizes

(e.g., 8 or 16) are reported. SS: Single-scale inference. MS: Multi-

scale inference.

SETR variants Three main variants of our model with different decoder designs (see Sec. 3.3), namely SETR-Naive, SETR-PUP and SETR-MLA. Besides, we form two variants of the encoder “T-Small” and “T-Large” with 12 and 24 layers respectively (see the conﬁgurations in Table 2). Unless otherwise speciﬁed, we use “T-Large” as the encoder for SETR-Naive, SETR-PUP and SETR-MLA. We denote SETR-Naive-S as the model utilizing “T-Small” in SETRNaive.
Though designed as a model with a pure transformer encoder, we also form a hybrid variant SETR-Hybrid by using a ResNet-50 based FCN encoder and feeding its output feature into SETR. To cope with the GPU memory constraint and for fair comparison, we only consider ‘T-Small” in SETR-Hybrid and set the output stride of FCN encoder to 1/16. That is, SETR-Hybrid is a combination of ResNet50 and SETR-Naive-S.
Pre-training We use the pre-trained weights provided by [17] to initialize all the transformer layers and the input linear projection layer in our model. We investigate the pre-trained weights trained on different datasets in Table 1(c). All the layers without pre-training are randomly initialized. For the FCN encoder of SETR-Hybrid, we investigate the initial weights pre-trained on ImageNet-1k or

6

Figure 4. Qualitative results on Cityscapes: SETR (right column) vs. dilated FCN baseline (left column) in each pair. Best viewed in color and zoom in.

Method

Backbone

mIoU

FCN (40k, SS) [40]

ResNet-101

73.93

FCN (40k, MS) [40]

ResNet-101

75.14

FCN (80k, SS) [40]

ResNet-101

75.52

FCN (80k, MS) [40]

ResNet-101

76.61

PSPNet [60] DeepLab-v3 [10] (MS) NonLocal [48] CCNet [26] GCNet [4]

ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101

78.50 79.30 79.10 80.20 78.10

Axial-DeepLab-XL [47] (MS) Axial-ResNet-XL 81.10

Axial-DeepLab-L [47] (MS) Axial-ResNet-L

81.50

SETR-PUP (40k, SS)

T-Large

78.39

SETR-PUP (40k, MS)

T-Large

81.57

SETR-PUP (80k, SS) SETR-PUP (80k, MS)

T-Large T-Large

79.34 82.15

Table 5. State-of-the-art comparison on the Cityscapes valida-

tion set. Performances of different training schedules (e.g., 40k

and 80k) are reported. SS: Single-scale inference. MS: Multi-

scale inference.

ImageNet-21k. For the transformer part, we use the weights pre-trained on ImageNet-21k [17] or randomly initialized.
We use patch size 16 × 16 for all the experiments. We perform 2D interpolation on the pre-trained position embeddings, according to their location in the original image for different input size ﬁne-tuning.

Evaluation metric Following the standard evaluation protocol [12], the metric of mean Intersection over Union (mIoU) averaged over all classes is reported. For ADE20K, additionally pixel-wise accuracy is reported following the existing practice.

4.2. Ablation studies
Table 1 shows ablation studies on (a) different variants of SETR on various training schedules, (b) comparison to Semantic FPN, (c) pre-training on different data, (d) compare

Method

Backbone

mIoU

PSPNet [60] DenseASPP [50]

ResNet-101 DenseNet-161

78.40 80.60

BiSeNet [52] PSANet [61] DANet [18] OCNet [54]

ResNet-101 ResNet-101 ResNet-101 ResNet-101

78.90 80.10 81.50 80.10

CCNet [26]

ResNet-101

Axial-DeepLab-L [47] Axial-ResNet-L

Axial-DeepLab-XL [47] Axial-ResNet-XL

81.90 79.50 79.90

SETR-PUP (100k) SETR-PUP‡

T-Large T-Large

81.08 81.64

Table 6. Comparison on the Cityscapes test set. ‡: trained on

ﬁne and coarse annotated data.

to SETR-Hybrid and scaling study of different variants, (e) compare to FCN with different pre-training. Unless otherwise speciﬁed, all experiments on Table 1 are trained on Cityscapes train ﬁne set with 40,000 iterations and batch size 8, and evaluated using the single scale test protocol on the Cityscapes validation set in mean IoU (%) rate. Experiments on ADE20K also follow the single scale test protocol.

From Table 1, we can make the following observations: (i) Progressively upsampling the feature maps, SETRPUP achieves the best performance among all the variants on Cityscapes. One possible reason for inferior performance of SETR-MLA is that the feature outputs of different transformer layers do not have the beneﬁts of resolution pyramid as in feature pyramid network (FPN) (see Figure 5). However, SETR-MLA performs slightly better than SETR-PUP, and much superior to the variant SETRNaive that upsamples the transformers output feature by 16× in one-shot, on ADE20K val set. (ii) The variants using “T-Large” (e.g., SETR-MLA and SETR-Naive) are superior to their “T-Small” counterparts, i.e., SETR-MLAS and SETR-Naive-S, as expected. (iii) While our SETR-

7

Figure 5. Visualization of output feature of layer Z1, Z9, Z17, Z24 of SETR trained on Pascal Context. Best viewed in color.
PUP-S (76.71) performs worse than SETR-Hybrid (76.90), it shines (78.02) when training with more iterations (80k). It suggests that FCN encoder design can be replaced in semantic segmentation, and further conﬁrms the effectiveness of our model. (iv) Pre-training is critical for our model (see (c)). Model pre-trained on the public ImageNet-21k gives the best performance, doubling the performance of random initialization. Further ﬁne-tuning the ImageNet-21k pretrained weights on ImageNet-1k even gives a slight performance drop in semantic segmentation task. (v) To study the power of pre-training and further verify the effectiveness of our proposed approach, we conduct the ablation study on the pre-training strategy in Table 1(e). For a fair comparison with the FCN baseline, we ﬁrst pre-train a ResNet101 on the Imagenet-21k dataset with a classiﬁcation task and then adopt the pre-trained weights for a dilated FCN training for the semantic segmentation task on ADE20K or Cityscapes. Table 1(e) shows that with ImageNet-21k pre-training FCN baseline experienced a clear improvement over the variant pre-trained on ImageNet-1k. However, our method outperforms the FCN counterparts by a large margin, verifying that the advantage of our approach largely comes from the proposed sequence-to-sequence modeling strategy rather than bigger pre-training data.
4.3. Comparison to state-of-the-art
Results on ADE20K Table 3 presents our results on the more challenging ADE20K dataset. The previous best method, ACNet [16] achieves mIoU 45.90% through combining richer local and global contexts. Our SETRMLA achieves superior mIoU of 48.64% with single-scale (SS) inference with a big margin over ACNet. When multiscale inference is adopted, our method achieves a new state of the art with mIoU hitting 50.28%. Figure 2 shows qualitative results of our model and dilated FCN on ADE20K. When training a single model on the train+validation set with the default 160,000 iterations, our method ranks 1st (SETR-PUP 44.42% mIoU) and 2nd (SETR-MLA 44.11% mIoU) places in the highly competitive ADE20K test server

Figure 6. Examples of attention maps from SETR trained on Pascal Context.
leaderboard 1. Note that we even do not adopt model ensemble and other tricks which may further improve the performance as they are highly model-agnostic.
Results on Pascal Context Table 4 compares the segmentation results on Pascal Context. Dilated FCN with the ResNet-101 backbone achieves a mIoU of 45.74%. Using the same training schedule, our proposed SETR signiﬁcantly outperforms this baseline, achieving mIoU of 54.40% (SETR-PUP) and 54.87% (SETR-MLA). SETRMLA further improves the performance to 55.83% when multi-scale (MS) inference is adopted, outperforming the nearest rival APCNet with a clear margin. Figure 3 gives some qualitative results of SETR and dilated FCN. Further visualization of the learned attention maps in Figure 6 shows that SETR can attend to semantically meaningful foreground regions, demonstrating its ability to learn discriminative feature representations useful for segmentation.
Results on Cityscapes Tables 5 and 6 show the comparative results on the validation and test set of Cityscapes respectively. We can see that our model SETR-PUP is superior to FCN baselines, and FCN plus attention based approaches, such as Non-local [48] and CCNet [26]; and its performance is on par with the best results reported so far. On this dataset we can now compare with the closely related Axial-DeepLab [11, 47] which aims to use an attentionalone model but still follows the basic structure of FCN. Note that Axial-DeepLab sets the same output stride 16 as ours. However, its input full resolution (1024 × 2048) is much larger than our crop size 768 × 768, and it runs more epochs (60k iteration with batch size 32) than our setting (80k iterations with batch size 8). Nevertheless, our model is still superior to Axial-DeepLab when multi-scale inference is adopted on Cityscapes validation set. Using the ﬁne set only, our model (trained with 100k iterations) outperforms Axial-DeepLab-XL with a clear margin on the test set. Figure 4 shows the qualitative results of our model and dilated FCN on Cityscapes.
1Please see the entries named “Sixiao Zheng” and “Jiachen Lu” with the submission time 2020-12-29 and 2020-12-16 at: http:// sceneparsing.csail.mit.edu/eval/leaderboard.php.

8

5. Conclusion
In this work, we have presented an alternative perspective for semantic segmentation in images by introducing a sequence-to-sequence prediction framework. In contrast to existing FCN based methods that enlarge the receptive ﬁeld typically with dilated convolutions and attention modules at the component level, we made a step change at the architectural level to completely eliminate the reliance on FCN and elegantly solve the limited receptive ﬁeld challenge. We implemented the proposed idea with Transformers that can model global context at every stage of feature learning. Along with a set of decoder designs in different complexity, strong segmentation models are established with none of the bells and whistles deployed by recent methods. Extensive experiments demonstrate that our models set new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Encouragingly, our method is ranked the 1st (44.42% mIoU) place in the highly competitive ADE20K test server leaderboard.
References
[1] Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. arXiv preprint, 2020. 11
[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. TPAMI, 2017. 1, 2
[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional networks. In ICCV, 2019. 2
[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. arXiv preprint, 2019. 7
[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. 2
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected CRFs. ICLR, 2015. 2
[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 2
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2018. 1
[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2018. 2
[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint, 2017. 7

[11] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020. 8
[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5, 7
[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In ACL, 2019. 2
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. 2
[15] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Semantic correlation promoted shape-variant context for segmentation. In CVPR, 2019. 6
[16] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. In CVPR, 2019. 6, 8
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint, 2020. 2, 3, 6, 7
[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019. 2, 6, 7
[19] Junjun He, Zhongying Deng, and Yu Qiao. Dynamic multiscale ﬁlters for semantic segmentation. In ICCV, 2019. 6
[20] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for semantic segmentation. In CVPR, 2019. 6
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 2, 3
[22] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. arXiv preprint, 2019. 3
[23] Matthias Holschneider, Richard Kronland-Martinet, Jean Morlet, and Ph Tchamitchian. A real-time algorithm for signal analysis with the help of the wavelet transform. In Wavelets, 1990. 1
[24] Qinbin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In CVPR, 2020. 6
[25] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019. 2
[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019. 1, 3, 6, 7, 8

9

[27] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019. 2
[28] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dolla´r. Panoptic feature pyramid networks. In CVPR, 2019. 4, 5, 6
[29] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improving semantic segmentation via decoupled body and edge supervision. In ECCV, 2020. 1
[30] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, and Yunhai Tong. Global aggregation then local distribution in fully convolutional networks. In BMVC, 2019. 1
[31] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, and Kuiyuan Yang. Gff: Gated fully fusion for semantic segmentation. In AAAI, 2020. 6
[32] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization attention networks for semantic segmentation. In CVPR, 2019. 6
[33] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. arXiv preprint, 2020. 3
[34] Tsung-Yi Lin, Piotr Dolla´r, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 4
[35] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-to-end lane shape prediction with transformers. arXiv preprint, 2020. 3
[36] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. Semantic image segmentation via deep parsing network. In ICCV, 2015. 2
[37] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1, 2, 3, 6
[38] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 5
[39] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. 2
[40] OpenMMLab. mmsegmentation. https://github. com/open-mmlab/mmsegmentation, 2020. 5, 6, 7
[41] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters — improve semantic segmentation by global convolutional network. In CVPR, 2017. 1, 2
[42] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens. Stand-alone self-attention in vision models. In NeurIPS, 2019. 2
[43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. MICCAI, 2015. 2
[44] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 2

[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2
[46] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2018. 4
[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Standalone axial-attention for panoptic segmentation. In ECCV, 2020. 1, 2, 3, 7, 8
[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018. 1, 2, 7, 8
[49] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In ICLR, 2019. 2
[50] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in street scenes. In CVPR, 2018. 1, 7
[51] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019. 2
[52] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In ECCV, 2018. 7
[53] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. ICLR, 2016. 2
[54] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv preprint, 2018. 6, 7
[55] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation. In CVPR, 2018. 6
[56] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, and Philip HS Torr. Dual graph convolutional network for semantic segmentation. In BMVC, 2019. 3
[57] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing networks. In CVPR, 2020. 1, 2, 3
[58] Richard Zhang. Making convolutional networks shiftinvariant again. In ICML, 2019. 1
[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. 2
[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 1, 2, 5, 6, 7
[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In ECCV, 2018. 2, 7
[62] Shuai Zheng, Sadeep Jayasumana, Bernardino RomeraParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip H. S. Torr. Conditional random ﬁelds as recurrent neural networks. In ICCV, 2015. 2

10

[63] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. arXiv preprint, 2016. 5
Appendix A. Visualizations
Features Figure 9 shows the feature visualization of of our SETR-PUP. For the encoder, 24 features from the output of 24 transformer layers namely Z1 − Z24 are collected. Meanwhile, 5 features (U 1 − U 5) right after each bi-linear interpolation in the decoder head are visited. Attention maps Attention maps (Figure 10) in each transformer layer catch our interest. There are 16 heads and 24 layers in T-large. Similar to [1], a recursion perspective into this problem is applied. Figure 7 shows the attention maps of different selected spatial points (red). Position embedding Visualization of the learned position embedding in Figure 8 shows that the model learns to encode distance within image in the similarity of position embeddings.
Figure 8. Similarity of position embeddings of SETR trained on Pascal Context. Tiles show the cosine similarity between the position embedding of the patch with the indicated row and column and the position embeddings of all other patches.
Figure 7. The ﬁrst column shows images from Pascal Context. The second column illustrates the attention map of the picked points (red).
11

Figure 9. Visualization of output feature of layer Z1 − Z24 and U 1 − U 5 of SETR-PUP trained on Pascal Context. Best view in color. First row: The input image. Second row: Layer Z1-Z12. Third row: Layer Z13-Z24. Fourth row: Layer U 1 − U 5.
Figure 10. More examples of attention maps from SETR trained on Pascal Context. 12

