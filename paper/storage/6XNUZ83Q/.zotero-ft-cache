IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

1

CCNet: Criss-Cross Attention for Semantic Segmentation

Zilong Huang, Xinggang Wang, Member, IEEE, Yunchao Wei, Lichao Huang, Humphrey Shi, Member, IEEE, Wenyu Liu, Senior Member, IEEE, and Thomas S. Huang, Life Fellow, IEEE

arXiv:1811.11721v2 [cs.CV] 9 Jul 2020

Abstract‚ÄîContextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efÔ¨Åcient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can Ô¨Ånally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11√ó less GPU memory usage. 2) High computational efÔ¨Åciency. The recurrent criss-cross attention signiÔ¨Åcantly reduces FLOPs by about 85% of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9%, 45.76% and 55.47% on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at https://github.com/speedinghzl/CCNet.
Index Terms‚ÄîSemantic Segmentation, Graph Attention, Criss-Cross Network, Context Modeling
!

1 INTRODUCTION
S EMANTIC segmentation, which is a fundamental problem in the computer vision community, aims at assigning semantic class labels to each pixel in a given image. It has been extensively and actively studied in many recent works and is also critical for various signiÔ¨Åcant applications such as autonomous driving [1], augmented reality [2],image editing [3], civil engineering [4], remote sensing imagery [5] and agricultural pattern analysis [6], [7]. SpeciÔ¨Åcally, current state-of-the-art semantic segmentation approaches based on the fully convolutional network (FCN) [8] have made remarkable progress. However, due to the Ô¨Åxed geometric structures, the conventional FCN is inherently limited to local receptive Ô¨Åelds that only provide short-range contextual information. The limitation of insufÔ¨Åcient contextual information imposes a great adverse effect on its segmentation accuracy.
To make up for the above deÔ¨Åciency of FCN, some works have been proposed to introduce useful contextual information to beneÔ¨Åt the semantic segmentation task. SpeciÔ¨Åcally, Chen et al. [10] proposed atrous spatial pyramid pooling

HxW

(a) Non-local block

H+W-1

H+W-1

(b) Criss-Cross Attention block

‚Ä¢ Z. Huang, X. Wang and W. Liu are with the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: hzl@hust.edu.cn, xgwang@hust.edu.cn, liuwy@hust.edu.cn).
‚Ä¢ Y. Wei is with the Centre for ArtiÔ¨Åcial Intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW 2007, Australia. (e-mail: yunchao.wei@uts.edu.au).
‚Ä¢ L. Huang is with Horizon Robotics. (e-mail: lichao.huang@horizon.ai). ‚Ä¢ H. Shi is with the University of Oregon and the University of Illinois at
Urbana-Champaign. (e-mail: shihonghui3@gmail.com). ‚Ä¢ T. S. Huang was with the University of Illinois at Urbana-Champaign.
(e-mail: t-huang1@illinois.edu).
Corresponding author: Xinggang Wang. Zilong Huang and Xinggang Wang contributed equally to this work.

Few context

Rich context

Fig. 1. Diagrams of two attention-based context aggregation methods. (a) For each position (e.g., blue), the Non-local module [9] generates a dense attention map which has N weights (in green). (b) For each position (e.g., blue), the criss-cross atte‚àöntion module generates a sparse attention map which only has about 2 N weights. After the recurrent operation, each position (e.g., red) in the Ô¨Ånal output feature maps can collect information from all pixels. For clear display, residual connections are ignored.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

2

module with multi-scale dilation convolutions for contextual information aggregation. Zhao et al. [11] further introduced PSPNet with pyramid pooling module to capture contextual information. However, the dilated convolution based methods [10], [12], [13] collect information from a few surrounding pixels and cannot generate dense contextual information actually. Meanwhile, the pooling based methods [11], [14] aggregate contextual information in a nonadaptive manner and the homogeneous context extraction procedure is adopted by all image pixels, which does not satisfy the requirement that different pixels need different contextual dependencies.
To incorporate dense and pixel-wise contextual information, some fully-connected graph neural network (GNN) [15] methods were proposed to augments traditional convolutional features with an estimated full-image context representation. PSANet [16] learns to aggregate contextual information for each position via a predicted attention map. Non-local Networks [9] utilizes a self-attention mechanism [17], [18], which enables a single feature from any position to perceive features of all the other positions, thus harvesting full-image contextual information, see Fig. 1 (a). These non-local operations could be viewed as a denselyconnected GNN module based on attention mechanism [18]. This feature augmentation method allows a Ô¨Çexible way to represent non-local relations between features and has led to signiÔ¨Åcant improvements in several vision recognition tasks. However, these GNN-based non-local neural networks need to generate huge attention maps to measure the relationships for each pixel-pair, leading to a very high complexity of O(N 2) for both time and space, where N is the number of input features. Since the dense prediction tasks, such as semantic segmentation, inherently require high resolution feature maps, the non-local based methods will often with high computation complexity and occupy a huge number of GPU memory. Thus, is there an alternative solution to achieve such a target in a more efÔ¨Åcient way?
To address the above mentioned issue, our motivation is to replace the common single densely-connected graph with several consecutive sparsely-connected graphs, which usually require much lower computational resources. Without loss of generality, we use two consecutive criss-cross attention mo‚àödules, in which each one only has sparse connections (about N ) for each position in the feature map. For each pixel/position, the criss-cross attention module aggregates contextual information in its horizontal and vertical directions. By serially stacking two criss-cross attention modules, each position can collect contextual information from all pixels in the given image. The above decomposition strategy will greatly reduce th‚àöe complexities of both time and space from O(N 2) to O(N N ).
We compare the differences between the non-local module [9] and our criss-cross attention module in Fig. 1. Concretely, both non-local module and criss-cross attention module feed the input feature map to generate an attention map for each position and transform the input feature map into an adapted feature map. Then, a weighted sum is adopted to collecting contextual information from other positions in the adapted feature map based on the attention maps. Different from the dense connections adopted by the non-local module, each position (e.g., blue) in the feature

map is sparsely connected with other ones which are in the same row and the same column in our criss-cross attention module‚àö, leading to the predicted attention map only has about 2 N weights rather than N in non-local module.
To achieve the goal of capturing the full-image dependencies, we innovatively and simply take a recurrent operation for the criss-cross attention module. In particular, the local features are Ô¨Årstly passed through one criss-cross attention module to collect the contextual information in horizontal and vertical directions. Then, by feeding the feature map produced by the Ô¨Årst criss-cross attention module into the second one, the additional contextual information obtained from the criss-cross path Ô¨Ånally enables the fullimage dependencies for all positions. As demonstrated in Fig. 1 (b), each position (e.g.red) in the second feature map can collect information from all others to augment the position-wise representations. We share parameters of the criss-cross modules to keep our model slim. Since the input and output are both convolutional feature maps, crisscross attention module can be easily plugged into any fully convolutional neural network, named as CCNet, for learning full-image contextual information in an end-to-end manner. Thanks to the good usability of criss-cross attention module, CCNet is straight forward to extend to 3D networks for capturing long-range temporal context information.
In addition, to drive the proposed recurrent criss-cross attention method to learn more discriminative features, we introduce a category consistent loss to augment CCNet. Particularly, the category consistent loss enforces the network to map each pixel in the image to an n-dimensional vector in the feature space, such that feature vectors of pixels that belong to the same category lie close together while feature vectors of pixels that belong to different categories lie far apart.
We have carried out extensive experiments on multiple large-scale datasets. Our proposed CCNet achieves top performance on four most competitive semantic segmentation datasets, i.e., Cityscapes [19], ADE20K [20], LIP [21] and CamVid [22]. In addition, the proposed criss-cross attention even improves the state-of-the-art instance segmentation method, i.e., Mask R-CNN with ResNet-101 [23]. These results well demonstrate that our criss-cross attention module is generally beneÔ¨Åcial to the dense prediction tasks. In summary, our main contributions are three-fold:
‚Ä¢ We propose a novel criss-cross attention module in this work, which can be leveraged to capture contextual information from full-image dependencies in a more efÔ¨Åcient and effective way.
‚Ä¢ We propose category consistent loss which can enforce criss-cross attention module to produce more discriminative features.
‚Ä¢ We propose CCNet by taking advantages of recurrent criss-cross attention module, achieving leading performance on segmentation-based benchmarks, including Cityscapes, ADE20K, LIP, CamVid and COCO.
Compare with our original conference version [24], the following improvements are conducted: 1) We further enhance the segmentation ability of CCNet by augmenting a simple yet effective category consistent loss; 2) we propose

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

3

a more generic CCNet by extending the criss-cross attention module from 2D to 3D; 3) we include more extensive experiments on the LIP, CamVid and COCO datasets to verify the effectiveness and generalization ability of our CCNet.
The rest of this paper is organized as follows. We Ô¨Årst review related work in Section 2 and describe the architecture of our network in Section 3. In Section 4, ablation studies are given and experimental results are analyzed. Section 5 presents our conclusion and future work.
2 RELATED WORK
2.1 Semantic segmentation
The last years have seen a renewal of interest on semantic segmentation. FCN [8] is the Ô¨Årst approach to adopt fully convolutional network for semantic segmentation. Later, FCN-based methods have made remarkable progress in image semantic segmentation. Chen et al. [25] and Yu et al. [26] removed the last two downsample layers to obtain dense prediction and utilized dilated convolutions to enlarge the receptive Ô¨Åeld. Unet [27], DeepLabv3+ [28], MSCI [29], SPGNet [30], ReÔ¨ÅneNet [31] and DFN [32] adopted encoderdecoder structures that fuse the information in low-level and high-level layers to make dense predictions. The scaleadaptive convolutions (SAC) [33] and deformable convolutional networks (DCN) [34] methods improved the standard convolutional operator to handle the deformation and various scales of objects. CRF-RNN [26] and DPN [35] used Graph model, i.e., CRF, MRF, for semantic segmentation. AAF [36] used adversarial learning to capture and match the semantic relations between neighboring pixels in the label space. BiSeNet [37] was designed for real-time semantic segmentation. DenseDecoder [38] built feature-level long-range skip connections on cascaded architecture. VideoGCRF [39] used a densely-connected spatio-temporal graph for video semantic segmentation. RTA [40] proposed the region-based temporal aggregation for leveraging the temporal information in videos. In addition, some works focus on human parsing task. JPPNet [21] embed pose estimation into human parsing task. CE2P [41] proposed a simple yet effective framework for computing context embedding while preserving edges. SANet [42] used parallel branches with scale attention to handle large scale variance in human parsing. Semantic segmentation is also actively studied in the context of domain adaptation and dstillation [43], [44], [45] and weakly supervised setting [46], [47], [48], etc.

et al. [16] proposed the point-wise spatial attention network which uses predicted attention map to guide contextual information collection. Auto-Deeplab [52] utilized neural architecture search to search an effective context modeling. He et al. [53] proposed an adaptive pyramid context module for semantic segmentation. Liu et al. [54] utilized recurrent neural networks (RNNs) to capture long-range dependencies.
There are some works use graph models to model the contextual information. Conditional random Ô¨Åeld (CRF) [25], [40], [55], Markov random Ô¨Åeld (MRF) [35] were also utilized to capture long-range dependencies for semantic segmentation. Vaswani et al. [18] applied a self-attention model on machine translation. Wang et al. [9] proposed the non-local module to generate the huge attention map by calculating the correlation matrix between each spatial point on the feature maps, then the attention map guided dense contextual information aggregation. OCNet [56] and DANet [57] utilized Non-local module [9] to harvest the contextual information. PSA [16] learned an attention map to aggregate contextual information for each individual point adaptively and speciÔ¨Åcally. Chen et al. [58] proposed graph-based global reasoning networks which implements relation reasoning via graph convolution on a small graph.
CCNet vs. Non-Local vs. GCN. Here, we speciÔ¨Åcally discuss the differences among GCN [59], Non-local Network [9] and CCNet. In term of contextual information aggregation, only the center point can perceive the contextual information from all pixels by the global convolution Ô¨Ålters in GCN [59]. In contrast, Non-local Network [9] and CCNet guarantee that a pixel at any position perceives contextual information from all pixels. Though GCN [59] alternatively decomposes the square-shape convolutional operation to horizontal and vertical linear convolutional operations which is related to CCNet, CCNet takes the crisscross way to harvest contextual information which is more effective than the horizontal-vertical separate way. Moreover, CCNet is proposed to mimic Non-local Network [9] for obtaining dense contextual information through a more effective and efÔ¨Åcient recurrent criss-cross attention module, in which dissimilar features get low attention weights and features with high attention weights are similar ones. GCN [59] is a conventional convolution neural network, while CCNet is a graph neural network in which each pixel in the convolutional feature map is considered as a node and the relation/context among nodes can be utilized to generate better node features.

2.2 Contextual information aggregation
It is a common practice to aggregate contextual information to augment the feature representation in semantic segmentation networks. Deeplabv2 [10] proposed atrous spatial pyramid pooling (ASPP) to use different dilation convolutions to capture contextual information. DenseASPP [49] brought dense connections into ASPP to generate features with various scale. DPC [50] utilized architecture search techniques to build multi-scale architectures for semantic segmentation. Chen et al. [51] made use of several attention masks to fuse feature maps or prediction maps from different branches. PSPNet [11] utilized pyramid spatial pooling to aggregate contextual information. Recently, Zhao

2.3 Graph neural networks
Our work is related to deep graph neural network (GNN). Prior to graph neural networks, graphical models, such as the conditional random Ô¨Åeld (CRF) [25], [40], [55], markov random Ô¨Åeld (MRF) [35], were widely used to model the long-range dependencies for image understanding. GNNs were early studied in [15], [60], [61]. Inspired by the success of CNNs, a large number of methods adapt graph structure into CNNs. These methods could be divided into two main steams, the spectral-based approaches [62], [63], [64], [65] and the spatial-based approaches [9], [66], [67], [68]. The proposed CCNet belongs to the latter.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

4

Reduction
Segmentation

CNN X
Input/output

Criss-Cross

Criss-Cross

H

Attention Module H‚Äô

Attention Module H‚Äô‚Äô

Recurrent Criss-Cross Attention (R=2)

feature extraction

Operation

Concatenation

Fig. 2. Overview of the proposed CCNet for semantic segmentation.

3 APPROACH
In this section, we give the details of the proposed CrissCross Network (CCNet) for semantic segmentation. We Ô¨Årst present a general framework of our CCNet. Then, the 2D criss-cross attention module which captures contextual information in horizontal and vertical directions will be introduced. To capture the dense and global contextual information, we propose to adopt a recurrent operation for the criss-cross attention module. To further improve RCCA, we introduce a discriminative loss function to drive RCCA to learn category consistent features. Finally we propose the 3D criss-cross attention module for leveraging temporal and spatial contextual information simultaneously.
3.1 Network Architecture
The network architecture is given in Fig. 2. An input image is passed through a deep convolutional neural network (DCNN), which is designed in a fully convolutional fashion [10], to produce feature map X with the spatial size of H √ó W . In order to retain more details and efÔ¨Åciently produce dense feature maps, we remove the last two downsampling operations and employ dilation convolutions in the subsequent convolutional layers, leading to enlarging the width/height of the output feature map X to 1/8 of the input image.
Given X, we Ô¨Årst apply a convolutional layer to obtain the feature map H of dimension reduction. Then, H is fed into the criss-cross attention module to generate a new feature map H which aggregate contextual information together for each pixel in its criss-cross path. The feature map H only contains the contextual information in horizontal and vertical directions which are not powerful enough for accurate semantic segmentation. To obtain richer and denser context information, we feed the feature map H into the criss-cross attention module again and output the feature map H . Thus, each position in H actually gathers the information from all pixels. Two criss-cross attention modules before and after share the same parameters to avoid adding too many extra parameters. We name this recurrent structure as recurrent criss-cross attention (RCCA) module.
Then, we concatenate the dense contextual feature H with the local representation feature X. It is followed by one or several convolutional layers with batch normalization

and activation for feature fusion. Finally, the fused features are fed into the segmentation layer to predict the Ô¨Ånal segmentation result.

3.2 Criss-Cross Attention
To model full-image dependencies over local feature repre-
sentations using light-weight computation and memory, we
introduce a criss-cross attention module. The criss-cross at-
tention module collects contextual information in horizontal
and vertical directions to enhance pixel-wise representative
capability. As shown in Fig. 3, given a local feature map H ‚àà RC√óW √óH , the module Ô¨Årst applies two convolutional layers with 1 √ó 1 Ô¨Ålters on H to generate two feature maps Q and K, respectively, where {Q, K} ‚àà RC √óW √óH . C is the number of channel, which is less than C for dimension
reduction. After obtaining Q and K, we further generate an atten-
tion map A ‚àà R(H+W ‚àí1)√ó(W √óH) via AfÔ¨Ånity operation. At each position u in the spatial dimension of Q, we can obtain a vector Qu ‚àà RC . Meanwhile, we can also obtain the set ‚Ñ¶u ‚àà R(H+W ‚àí1)√óC by extracting feature vectors from K which are in the same row or column with position u. ‚Ñ¶i,u ‚àà RC is the i-th element of ‚Ñ¶u. The AfÔ¨Ånity operation is then deÔ¨Åned as follows.

di,u = Qu‚Ñ¶i,u,

(1)

where di,u ‚àà D is the degree of correlation between features Qu and ‚Ñ¶i,u, i = [1, ..., H + W ‚àí 1], and D ‚àà R(H+W ‚àí1)√ó(W √óH). Then, we apply a softmax layer on D
over the channel dimension to calculate the attention map
A. Another convolutional layer with 1 √ó 1 Ô¨Ålters is applied
on H to generate V ‚àà RC√óW √óH for feature adaptation. At
each position u in the spatial dimension of V, we can obtain a vector Vu ‚àà RC and a set Œ¶u ‚àà R(H+W ‚àí1)√óC . The set Œ¶u is a collection of feature vectors in V which are in the same
row or column with position u. The contextual information
is collected by an Aggregation operation deÔ¨Åned as follows.

H+W ‚àí1

Hu =

Ai,uŒ¶i,u + Hu,

(2)

i=0

where Hu is a feature vector in H ‚àà RC√óW √óH at position u and Ai,u is a scalar value at channel i and position u in

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

5

1x1 Conv

Affinity

Q

A

softmax

H

K

1x1 Conv

1x1 Conv

V

Fig. 3. The details of criss-cross attention module.

(uÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)

(Ì†µÌºΩÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)
Ì†µÌ≤á(Ì†µÌ±®, uÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö, Ì†µÌºΩÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)

(uÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)

Aggregation

H‚Äô (Ì†µÌºΩÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)

Ì†µÌ≤á(Ì†µÌ±®, Ì†µÌºΩÌ†µÌ≤ô,uÌ†µÌ≤ö, Ì†µÌºΩÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö) Ì†µÌ≤á(Ì†µÌ±®‚Ä≤, uÌ†µÌ≤ô,uÌ†µÌ≤ö, uÌ†µÌ≤ô,Ì†µÌºΩÌ†µÌ≤ö)

(uÌ†µÌ≤ô,uÌ†µÌ≤ö)

Loop 1

(Ì†µÌºΩÌ†µÌ≤ô,uÌ†µÌ≤ö)

Ì†µÌ≤á(Ì†µÌ±®‚Ä≤, uÌ†µÌ≤ô,uÌ†µÌ≤ö, Ì†µÌºΩÌ†µÌ≤ô,uÌ†µÌ≤ö)

(uÌ†µÌ≤ô,uÌ†µÌ≤ö)

(Ì†µÌºΩÌ†µÌ≤ô,uÌ†µÌ≤ö)

Loop 2

Fig. 4. An example of information propagation when the loop number is 2.

A. The contextual information is added to local feature H to augment the pixel-wise representation. Therefore, it has a wide contextual view and selectively aggregates contexts according to the spatial attention map. These feature representations achieve mutual gains and are more robust for semantic segmentation.

1 √ó 1 Ô¨Ålters can be view as the identical connection. In the case of R = 2, the connections between any two spatial positions in the feature map built up by the RCCA module can be clearly and quantitatively described by introducing function f deÔ¨Åned as follows.

‚àÉi

‚àà

H
R

+W

‚àí1,

s.t.

Ai,u

=

f (A,

uCx C ,

uCy C ,

ux,

uy ),

where u(ux, uy) ‚àà RH√óW is any spatial position in H and uCC (uCx C , uCy C ) ‚àà RH+W ‚àí1 is a position in the crisscross structure centered at u. The function f is actually an one-to-one mapping from the position pair (uCC , u) ‚àà R(H+W ‚àí1)√ó(H√óW ) in the feature map to a particular element Ai,u ‚àà R(H+W ‚àí1)√ó(H√óW ) in the attention map A ‚äÇ R(H+W ‚àí1)√ó(H√óW ), where uCC maps to a particular row i in A and u maps to a particular column in A.
With the help of function f , we can easily describe the information propagation between any position u in H and any position Œ∏ in H. It is obvious that information could Ô¨Çow from Œ∏ to u when Œ∏ is in the criss-cross path of u.
Then, we focus on another situation in which Œ∏(Œ∏x, Œ∏y) is NOT in the criss-cross path of u(ux, uy). To make it easier to understand, we visualize the information propagation in Fig. 4. The position (Œ∏x, Œ∏y), which is blue, Ô¨Årstly passes the information into the (ux, Œ∏y) and (Œ∏x, uy) (light green) in the loop 1. The propagation could be quantiÔ¨Åed by function f . It should be noted that these two points (ux, Œ∏y) and (Œ∏x, uy) are in the criss-cross path of u(ux, uy). Then, the positions (ux, Œ∏y) and (Œ∏x, uy) pass the information into the (ux, uy) (dark green) in the loop 2. Thus, the information in Œ∏(Œ∏x, Œ∏y) could eventually Ô¨Çow into u(ux, uy) even if Œ∏(Œ∏x, Œ∏y) is NOT in the criss-cross path of u(ux, uy).
In general, our RCCA module makes up for the deÔ¨Å-
ciency of criss-cross attention that cannot obtain the dense
contextual information from all pixels. Compared with crisscross attention, the RCCA module (R = 2) does not bring
extra parameters and can achieve better performance with
the cost of a minor computation increment.

3.3 Recurrent Criss-Cross Attention (RCCA)
Despite the criss-cross attention module can capture contextual information in horizontal and vertical directions, the connections between one pixel and its around ones that are not in the criss-cross path are still absent. To tackle this problem, we innovatively and simply introduce a RCCA operation based on the criss-cross attention. The RCCA module can be unrolled into R loops. In the Ô¨Årst loop, the criss-cross attention takes the feature map H extracted from a CNN model as the input and output the feature map H , where H and H are with the same shape. In the second loop, the criss-cross attention takes the feature map H as the input and output the feature map H . As shown in Fig. 2, the RCCA module is equipped with two loops (R = 2) which is able to harvest full-image contextual information from all pixels to generate new features with dense and rich contextual information.
We denote A and A as the attention maps in loop 1 and loop 2, respectively. Since we are interested only in contextual information spreads in spatial dimension rather than in channel dimension, the convolutional layer with

3.4 Learning Category Consistent Features
For semantic segmentation tasks, the pixels belonging to the same category should have the similar features, while the pixels from different categories should have far apart features. We name such a characteristic as category consistency. The deep features produced by RCCA have fullimage context; however, the aggregated feature may have the problem of over-smoothing, which is a common issue in graph neural networks. To address this potential issue, beside the cross-entropy loss seg to penalize the mismatch between the Ô¨Ånal predicted segmentation maps and ground truth, we further introduce the category consistent loss to drive RCCA module to learn category consistent features directly.
In [69], a discriminative loss function with three competing terms is proposed for instance segmentation. In particular, the three terms, denoted as var, dis, reg, are adopted to 1) penalize large distances between features with the same label for each instance, 2) penalize small distances between the mean features of different labels, and 3) draw mean features of all categories towards the origin, respectively.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

6

Motivated by [69], we Ô¨Årst adapt a discriminative loss for semantic segmentation rather than instance segmentation, then replace the Ô¨Årst term with more robust one: instead of using quadratic function as the distance function to penalize mismatch all along, we design a piece-wise distance function to make the optimization more robust.
Let C be the set of classes that are present in the minibatch images. Nc is the number of valid elements belonging to category c ‚àà C. hi ‚àà H is the feature vector at spatial position i. ¬µc is the mean feature of category c ‚àà C (the cluster center). œï is a piece-wise distance function. Œ¥v and Œ¥d are respectively the margins. In particular, Eq. 6 is a piecewise distance function and the function œïvar will be zero, quadratic, and linear function when the distance from the center ¬µc is within dv, in range of (Œ¥v, Œ¥d], and exceeds Œ¥d, respectively.

1

1 Nc

var

=

|C |

c‚ààC

Nc

œïvar(hi, ¬µc),
i=1

(3)

1x1x1 Conv

Affinity

A

H

Q

softmax

H‚Äô

K

Aggregation

1x1x1 Conv

V

1x1x1 Conv

Fig. 5. The details of 3D criss-cross attention module.
the AfÔ¨Ånity operation. At each position u of Q, we can obtain a vector Qu ‚àà RC . u contains three coordinate values (t, x, y). We can also obtain the set ‚Ñ¶u ‚àà R(T +H+W ‚àí2)√óC by extracting feature vectors from K with at least two coordinate values equal to u. ‚Ñ¶i,u ‚àà RC is the i-th element of ‚Ñ¶u. The AfÔ¨Ånity operation is then deÔ¨Åned as follows.

1

dis = |C|(|C| ‚àí 1)

œïdis(¬µca , ¬µcb ),

(4)

ca‚ààC cb‚ààC

ca =cb

1

reg = |C|

¬µc ,

(5)

c‚ààC

Ô£± Ô£≤

¬µc ‚àí hi ‚àí Œ¥d + (Œ¥d ‚àí Œ¥v)2,

œïvar = ( ¬µc ‚àí hi ‚àí Œ¥v)2,

Œ¥v <

Ô£≥ 0,

¬µc ‚àí hi > Œ¥d ¬µc ‚àí hi ‚â§ Œ¥d ¬µc ‚àí hi ‚â§ Œ¥v
(6)

œïdis =

(2Œ¥d ‚àí ¬µca ‚àí ¬µcb )2, 0,

¬µca ‚àí ¬µcb ‚â§ 2Œ¥d ¬µca ‚àí ¬µcb > 2Œ¥d

(7)

To reduce the computation load, we Ô¨Årst apply a convolutional layer with 1 √ó 1 Ô¨Ålters on the output of RCCA module for dimension reduction and then apply these three loss on the feature map with fewer channels. The Ô¨Ånal loss
is weighted sum of all losses.

di,u = Qu‚Ñ¶i,u ,

(9)

where di,u ‚àà D is the degree of correlation between fea-
ture Qu and ‚Ñ¶i,u, i = [1, ..., (T + H + W ‚àí 2)], D ‚àà R(T +H+W ‚àí2)√óT √óW √óH . Then, we apply a softmax layer on
D over the Ô¨Årst dimension to calculate the attention map A.
Another convolutional layer with 1 √ó 1 √ó 1 Ô¨Ålters is applied on H to generate V ‚àà RC√óT √óW √óH for feature
adaptation. At each position u in the spatial dimension of V, we can obtain a vector Vu ‚àà RC and a set Œ¶u ‚àà R(T +H+W ‚àí2)√óC . The the set Œ¶u is a collection of
feature vectors in V which are in the criss-cross structure
centered at u. The contextual information is collected by the
Aggregation operation:

T +H+W ‚àí2

Hu =

Ai,uŒ¶i,u + Hu,

(10)

i=0

where Hu is a feature vector in the output feature map H ‚àà RC√óT √óW √óH at position u. Ai,u is a scalar value at channel i and position u in A.

= seg + Œ± var + Œ≤ dis + Œ≥ reg,

(8)

where Œ±, Œ≤ and are the weight parameters. In our experiments we set Œ¥v = 0.5, Œ¥d = 1.5, Œ± = Œ≤ = 1, Œ≥ = 0.001 and 16 as the number of channels for dimension reduction.

3.5 3D Criss-Cross Attention
To adapt our method from 2D applications to 3D dense prediction tasks, we introduce 3D Criss-Cross Attention. In general, the architecture of 3D Criss-Cross Attention is an extension the 2D version by additional collecting more contextual information from the temporal dimension. As shown in Fig. 5, given a local feature map H ‚àà RC√óT √óW √óH , where T is axial dimension (i.e., temporal dimension in video data). The module Ô¨Årstly applies two convolutional layers with 1 √ó 1 √ó 1 Ô¨Ålters on H to generate two feature maps Q and K, respectively, where {Q, K} ‚àà RC √óT √óW √óH .
After obtaining the feature maps Q and K, we further generate an attention map A ‚àà R(T +H+W ‚àí2)√óT √óW √óH via

4 EXPERIMENTS
To evaluate the effectiveness of the CCNet, we carry out comprehensive experiments on the Cityscapes dataset [19], the ADE20K dataset [20], the COCO dataset [70], the LIP dataset [21] and the CamVid dataset [71]. Experimental results demonstrate that CCNet achieves state-of-the-art performance on Cityscapes, ADE20K and LIP. Meanwhile, CCNet can bring constant performance gain on COCO for instance segmentation. In the following subsections, we Ô¨Årst introduce the datasets and implementation details, then we perform a series of ablation experiments on Cityscapes dataset. Finally, we report our results on ADE20K, LIP, COCO and CamVid datasets.
4.1 Datasets and Evaluation Metrics
We adopt Mean IoU (mIOU, mean of class-wise intersection over union) for Cityscapes, ADE20K, LIP and CamVid and

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

7

the standard COCO metrics Average Precision (AP) for COCO.

TABLE 1 Comparison with state-of-the-arts on Cityscapes (test).

‚Ä¢ Cityscapes is tasked for urban segmentation. Only the 5,000 Ô¨Ånely annotated images are used in our experiments and are divided into 2,975/500/1,525 images for training, validation, and testing, respectively.
‚Ä¢ ADE20K is a recent scene parsing benchmark containing dense labels of 150 stuff/object categories. The dataset includes 20k/2k/3k images for training, validation and testing, respectively.
‚Ä¢ LIP is a large-scale single human parsing dataset. There are 50,462 images with Ô¨Åne-grained annotations at pixel-level with 19 semantic human part labels and one background label. Those images are further divided into 30k/10k/10k for training, validation and testing, respectively.
‚Ä¢ COCO is a very challenging dataset for instance segmentation that contains 115k images over 80 categories for training, 5k images for validation and 20k images for testing.
‚Ä¢ CamVid is one of the datasets focusing on semantic segmentation for autonomous driving scenarios. It is composed of 701 densely annotated images with size 720 √ó 960 from Ô¨Åve video sequences.

Method

Backbone

mIOU(%)

Performance on val set

DeepLabv3 [12]

ResNet-101

79.3

DeepLabv3+ [28]

Xception-65

79.1

DPC [50] ‚Ä†

Xception-71

80.8

CCNet

ResNet-101

80.5

Performance on test set

DeepLab-v2 [10]

ResNet-101

70.4

ReÔ¨ÅneNet [31] ‚Ä°

ResNet-101

73.6

SAC [33] ‚Ä°

ResNet-101

78.1

GCN [59] ‚Ä°

ResNet-101

76.9

DUC [73] ‚Ä°

ResNet-101

77.6

ResNet-38 [74]

WiderResnet-38

78.4

PSPNet [11]

ResNet-101

78.4

BiSeNet [37] ‚Ä°

ResNet-101

78.9

AAF [36]

ResNet-101

79.1

PSANet [16] ‚Ä°

ResNet-101

80.1

DFN [32] ‚Ä°

ResNet-101

79.3

DenseASPP [49] ‚Ä° DenseNet-161

80.6

CCNet ‚Ä°

ResNet-101

81.9

‚Ä† use extra COCO dataset for training. ‚Ä° train with both the train-Ô¨Åne and val-Ô¨Åne datasets.

4.2 Implementation Details
Network Structure For semantic segmentation, we choose the ImageNet pre-trained ResNet-101 [23] as our backbone network, remove its last two down-sampling operations, and employ dilated convolutions in the subsequent convolutional layers following the previous work [25], resulting in the output stride as 8. For human parsing, we choose CE2P [41] as our baseline and replace the Context Embedding module with RCCA. For instance segmentation, we choose Mask-RCNN [72] as our baseline. For video semantic segmentation, we also choose Cityscapes pre-trained ResNet-101 [23] as our backbone network with 3D RCCA.

Training settings SGD with mini-batch is used for training.

For semantic segmentation, the initial learning rate is 1e-2

for Cityscapes and ADE20K. Following the prior works [10],

[14], we employ a poly learning rate policy where the initial

learning

rate

is

multiplied

by

1‚àí(

iter max iter

)power

with

power

= 0.9. We use the momentum of 0.9 and a weight decay of

0.0001. For Cityscapes, the training images are augmented

by randomly scaling (from 0.75 to 2.0), then randomly

cropping out high-resolution patches (769 √ó 769) from the

resulting images. Since the images from ADE20K are with

various sizes, we adopt an augmentation strategy of resizing

the short side of input image to a length randomly chosen

from the set {300, 375, 450, 525, 600}. For human parsing,

the model are trained and tested with the input size of

473 √ó 473. For instance segmentation, we take the same

training settings as that of Mask-RCNN [72]. For video

semantic segmentation, we sample 5 temporally ordered

frames from a training video as training data and the input

size is 504 √ó 504.

4.3 Experiments on Cityscapes
4.3.1 Comparisons with state-of-the-arts
Results of other state-of-the-art semantic segmentation solutions on Cityscapes are summarized in Tab. 1. For val set, we provide these results for reference and emphasize that these results should not be simply compared with our method, since these methods are trained on different (even larger) training sets or different basic network. Among these approaches, Deeplabv3 [12] adopts multi-scale testing strategy. Deeplabv3+ [28] and DPC [50] both use a more stronger backbone (i.e., Xception-65 & 71 vs. ResNet-101). In addition, DPC [50] makes use of additional dataset, i.e., COCO, for pre-training beyond the training set of Cityscapes. The results show that the proposed CCNet with single-scale testing still achieve comparable performance without bells and whistles.
Additionally, we also train the best learned CCNet with ResNet-101 as the backbone using both training and validation sets and make the evaluation on the test set by submitting our test results to the ofÔ¨Åcial evaluation server. Most of methods [10], [11], [16], [31], [32], [33], [36], [37], [59], [73] adopt the same backbone as ours and the others [49], [74] utilize stronger backbones. From Tab. 1, it can be observed that our CCNet substantially outperforms all the previous state-of-the-arts on test set. Among the approaches, PSANet [16] is the most related to our method which generates sub attention map for each pixel. One of the differences is that the sub attention map has 2 √ó H √ó W weights in PSANet and H + W ‚àí 1 weights in CCNet. Even with lower computation cost and memory usage, our method still achieves better performance.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

8

Image

Baseline

R=1

R=2

Fig. 6. Visualization results of RCCA with different loops on Cityscapes validation set.

Ground Truth

TABLE 2 Performance on Cityscapes (val) for different number of loops in RCCA.
FLOPs and memory increment are estimated for an input of 1 √ó 3 √ó 769 √ó 769.

Loops baseline R=1 R=2 R=3

GFLOPs( ) 0 8.3
16.5 24.7

Memory(M ) 0 53 127 208

mIOU(%) 75.1 78.0 79.8 80.2

4.3.2 Ablation studies
To verify the rationality of the CCNet, we conduct extensive ablation experiments on the validation set of Cityscapes with different settings for CCNet.
The effect of the RCCA module Tab. 2 shows the performance on the Cityscapes validation set by adopting different number of loop in RCCA. All experiments are conducted using ResNet-101 as the backbone. Besides, the input size of training images is 769 √ó 769 and the size of the input feature map H of RCCA is 97 √ó 97. Our baseline network is the ResNet-based FCN with dilated convolutional module incorporated at stage 4 and 5, i.e., dilation rates are set to 2 and 4 for these two stages respectively. The increment of FLOPs and memory usage are estimated when R = 1, 2, 3, respectively.
We observe that adding a criss-cross attention module into the baseline, donated as R = 1, improves the performance by 2.9%, which can effectively demonstrates the signiÔ¨Åcance of criss-cross attention. Furthermore, increasing the number of loops from 1 to 2 can further improve the performance by 1.8%, demonstrating the effectiveness of dense contextual information. Finally, increasing loops from 2 to 3 slightly improves the performance by 0.4%. Meanwhile, with the increasing the number of loops, the FLOPs and usage of GPU memory keep increasing. These results prove that the proposed criss-cross attention can signiÔ¨Åcantly improve the performance by capturing contextual information in horizontal and vertical direction. In addition,

TABLE 3 Performance on Cityscapes (val) for different kinds of category
consistent loss.

Function Type

Successes Mean mIOU(%)

Quadratic function 6/10

79.2

Piece-wise function 9/10

79.3

the proposed RCCA is effective in capturing the dense and global contextual information, which can Ô¨Ånally beneÔ¨Åt the performance of semantic segmentation. To balance the performance and resource usage, we choose R = 2 as default settings in all the following experiments.
To further validate the effectiveness of the criss-cross module, we provide the qualitative comparisons in Fig. 6. We leverage the white circles to indicate those challenging regions that are easily to be misclassiÔ¨Åed. It can be seen that these challenging regions are progressively corrected with the increasing the number of loops, which can well prove the effectiveness of dense contextual information aggregation for semantic segmentation.
The effect of the category consistent loss Tab. 4 also shows the performance on the Cityscapes validation set by adopting the proposed category consistent loss. The category consistent loss is donated as ‚ÄúCCL‚Äù in the table. As we can see, adopting the category consistent loss could stably bring 0Àú.7% mIoU gain with both Resnet-101 and Resnet50, which prove the effectiveness of the proposed category consistent loss for semantic segmentation. To prove that the proposed piece-wise function is more robust than the original one, we conduct 10 times of the training processes using ResNet-50 for each kind of loss function. The training is deemed to fail when the loss value is NaN, thus we can calculate the success rate (number of successful training / total number of training). The experimental results in Table 3 demonstrate that using the piece-wise function has higher training success rate than using the original one. Besides, using the piece-wise function could achieve slightly better performance than a single quadratic function. Because we

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

9

TABLE 4 Comparison of context aggregation approaches on Cityscapes (val).

Method ResNet101-Baseline ResNet101+GCN ResNet101+PSP ResNet101+ASPP ResNet101+NL ResNet101+RCCA(R=2) ResNet101+RCCA(R=2)+CCL
ResNet50-Baseline ResNet50+GCN ResNet50+PSP ResNet50+ASPP ResNet50+NL ResNet50+HV ResNet50+HV&VH ResNet50+RCCA(R=2) ResNet50+RCCA(R=2)+CCL

mIOU(%) 75.1 78.1 78.5 78.9 79.1 79.8 80.5
73.3 76.2 76.4 77.1 77.3 77.3 77.8 78.5 79.3

relax the punishment in the Eq. 6 to reduce the numerical values and gradients especially when the distance from the center exceeds Œ¥d. This relaxation makes the optimization much more stable.
Comparison of other context aggregation approaches We compare the performance of several different context aggregation approaches on the Cityscapes validation set with ResNet-50 and ResNet-101 as backbone networks.
SpeciÔ¨Åcally, the baselines of context aggregation mainly include: 1) Peng et al. [59] utilized global convolution Ô¨Ålters for contextual information aggregation, donated as ‚Äú+GCN‚Äù. 2) Zhao et al. [11] proposed Pyramid pooling which is the simple and effective way to capture global contextual information, donated as ‚Äú+PSP‚Äù; 3) Chen et al. [12] used different dilation convolutions to harvest pixel-wise contextual information at the different range, donated as ‚Äú+ASPP‚Äù; 4) Wang et al. [9] introduced non-local network for context aggregation, donated as ‚Äú+NL‚Äù.
In Tab. 4, both ‚Äú+NL‚Äù and ‚Äú+RCCA‚Äù achieve better performance compared with the other context aggregation approaches, which demonstrates the importance of capturing full-image contextual information. More interestingly, our method achieves better performance than ‚Äú+NL‚Äù. This reason may be attributed to the sequentially recurrent operation of criss-cross attention. Concretely, ‚Äú+NL‚Äù generates an attention map directly from the feature which has limit receptive Ô¨Åeld and short-range dependencies. In contrast, our ‚Äú+RCCA‚Äù takes two steps to form dense contextual information, leading to that the latter step can learn a better attention map beneÔ¨Åting from the feature map produced by the Ô¨Årst step in which some long-range dependencies has already been embedded.
To prove the effectiveness of attention with criss-cross shape, we compare criss-cross shape with other shapes in Tab. 4. ‚Äú+HV‚Äù means stacking horizontal attention and vertical attention. ‚Äú+HV&VH‚Äù means summing up features

TABLE 5 Comparison of Non-local module and RCCA. FLOPs and memory
increment are estimated for an input of 1 √ó 3 √ó 769 √ó 769.

Method

GFLOPs( ) Memory(M ) mIOU(%)

baseline

0

0

73.3

+NL

108

1411

77.3

+NL(R=2)

216

2820

78.7

+RCCA(R=2) 16.5

127

78.5

of two parallel branches, i.e. ‚ÄúHV‚Äù and ‚ÄúVH‚Äù. We further explore the amount of computation and
memory footprint of RCCA. As shown in Tab. 5, compared with ‚Äú+NL‚Äù method, the proposed ‚Äú+RCCA‚Äù requires 11√ó less GPU memory usage and signiÔ¨Åcantly reduces FLOPs by about 85% of non-local block in computing full-image dependencies, which shows that CCNet is an efÔ¨Åcient way to capture full-image contextual information in the least amount of computation and memory footprint. To further prove the effectiveness of the recurrent operation, we also run non-local module in the recurrent way, donated as ‚Äú+NL(R=2)‚Äù. As we can seen, the recurrent operation can bring more than 1 point gain. Because the recurrent operation leads to that the latter step can learn a better attention map beneÔ¨Åting from the feature map produced by the Ô¨Årst step in which some long-range dependencies has already been embedded. However, compared with ‚Äú+RCCA‚Äù, ‚Äú+NL(R=2)‚Äù needs huge GPU memory usage, which limits the use of self-attention.
Visualization of Attention Map To get a deeper understanding of our RCCA, we visualize the learned attention masks as shown in Fig. 7. For each input image, we select one point (cross in green) and show its corresponding attention maps when R = 1 and R = 2 in columns 2 and 3, respectively. It can be observed that only contextual information from the criss-cross path of the target point is captured when R = 1. By adopting one more criss-cross module, i.e., R = 2, RCCA can Ô¨Ånally aggregate denser and richer contextual information compared with that of R = 1. Besides, we observe that the attention module could capture semantic similarity and full-image dependencies.
4.4 Experiments on ADE20K
In this subsection, we conduct experiments on the AED20K dataset, which is a very challenging scene parsing dataset. As shown in Tab. 6, CCNet with CCL achieves the stateof-the-art performance of 45.76%, outperforms the previous state-of-the-art methods by more than 1.1% and also outperforms the conference version CCNet by 0.5%. Some successful segmentation results are given in Fig 8. Among the approaches, most of methods [11], [14], [16], [33], [75], [76] adopt the ResNet-101 as backbone and ReÔ¨ÅneNet [31] adopts a more powerful network, i.e., ResNet-152, as the backbone. EncNet [14] achieves previous best performance among the methods and utilizes global pooling with imagelevel supervision to collect image-level context information. In contrast, our CCNet adopts an alternative way to integrate contextual information by capture full-image dependencies and achieve better performance.

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

10

Image

R=1

R=2

Ground Truth

Fig. 7. Visualization of attention module on Cityscapes validation set. The left column is the input images, the 2 and 3 columns are pixel-wise attention maps when R = 1 and R = 2 in RCCA.

Image

CCNet

+CCL

Ground Truth

Fig. 8. Visualized examples on ADE20K val set with/without category consistent loss (CCL).

4.5 Experiments on LIP

In this subsection, we conduct experiments on the LIP dataset, which is a very challenging human parsing dataset. The framework of CE2P [41] is utilized, with ImageNet pretrained ResNet-101 as bockbone and using RCCA (R=2) rather than PSP [11] as context embedding module. The category consistent loss is used to boost the performance. The hyper-parameter setting strictly follows that in the CE2P [41]. Among the approaches, Deeplab (VGG-16) [25], Attention [51] and SAN [42] adopt the VGG-16 as backbone and Deeplab (ResNet-101) [10], JPPNet [21], CE2P [41] and CCNet adopt ResNet-101 as the backbone. As shown in Tab. 7, CCNet achieves the state-of-the-art performance of 55.47%, outperforms the previous state-of-the-art methods by more than 2.3%. This signiÔ¨Åcant improvement demonstrates the effectiveness of proposed method on human parsing task. Fig. 9 shows some visualized segmentation results. The top two rows show some successful segmentation results It shows our method can produce accurate

Image

CE2P

CCNet

Ground Truth

Fig. 9. Visualized examples for human parsing result on LIP val set.
segmentation even for complicated poses. The third row shows a failure segmentation result where the ‚Äúskirt‚Äù is misclassiÔ¨Åed as ‚Äúpants‚Äù. But it‚Äôs difÔ¨Åcult to recognize even for humans.

4.6 Experiments on COCO
To further demonstrate the generality of CCNet, we conduct the instance segmentation task on COCO [70] using the competitive Mask R-CNN model [72] as the baseline. Following [9], we modify the Mask R-CNN backbone by adding the RCCA module right before the last convolutional residual block of res4. We evaluate a standard baseline of ResNet-50/101. All models are Ô¨Åne-tuned from ImageNet

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

11

Image

Baseline

+RCCA

Image

Fig. 10. Visualized examples for instance segmentation result on COCO val set.

Baseline

+RCCA

pre-training. We use the ofÔ¨Åcial implementation1 with endto-end joint training whose performance is almost the same as the baseline reported in [9]. For fair comparison, we do not use the category consistent loss in our method. We report the results in terms of box AP and mask AP in Tab. 8 on COCO. The results demonstrate that our method substantially outperforms the baseline in all metrics. Some segmentation results for comparing baseline with ‚Äú+RCCA‚Äù are given in Fig 10. Meanwhile, the network with ‚Äú+RCCA‚Äù also achieves the better performance than the network with one non-local block ‚Äú+NL‚Äù.
4.7 Experiments on CamVid
To further demonstrate the effectiveness of 3D-RCCA, we carry out the experiments on CamVid [71], which is one of the Ô¨Årst datasets focusing on video semantic segmentation for driving scenarios. We follow the standard protocol proposed in [77] to split the dataset into 367 training, 101 validation and 233 test images. For fair comparison, we only report single-scale evaluation scores. As can be seen in Tab. 9, we achieve an mIoU of 79.1%, outperforming all other methods by a large margin.
To demonstrate the effectiveness of our proposed techniques, we perform training under the same settings with the different length of input frames. We apply the CNNs on each frame for extracting features and then concatenate and reshape them to satisfy the required shape of 3D CrissCoss Attention module. We use the R = 3 for collecting dense spatial and temporal contextual information. Here, to make a training sample, we try two kinds of length (T ) of input frames. For T = 1, we randomly sample 1 frame from a training video, donated as ‚ÄúCCNet3D (T = 1)‚Äù. For T = 5, we sample 5 temporally ordered frames from a training video, donated as ‚ÄúCCNet3D (T = 5)‚Äù. As can be seen in Tab. 9, ‚ÄúCCNet3D (T = 5)‚Äù outperforms ‚ÄúCCNet3D (T = 1)‚Äù by 1.2%.
5 CONCLUSION AND FUTURE WORK
In this paper, we have presented a Criss-Cross Network (CCNet) for deep learning based dense prediction tasks,
1. https://github.com/facebookresearch/maskrcnn-benchmark

TABLE 6 Comparison with state-of-the-arts on ADE20K (val).

Method ReÔ¨ÅneNet [31] SAC [33] PSPNet [11] PSANet [16] DSSPN [75] UperNet [76] EncNet [14] CCNet

Backbone ResNet-152 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101

mIOU(%) 40.70 44.30 43.29 43.77 43.68 42.66 44.65 45.76

TABLE 7 Comparison with state-of-the-arts on LIP (val).

Method DeepLab (VGG-16) [10] Attention [51] SAN [42] DeepLab (ResNet-101) [10] JPPNet [21] CE2P [41] CCNet

pixel acc 82.66 83.43 84.22 84.09 86.39 87.37 88.01

mean acc 51.64 54.39 55.09 55.63 62.32 63.20 63.91

mIoU 41.64 42.92 44.81 44.80 51.37 53.10 55.47

TABLE 8 Comparison on COCO (val).

Method

baseline

R50

+NL

+RCCA

baseline

R101 +NL

+RCCA

APbox 38.2 39.0 39.3 40.1 40.8 41.0

APmask 34.8 35.5 36.1 36.2 37.1 37.3

which adaptively captures contextual information on the criss-cross path. To obtain dense contextual information, we introduce RCCA which aggregates contextual information

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

12

TABLE 9 Results on the CamVid test set.

Method

Bockbone

mIoU (%)

SegNet [77]

VGG16

60.1

RTA [40]

VGG16

62.5

Dilate8 [26]

Dilate

65.3

BiSeNet [37]

ResNet18

68.7

PSPNet [11]

ResNet50

69.1

DenseDecoder [38] ResNeXt101

70.9

VideoGCRF‚Ä° [39]

ResNet101

75.2

CCNet3D (T=1) ‚Ä°

ResNet101

77.9

CCNet3D (T=5) ‚Ä°

ResNet101

79.1

‚Ä° the initialized model is pre-trained on Cityscapes.

from all pixels. The experiments demonstrate that RCCA captures full-image contextual information in less computation cost and less memory cost. Besides, to learn discriminative features, we introduce the category consistent loss. Our CCNet achieves outstanding performance consistently on several semantic segmentation datasets, i.e., Cityscapes, ADE20K, LIP, CamVid and instance segmentation dataset, i.e., COCO. The source codes of CCNet are released to facilitate related research and applications.
ACKNOWLEDGEMENTS
This work was in part supported by NSFC (No. 61733007 and No. 61876212), ARC DECRA DE190101315, ARC DP200100938, HUST-Horizon Computer Vision Research Center, and IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM AI Horizons Network.
REFERENCES
[1] J. Fritsch, T. Kuehnl, and A. Geiger, ‚ÄúA new performance measure and evaluation benchmark for road detection algorithms,‚Äù in ITSC, 2013, pp. 1693‚Äì1700. 1
[2] R. T. Azuma, ‚ÄúA survey of augmented reality,‚Äù Presence: Teleoperators & Virtual Environments, vol. 6, no. 4, pp. 355‚Äì385, 1997. 1
[3] M. Evening, Adobe Photoshop CS3 for photographers: a professional image editor‚Äôs guide to the creative use of Photoshop for the Macintosh and PC. Focal press, 2012. 1
[4] Y. Song, Z. Huang, C. Shen, H. Shi, and D. A. Lange, ‚ÄúDeep learning-based automated image segmentation for concrete petrographic analysis,‚Äù Cement and Concrete Research, vol. 135, p. 106118, 2020. 1
[5] Z. Zheng, Y. Zhong, J. Wang, and A. Ma, ‚ÄúForeground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery,‚Äù in CVPR, 2020. 1
[6] M. T. Chiu, X. Xu, Y. Wei, Z. Huang, A. G. Schwing, R. Brunner, H. Khachatrian, H. Karapetyan, I. Dozier, G. Rose et al., ‚ÄúAgriculture-vision: A large aerial image database for agricultural pattern analysis,‚Äù in CVPR, 2020, pp. 2828‚Äì2838. 1
[7] M. Tik Chiu, X. Xu, K. Wang, J. Hobbs, N. Hovakimyan, S. H. Huang, Thomas S et al., ‚ÄúThe 1st agriculture-vision challenge: Methods and results,‚Äù in CVPR Workshops, 2020, pp. 48‚Äì49. 1
[8] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks for semantic segmentation,‚Äù in CVPR, 2015, pp. 3431‚Äì3440. 1, 3
[9] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural networks,‚Äù in CVPR, 2018, pp. 7794‚Äì7803. 1, 2, 3, 9, 10, 11
[10] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, ‚ÄúDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,‚Äù IEEE TPAMI, vol. 40, no. 4, pp. 834‚Äì848, 2018. 1, 2, 3, 4, 7, 10, 11

[11] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, ‚ÄúPyramid scene parsing network,‚Äù in CVPR, 2017, pp. 2881‚Äì2890. 2, 3, 7, 9, 10, 11, 12
[12] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, ‚ÄúRethinking atrous convolution for semantic image segmentation,‚Äù arXiv preprint arXiv:1706.05587, 2017. 2, 7, 9
[13] H. Ding, X. Jiang, B. Shuai, A. Qun Liu, and G. Wang, ‚ÄúContext contrasted feature and gated multi-scale aggregation for scene segmentation,‚Äù in CVPR, June 2018. 2
[14] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, ‚ÄúContext encoding for semantic segmentation,‚Äù in CVPR, 2018. 2, 7, 9, 11
[15] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, ‚ÄúThe graph neural network model,‚Äù IEEE TNN, vol. 20, no. 1, pp. 61‚Äì80, 2008. 2, 3
[16] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, ‚ÄúPsanet: Point-wise spatial attention network for scene parsing,‚Äù in ECCV, 2018, pp. 270‚Äì286. 2, 3, 7, 9, 11
[17] J. Cheng, L. Dong, and M. Lapata, ‚ÄúLong short-term memorynetworks for machine reading,‚Äù 2016. 2
[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in NeurIPS, 2017, pp. 5998‚Äì6008. 2, 3
[19] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset for semantic urban scene understanding,‚Äù in CVPR, 2016, pp. 3213‚Äì3223. 2, 6
[20] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, ‚ÄúScene parsing through ade20k dataset,‚Äù in CVPR, 2017. 2, 6
[21] X. Liang, K. Gong, X. Shen, and L. Lin, ‚ÄúLook into person: Joint body parsing & pose estimation network and a new benchmark,‚Äù IEEE TPAMI, vol. 41, no. 4, pp. 871‚Äì885, 2018. 2, 3, 6, 10, 11
[22] G. J. Brostow, J. Fauqueur, and R. Cipolla, ‚ÄúSemantic object classes in video: A high-deÔ¨Ånition ground truth database,‚Äù Pattern Recognition Letters, vol. 30, no. 2, pp. 88‚Äì97, 2009. 2
[23] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image recognition,‚Äù in CVPR, 2016, pp. 770‚Äì778. 2, 7
[24] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, ‚ÄúCcnet: Criss-cross attention for semantic segmentation,‚Äù in ICCV, 2019. 2
[25] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, ‚ÄúSemantic image segmentation with deep convolutional nets and fully connected crfs,‚Äù ICLR, 2015. 3, 7, 10
[26] F. Yu and V. Koltun, ‚ÄúMulti-scale context aggregation by dilated convolutions,‚Äù ICLR, 2016. 3, 12
[27] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional networks for biomedical image segmentation,‚Äù in International Conference on Medical image computing and computer-assisted intervention, 2015, pp. 234‚Äì241. 3
[28] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, ‚ÄúEncoder-decoder with atrous separable convolution for semantic image segmentation,‚Äù ECCV, 2018. 3, 7
[29] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang, ‚ÄúMultiscale context intertwining for semantic segmentation,‚Äù in ECCV, 2018, pp. 603‚Äì619. 3
[30] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. Huang, W.-M. Hwu, and H. Shi, ‚ÄúSpgnet: Semantic prediction guidance for scene parsing,‚Äù in ICCV, 2019. 3
[31] G. Lin, A. Milan, C. Shen, and I. D. Reid, ‚ÄúReÔ¨Ånenet: Multi-path reÔ¨Ånement networks for high-resolution semantic segmentation.‚Äù in CVPR, 2017. 3, 7, 9, 11
[32] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, ‚ÄúLearning a discriminative feature network for semantic segmentation,‚Äù CVPR, 2018. 3, 7
[33] R. Zhang, S. Tang, Y. Zhang, J. Li, and S. Yan, ‚ÄúScale-adaptive convolutions for scene parsing,‚Äù in ICCV, 2017, pp. 2031‚Äì2039. 3, 7, 9, 11
[34] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, ‚ÄúDeformable convolutional networks,‚Äù ICCV, 2017. 3
[35] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang, ‚ÄúSemantic image segmentation via deep parsing network,‚Äù in ICCV, 2015, pp. 1377‚Äì 1385. 3
[36] T.-W. Ke, J.-J. Hwang, Z. Liu, and S. X. Yu, ‚ÄúAdaptive afÔ¨Ånity Ô¨Åeld for semantic segmentation,‚Äù ECCV, 2018. 3, 7
[37] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, ‚ÄúBisenet: Bilateral segmentation network for real-time semantic segmentation,‚Äù ECCV, 2018. 3, 7, 12

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, JULY 2020

13

[38] P. Bilinski and V. Prisacariu, ‚ÄúDense decoder shortcut connections for single-pass semantic segmentation,‚Äù in CVPR, 2018, pp. 6596‚Äì 6605. 3, 12
[39] S. Chandra, C. Couprie, and I. Kokkinos, ‚ÄúDeep spatio-temporal random Ô¨Åelds for efÔ¨Åcient video segmentation,‚Äù in CVPR, 2018, pp. 8915‚Äì8924. 3, 12
[40] P.-Y. Huang, W.-T. Hsu, C.-Y. Chiu, T.-F. Wu, and M. Sun, ‚ÄúEfÔ¨Åcient uncertainty estimation for semantic segmentation in videos,‚Äù in ECCV, 2018, pp. 520‚Äì535. 3, 12
[41] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, ‚ÄúDevil in the details: Towards accurate single and multiple human parsing,‚Äù in AAAI, vol. 33, 2019, pp. 4814‚Äì4821. 3, 7, 10, 11
[42] Z. Huang, C. Wang, X. Wang, W. Liu, and J. Wang, ‚ÄúSemantic image segmentation by scale-adaptive networks,‚Äù IEEE TIP, 2019. 3, 10, 11
[43] Z. Wang, M. Yu, Y. Wei, R. Feris, J. Xiong, W.-m. Hwu, T. S. Huang, and H. Shi, ‚ÄúDifferential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation,‚Äù in CVPR, 2020, pp. 12 635‚Äì12 644. 3
[44] Z. Wang, Y. Wei, R. Feris, J. Xiong, W.-M. Hwu, T. S. Huang, and H. Shi, ‚ÄúAlleviating semantic-level shift: A semi-supervised domain adaptation method for semantic segmentation,‚Äù in CVPR Workshops, 2020, pp. 936‚Äì937. 3
[45] J. Jiao, Y. Wei, Z. Jie, H. Shi, R. W. Lau, and T. S. Huang, ‚ÄúGeometry-aware distillation for indoor semantic segmentation,‚Äù in CVPR, 2019, pp. 2869‚Äì2878. 3
[46] Z. Huang, X. Wang, J. Wang, W. Liu, and J. Wang, ‚ÄúWeaklysupervised semantic segmentation network with deep seeded region growing,‚Äù in CVPR, 2018, pp. 7014‚Äì7023. 3
[47] Y. Wei, H. Xiao, H. Shi, Z. Jie, J. Feng, and T. S. Huang, ‚ÄúRevisiting dilated convolution: A simple approach for weakly-and semisupervised semantic segmentation,‚Äù in CVPR, 2018, pp. 7268‚Äì7277. 3
[48] R. Qian, Y. Wei, H. Shi, J. Li, J. Liu, and T. Huang, ‚ÄúWeakly supervised scene parsing with point-based distance metric learning,‚Äù in AAAI, vol. 33, 2019, pp. 8843‚Äì8850. 3
[49] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, ‚ÄúDenseaspp for semantic segmentation in street scenes,‚Äù in CVPR, 2018, pp. 3684‚Äì 3692. 3, 7
[50] L.-C. Chen, M. D. Collins, Y. Zhu, G. Papandreou, B. Zoph, F. Schroff, H. Adam, and J. Shlens, ‚ÄúSearching for efÔ¨Åcient multiscale architectures for dense image prediction,‚Äù NeurIPS, 2018. 3, 7
[51] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, ‚ÄúAttention to scale: Scale-aware semantic image segmentation,‚Äù in CVPR, 2016, pp. 3640‚Äì3649. 3, 10, 11
[52] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei, ‚ÄúAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation,‚Äù in CVPR, 2019, pp. 82‚Äì92. 3
[53] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, ‚ÄúAdaptive pyramid context network for semantic segmentation,‚Äù in CVPR, 2019, pp. 7519‚Äì7528. 3
[54] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and J. Kautz, ‚ÄúLearning afÔ¨Ånity via spatial propagation networks,‚Äù in NeurIPS, 2017, pp. 1520‚Äì1530. 3
[55] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. Torr, ‚ÄúConditional random Ô¨Åelds as recurrent neural networks,‚Äù in ICCV, 2015, pp. 1529‚Äì1537. 3
[56] Y. Yuan and J. Wang, ‚ÄúOcnet: Object context network for scene parsing,‚Äù arXiv preprint arXiv:1809.00916, 2018. 3
[57] J. Fu, J. Liu, H. Tian, Z. Fang, and H. Lu, ‚ÄúDual attention network for scene segmentation,‚Äù CVPR, 2019. 3
[58] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, and Y. Kalantidis, ‚ÄúGraph-based global reasoning networks,‚Äù in CVPR, 2019, pp. 433‚Äì442. 3
[59] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, ‚ÄúLarge kernel mattersimprove semantic segmentation by global convolutional network,‚Äù in CVPR, 2017, pp. 1743‚Äì1751. 3, 7, 9
[60] A. Sperduti and A. Starita, ‚ÄúSupervised neural networks for the classiÔ¨Åcation of structures,‚Äù IEEE TNN, vol. 8, no. 3, pp. 714‚Äì735, 1997. 3
[61] M. Gori, G. Monfardini, and F. Scarselli, ‚ÄúA new model for learning in graph domains,‚Äù in IJCNN, vol. 2, 2005, pp. 729‚Äì734. 3
[62] M. Henaff, J. Bruna, and Y. LeCun, ‚ÄúDeep convolutional networks on graph-structured data,‚Äù arXiv preprint arXiv:1506.05163, 2015. 3

[63] M. Defferrard, X. Bresson, and P. Vandergheynst, ‚ÄúConvolutional neural networks on graphs with fast localized spectral Ô¨Åltering,‚Äù in NeurIPS, 2016, pp. 3844‚Äì3852. 3
[64] T. N. Kipf and M. Welling, ‚ÄúSemi-supervised classiÔ¨Åcation with graph convolutional networks,‚Äù arXiv preprint arXiv:1609.02907, 2016. 3
[65] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, ‚ÄúCayleynets: Graph convolutional neural networks with complex rational spectral Ô¨Ålters,‚Äù IEEE TSP, vol. 67, no. 1, pp. 97‚Äì109, 2018. 3
[66] J. Atwood and D. Towsley, ‚ÄúDiffusion-convolutional neural networks,‚Äù in NeurIPS, 2016, pp. 1993‚Äì2001. 3
[67] M. Niepert, M. Ahmed, and K. Kutzkov, ‚ÄúLearning convolutional neural networks for graphs,‚Äù in ICML, 2016, pp. 2014‚Äì2023. 3
[68] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, ‚ÄúNeural message passing for quantum chemistry,‚Äù in ICML, 2017, pp. 1263‚Äì1272. 3
[69] B. De Brabandere, D. Neven, and L. Van Gool, ‚ÄúSemantic instance segmentation with a discriminative loss function,‚Äù arXiv preprint arXiv:1708.02551, 2017. 5, 6
[70] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla¬¥r, and C. L. Zitnick, ‚ÄúMicrosoft coco: Common objects in context,‚Äù in ECCV, 2014, pp. 740‚Äì755. 6, 10
[71] G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla, ‚ÄúSegmentation and recognition using structure from motion point clouds,‚Äù in ECCV, 2008, pp. 44‚Äì57. 6, 11
[72] K. He, G. Gkioxari, P. Dolla¬¥r, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in ICCV, 2017, pp. 2980‚Äì2988. 7, 10
[73] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell, ‚ÄúUnderstanding convolution for semantic segmentation,‚Äù in WACV, 2018, pp. 1451‚Äì1460. 7
[74] Z. Wu, C. Shen, and A. v. d. Hengel, ‚ÄúWider or deeper: Revisiting the resnet model for visual recognition,‚Äù arXiv preprint arXiv:1611.10080, 2016. 7
[75] X. Liang, H. Zhou, and E. Xing, ‚ÄúDynamic-structured semantic propagation network,‚Äù in CVPR, 2018, pp. 752‚Äì761. 9, 11
[76] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, ‚ÄúUniÔ¨Åed perceptual parsing for scene understanding,‚Äù ECCV, 2018. 9, 11
[77] V. Badrinarayanan, A. Kendall, and R. Cipolla, ‚ÄúSegnet: A deep convolutional encoder-decoder architecture for image segmentation,‚Äù IEEE TPAMI, vol. 39, no. 12, pp. 2481‚Äì2495, 2017. 11, 12

