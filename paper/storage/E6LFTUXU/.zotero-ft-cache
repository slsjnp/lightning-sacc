1

Transformers in Vision: A Survey

Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah

arXiv:2101.01169v2 [cs.CV] 22 Feb 2021

Abstract—Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient beneﬁts, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classiﬁcation, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classiﬁcation and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges towards the application of transformer models in computer vision.
Index Terms—Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision.
!

1 INTRODUCTION

T RANSFORMER models [1] have recently demonstrated exemplary performance on a broad range of language tasks e.g., text classiﬁcation, machine translation [2] and question answering. Among these models, the most popular ones include BERT (Bidirectional Encoder Representations from Transformers) [3], GPT (Generative Pre-trained Transformer) v1-3 [4]–[6], RoBERTa (Robustly Optimized BERT Pre-training) [7] and T5 (Text-to-Text Transfer Transformer) [8]. The profound impact of Transformer models has become more clear with their scalability to very large capacity models [9], [10]. For example, the BERT-large [3] model with 340 million parameters was signiﬁcantly outperformed by the GPT-3 [6] model with 175 billion parameters while the latest mixture-of-experts Switch transformer [10] scales up to a whopping 1.6 trillion parameters!
The breakthroughs from Transformer networks in Natural Language Processing (NLP) domain has sparked great interest in the computer vision community to adapt these models for vision and multi-modal learning tasks (Fig. 1).
• S. Khan, M. Naseer and F. S. Khan are with the MBZ University of Artiﬁcial Intelligence, Abu Dhabi, UAE. E-mail: ﬁrstname.lastname@mbzuai.ac.ae
• M. Hayat is with the Faculty of IT, Monash University, Clayton VIC 3800, Australia.
• S. W. Zamir is with the Inception Institute of Artiﬁcial Intelligence, Abu Dhabi, UAE.
• S. Khan and M. Naseer are also with the CECS, Australian National University, Canberra ACT 0200, Australia.
• F. S. Khan is also with the Computer Vision Laboratory, Linko¨ping University, Sweden.
• M. Shah is with the Center for Research in Computer Vision, University of Central Florida, Orlando, FL 32816, United States.
Manuscript received March, 2021.

However, visual data follows a typical structure (e.g., spatial and temporal coherence), thus demanding novel network designs and training schemes. As a result, Transformer models and their variants have been successfully used for image recognition [11], [12], object detection [13], [14], segmentation [15], image super-resolution [16], video understanding [17], [18], image generation [19], text-image synthesis [20] and visual question answering [21], [22], among several other use cases [23]–[26]. This survey aims to cover such recent and exciting efforts in the computer vision domain, providing a comprehensive reference to interested readers.
Transformer architectures are based on a self-attention mechanism that learns the relationships between elements of a sequence. As opposed to recurrent networks that process sequence elements recursively and can only attend to short-term context, transformer architectures can attend to complete sequences thereby learning long-range relationships and can be easily parallelized. An important feature of these models is their scalability to very high-complexity models and large-scale datasets. Since Transformers assume minimal prior knowledge about the structure of the problem as compared to their convolutional and recurrent counterparts in deep learning [27]–[29], they are typically pre-trained using pretext tasks on large-scale (unlabelled) datasets [1], [3]. Such a pre-training avoids costly manual annotations, thereby encoding highly expressive and generalizable representations that model rich relationships between the entities present in a given dataset. The learned representations are then ﬁne-tuned on the downstream tasks in a supervised manner to obtain favorable results.
This paper provides a holistic overview of the transformer models developed for computer vision applications.

2

Fig. 1: Statistics on the number of times keywords such as BERT, Self-Attention, and Transformers appear in the titles of Peerreviewed and arXiv papers over the past few years (in Computer Vision and Machine Learning). The plots show consistent growth in recent literature. This survey covers recent progress on Transformers in the computer vision domain.

We develop a taxonomy of the network design space and highlight the major strengths and shortcomings of the existing methods. Other literature reviews mainly focus on the NLP domain [30], [31] or cover generic attention-based approaches [30], [32]. By focusing on the newly emerging area of visual transformers, we comprehensively organize the recent approaches according to the intrinsic features of self-attention and the investigated task. We ﬁrst provide an introduction to the salient concepts underlying Transformer networks and then elaborate on the speciﬁcs of recent vision transformers. Where ever possible, we draw parallels between the Transformers used in the NLP domain [1] and the ones developed for vision problems to ﬂash major novelties and interesting domain-speciﬁc insights. Recent approaches show that convolution operations can be fully replaced with attention-based transformer modules and have also been used jointly in a single design to encourage symbiosis between the two complementary set of operations. This survey ﬁnally details open research questions with an outlook towards the possible future work.
2 FOUNDATIONS
There exist two key ideas that have contributed towards the development of transformer models. (a) The ﬁrst one is selfattention, which allows capturing ‘long-term’ information and dependencies between sequence elements as compared to conventional recurrent models that ﬁnd it challenging to encode such relationships. (b) The second key idea is that of pre-training on a large (un)labelled corpus in a (un)supervised manner, and subsequently ﬁne-tuning to the target task with a small labeled dataset [3], [7], [33]. Below, we provide a brief tutorial on these two ideas (Sec. 2.2 and 2.1), along with a summary of seminal Transformer networks (Sec. 2.3 and 2.4) where these ideas have been applied. This background will help us better understand the forthcoming Transformer based models used in the computer vision domain (Sec. 3).
2.1 Self-Attention
Given a sequence of items, self-attention mechanism estimates the relevance of one item to other items (e.g., which words are likely to come together in a sentence). The selfattention mechanism is an integral component of Transformers, which explicitly models the interactions between

Fig. 2: An example self-attention block used in the vision domain [34]. Given the input sequence of convolutional features from an image, the triplet of (key, query, value) is calculated followed by attention calculation and applying it to reweight the values. Note that a single head is shown here and an output projection (W) is ﬁnally applied to obtain output features with the same dimension as the input. Figure adapted from [34].

all entities of a sequence for structured prediction tasks.
Basically, a self-attention layer updates each component
of a sequence by aggregating global information from the
complete input sequence.
Lets denote a sequence of n entities (x1, x2, · · · xn) by X ∈ Rn×d, where d is the embedding dimension to repre-
sent each entity. The goal of self-attention is to capture the interaction amongst all n entities by encoding each entity
in terms of the global contextual information. This is done
by deﬁning three learnable weight matrices to transform Queries (WQ ∈ Rn×dq ), Keys (WK ∈ Rn×dk ) and Values (WV ∈ Rn×dv ). The input sequence X is ﬁrst projected onto these weight matrices to get Q = XWQ, K = XWK and V = XWV . The output Z ∈ Rn×dv of the self attention
layer is then given by,

QKT

Z = softmax

V.

dq

For a given entity in the sequence, the self-attention basically computes the dot-product of the query with all keys, which is then normalized using softmax operator to get the attention scores. Each entity then becomes the weighted sum of all entities in the sequence, where weights are given by the attention scores (Fig. 2 and Fig. 3, top row-left block).
Masked Self-Attention: The standard self-attention layer attends to all entities. For the Transformer model [1] which is trained to predict the next entity of the sequence,

3

Fig. 3: Architecture of the Transformer Model [1]. The model was ﬁrst developed for the language translation task where an input sequence in one language is required to be converted to the output sequence in another language. The Transformer encoder (shown in the middle row) operates on the input language sequence and converts it to an embedding before passing it on to the encoder blocks. The Transformer decoder (shown in the bottom row) operates on the previously generated outputs in the translated language and the encoded input sequence from the middle branch to output the next word in the output sequence. The sequence of previous outputs (used as input to the decoder) is obtained by shifting the output sentence to the right by one position and appending start-of-sentence token at the beginning. This shifting avoids the model to learn to simply copy the decoder input to the output. The ground-truth used to train the model is simply the output language sequence (without any right shift) appended with an end-of-sentence token. The blocks consisting of multi-head attention (shown in the top-most row) and feed-forward layers are repeated N × in both the encoder and decoder as indicated on the top right corner.

the self-attention blocks used in the decoder are masked to prevent attending to the subsequent future entities. This is simply done by an element-wise multiplication operation with a mask M ∈ Rn×n, where M is an upper-triangular matrix. The masked self-attention is deﬁned by,

QKT

softmax

◦M ,

dq

where ◦ denotes Hadamard product. Basically, while predicting an entity in the sequence, the attention scores of the future entities are set to zero in masked self-attention.
Multi-Head Attention: In order to encapsulate multiple complex relationships amongst different elements in the sequence, the multi-head attention comprises multiple selfattention blocks (h = 8 in the original Transformer model [1]). Each block has its own set of learnable weight matrices {WQi , WKi , WVi }, where i = 0 · · · (h−1). For an input X, the output of the h self-attention blocks in multihead attention is then concatenated into a single matrix [Z0, Z1, · · · Zh−1] ∈ Rn×h·dv and projected onto a weight matrix W ∈ Rh·dv×d (Fig. 3, top row).
The main difference of self-attention with convolution operation is that the weights are dynamically calculated instead of static weights (that stay the same for any input) as in the case of convolution. Further, self-attention is invariant to permutations and changes in the number of input points. As a result, it can easily operate on irregular inputs as opposed to standard convolution that requires grid structure.

2.2 (Un)Supervised Pre-training
Self-attention based Transformer models generally operate in a two-stage training mechanism. First, pre-training is performed on a large-scale dataset (and sometimes a combination of several available datasets [22], [35]) in either a supervised [11] or an unsupervised manner [3], [36], [37]. Later, the pre-trained weights are adapted to the downstream tasks using small-mid scale datasets. Examples of downstream tasks include image classiﬁcation [38], object detection [13], zero-shot learning [20], question-answering [10] and action recognition [18]. The effectiveness of pretraining for large-scale Transformers has been advocated in both the language and vision domains. For example, Vision Transformer model (ViT-L) [11] experiences an absolute 13% drop in accuracy on ImageNet test set when trained only on ImageNet train set as compared to the case when pretrained on JFT dataset [39] with 300 million images.
Since acquiring manual labels at a massive scale is cumbersome, self-supervised learning has been very effectively used in the pre-training stage. The self-supervision based pre-training stage training has played a crucial role in unleashing the scalability and generalization of Transformer networks, enabling training even above a trillion parameter networks (e.g., the latest Switch Transformer [10] from Google). An extensive survey on SSL can be found in [40], [41]. As nicely summarized by Y. LeCun [42], the basic idea of SSL is to ﬁll in the blanks, i.e., try to predict the occluded data in images, future or past frames in temporal video sequences or predict a pretext task e.g., the amount of rotation applied to inputs, the permutation applied to image patches or the color of a gray-scale image. Another

4

effective way to impose self-supervised constraints is via contrastive learning. In this case, nuisance transformations are used to create two types of modiﬁed versions of the same image i.e., without changing the underlying class semantics (e.g., image stylizing, cropping) and with semantic changes (e.g., replacing an object with another in the same scene, or changing the class with minor adversarial changes to the image). Subsequently, the model is trained to be invariant to the nuisance transformations and emphasize on modeling minor changes that can alter semantic labels.
Self-supervised learning provides a promising learning paradigm since it enables learning from a vast amount of readily available non-annotated data. In the SSL based pretraining stage, a model is trained to learn a meaningful representation of the underlying data by solving a pretext task. The pseudo-labels for the pretext task are automatically generated (without requiring any expensive manual annotations) based on data attributes and task deﬁnition. Therefore, the pretext task deﬁnition is a critical choice in SSL. We can broadly categorize existing SSL methods based upon their pretext tasks into (a) generative approaches which synthesize images or videos (given conditional inputs), (b) context-based methods which exploit the relationships between image patches or video frames, and (c) cross-modal methods which leverage from multiple data modalities. Examples of generative approaches include conditional generation tasks such as masked image modeling [35] and image colorization [43], image super-resolution [44], image in-painting [45], and GANs based methods [46], [47]. The context-based pretext methods solve problems such as a jigsaw puzzle on image patches [48]–[50], masked object classiﬁcation [22], predict geometric transformation such as rotation [38], [51], or verify temporal sequence of video frames [52]–[54]. Cross-modal pretext methods verify the correspondence of two input modalities e.g., text & image [55], audio & video [56], [57] or RGB & ﬂow [58].
2.3 Transformer Model
The architecture of the Transformer model proposed in [1] is shown in Fig. 3. It has an encoder-decoder structure. The encoder (middle row) consists of six identical blocks (i.e., N =6 in Fig. 3), with each block having two sub-layers: a multi-head self-attention network, and a simple positionwise fully connected feed-forward network. Residual connections [59] alongside layer normalization [60] are employed after each block as in Fig. 3. Note that, different from regular convolutional networks where feature aggregation and feature transformation are simultaneously performed (e.g., with a convolution layer followed by a non-linearity), these two steps are decoupled in the Transformer model i.e., self-attention layer only performs aggregation while the feed-forward layer performs transformation. Similar to the encoder, the decoder (bottom row) in the Transformer model comprises six identical blocks. Each decoder block has three sub-layers, ﬁrst two (multi-head self-attention, and feedforward) are similar to the encoder, while the third sublayer performs multi-head attention on the outputs of the corresponding encoder block, as shown in Fig. 3.
The original Transformer model in [1] was trained for the Machine Translation task. The input to the encoder is

a sequence of words (sentence) in one language. Positional encodings are added to the input sequence to capture the relative position of each word in the sequence. Positional encodings have the same dimensions as the input d = 512, and can be learned or pre-deﬁned e.g., by sine or cosine functions. Being an auto-regressive model, the decoder of the Transformer [1] uses previous predictions to output the next word in the sequence. The decoder, therefore, takes inputs from the encoder as well as the previous outputs to predict the next word of the sentence in the translated language. To facilitate residual connections the output dimensions of all layers are kept the same i.e., d = 512. The dimensions of query, key and value weight matrices in multi-head attention are set to dq = 64, dk = 64, dv = 64.
2.4 Bidirectional Representations
The training strategy of the original Transformer model [1] could only attend to the context on the left of a given word in the sentence. This is limiting, since for most language tasks, contextual information from both left and right sides is important. Bidirectional Encoder Representations from Transformers (BERT) [3] proposed to jointly encode the right and left context of a word in a sentence, thus improving the learned feature representations for textual data in an unsupervised manner. To enable bidirectional training, [3] basically introduced two pretext tasks: Masked Language Model and Next Sentence Prediction. The model pre-trained on these pretext tasks in an unsupervised manner was then ﬁne-tuned for the downstream task. For this purpose, taskspeciﬁc additional output module is appended to the pretrained model, and the full model is ﬁne-tuned end-to-end.
The network architecture of the base BERT [3] model is based upon the original Transformer model proposed in [1] and is similar to the GPT [4]. The main architectural difference compared to [1] is that the BERT model only uses the Transformer encoder (similar to the middle row, Fig. 3) while the GPT [4] only uses the Transformer decoder (similar to the bottom row, Fig. 3). The core contribution of BERT [3] is the pretext task deﬁnition, which enables bidirectional feature encoding in an unsupervised manner. To this end, BERT [3] proposed two strategies: (1) Masked Language Model (MLM) - A ﬁxed percentage (15%) of words in a sentence are randomly masked and the model is trained to predict these masked words using cross-entropy loss. In predicting the masked words, the model learns to incorporate the bidirectional context. (2) Next Sentence Prediction (NSP) - Given a pair of sentences, the model predicts a binary label i.e., whether the pair is valid from the original document or not. The training data for this can easily be generated from any monolingual text corpus. A pair of sentences A and B is formed, such that B is the actual sentence (next to A) 50% of the time, and B is a random sentence for other 50% of the time. NSP enables the model to capture sentence-to-sentence relationships which are crucial in many language modeling tasks such as Question Answering and Natural Language Inference.
3 TRANSFORMERS & SELF-ATTENTION IN VISION
We provide an overview of main themes followed in Transformers designed for vision applications in Fig. 4. Exist-

5

Efficient Attention

Transformer

Local Relation

Networks
Hu et al. 2019

Stand-Alone Attention

Ramachandran et al. 2019

Color

Criss-Cross Transformer

Attention

2020

ViT

Haung et al. 2019

Dosovitskiv et al.

Global-Contex Networks
Coe et al. 2020

D-DETR
Zhu et al. 2020

2020
Point

Tranformer

Non-Local Networks

Zho et al. 2020

DeiT
Tourvon et al. 2020

Buades et al. 2018

Global

Attenion

Attention-Augmented

Convolution

Cross

Bello et al. 2019

Transformer
Doersch et al. 2020
DETR

Local Attention

Carion et al.
2020 SA-TA

Tourvon et al. 2020

Pair/Patch Attention
Zhao et al. 2020
Vectorized Attention

CNN

Fig. 4: A taxonomy of self-attention design space. Existing works explore local vs. global self-attention both with and without transformer-based encoding/decoding stages. Sparse attention, vector/scalar attention and low-rank matrix factorizations have been explored to study the trade-off between computational efﬁciency and expressivity of the developed models. Interesting efforts have also been made to utilize knowledge from convolution based architectures that already encode priors suitable for visual data to improve data efﬁciency.

ing frameworks generally apply global or local attention, leverage CNN representations or utilize matrix factorization to enhance design efﬁciency and use vectorized attention models. We explain these research directions below in the form of task-speciﬁc groups of approaches.
3.1 Transformers for Image Recognition
Convolution operation is the work-horse of the conventional deep neural networks used in computer vision and it brought breakthroughs such as solving complex image recognition tasks on high dimensional datasets like ImageNet [61]. However, convolution also comes with its shortcomings e.g., it operates on a ﬁxed-sized window thus unable to capture long-range dependencies such as arbitrary relations between pixels in both spatial and time domains in a given video. Furthermore, convolution ﬁlter weights remain ﬁxed after training so the operation cannot adapt dynamically to any variation to the input. In this section, we review methods that alleviate the above-mentioned issues in conventional deep neural networks by using Self-attention operations and Transformer networks (a speciﬁc form of self-attention). There are two main design approaches to self-attention. (a) Global self-attention which is not restricted by the size of input features e.g., [62] introduces a layer inspired from non-local means that applies attention to the whole feature map while [63] reduces the computational complexity of non-local operation [62] by designing sparse attention maps. (b) Local self-attention tries to model relation within a given neighborhood e.g., [64] proposed to restrict the attention within a speciﬁc window around a given pixel position to reduce the computational overhead. Similarly, [62] further improved local self-attention such that it can dynamically adapt its weight aggregation to variations in the input data/features.

Recently, global self-attention has been successfully applied by using NLP Transformer encoder directly on image patches [11], removing the need for handcrafted network design. Transformer is data-hungry in nature e.g., a largescale dataset like ImageNet is not enough to train Vision Transformer from scratch so [12] proposes to distill knowledge from a CNN teacher to a student Vision Transformer which allowed Transformer training on only ImageNet without any additional data. Here, we describe key insights from different methods based on local/global self-attention including Transformers speciﬁcally designed to solve the image recognition task.
3.1.1 Non-Local Neural Networks
This approach is inspired by non-local means operation [65] which was mainly designed for image denoising. This operation modiﬁes a given pixel in a patch with a weighted sum of other pixel values in an image. However, instead of considering a ﬁxed-sized window around a pixel, it selects distant pixels to contribute to the ﬁlter response based on the similarity between the patches. By design, the non-local operation models long-range dependencies in the image space. Motivated by this, Wang et al. [66] proposed a differentiable non-local operation for deep neural networks to capture long-range dependencies both in both space and time in a feed-forward fashion. Given a feature map, their proposed operator [66] computes the response at a position as a weighted sum of the features at all positions in the feature map. This way, the non-local operation is able to capture interactions between any two positions in the feature map regardless of the distance between them. Videos classiﬁcation is an example of a task where longrange interactions between pixels exist both in space and time. Equipped with the capability to model long-range interactions, [66] demonstrated the superiority of non-local deep neural networks for more accurate video classiﬁcation on Kinetics dataset [67].
3.1.2 Criss-Cross Attention
Although the self-attention mechanism allows us to model full-image contextual information, it is both memory and compute intensive procedure. As shown in Fig. 5(a), in order to encode global context for a given pixel location, non-local block [66] computes a dense attention map (in green). The non-local block [66] has a high complexity of O(N 2), where N denotes the number of input feature maps. To reduce this computational burden, Huang et al. [63] propose the criss-cross attention module that for each pixel position generates a sparse attention map only on the criss-cross path, as illustrated in Fig. 5(b). Further, by applying criss-cross attention recurrently, each pixel position can capture context from all other pixels. Compared to nonlocal block, the criss-cros√s uses 11× lesser GPU memory, and has a complexity of O(2 N ). State-of-the-art results are reported [63] for the semantic and instance segmentation tasks on several benchmark datasets including Cityscapes [68], ADE20K [69], COCO [70], LIP [71] and CamVid [72].
3.1.3 Stand-Alone Self-Attention
As discussed above, convolutional layers possess translation equivariance but can not scale with a large receptive

6

Fig. 5: The ﬁgure compares two different self-attention approaches. (a) Non-local block [66], and (b) Criss-cross attention module [63]. Figure is from [63].
ﬁeld, therefore can not capture long-range interactions [64]. On the other hand, global attention [1] which attend to all spatial locations of the input can be computationally intensive and is preferred on down-sampled small images, image patches [11] or augmenting the convolutional features space [73]. Ramachandran et al. [64] proposed to replace convolutional layers in deep neural networks with a local self-attention layer which can be applied to small or large inputs without increasing the computational cost. At a basic level, the proposed self-attention layer [64] considers all pixel positions in a speciﬁc window size around a given pixel, compute queries, keys and value vectors for these pixels, and then aggregates the spatial information within this window. The value vectors are aggregated after projecting the softmax score of queries and keys. This process is repeated for all given pixels and the response is concatenated to produce the output pixel. ResNet models with local self-attention layer can solve ImageNet and COCO object detection with fewer parameters as compared to ResNet models based on convolutional layers [64].
3.1.4 Local Relation Networks
Another shortcoming of the convolutional operator comes from the fact that after training, it applies ﬁxed weights regardless of any changes to the visual input. Hu et al. [62] proposed to adaptively compose pixels in a local window. They introduced a new differentiable layer (Fig. 6) that adapts its weight aggregation based on the compositional relations (similarity) between pixels/features within a local window. Such adaptive weight aggregation introduces geometric priors into the network which are important for the recognition tasks [62]. Convolution is considered to be a top-down operator as it remains ﬁxed across positions while a non-local operation such as introduced in [65] is a bottom-up method as it aggregates input features over the full image. The local relation layer belongs to the category of bottom-up methods but it is restricted to a ﬁxed window size e.g., 7x7 neighborhood.

Fig. 6: Local Relation Layer. It adapts weights based on the relationships between features in a local window. Given an input feature tensor and a predeﬁned geometric prior (which deﬁne composability of pixel pairs based on their relative position), self attention is computed to adapt weights according to incoming features. The function Φ simply deﬁnes the squared difference between key and query values. Figure is from [62].
3.1.5 Attention Augmented Convolutional Networks
Bello et al. [73] explore the possibility of employing selfattention as an alternative to convolutional operators. They propose to use the relative position encoding [74] in two dimensions to develop a new self-attention mechanism that maintains translation equivariance, which is a desirable property for handling images. Although this self-attention provides competitive results as a stand-alone computational primitive, the best performance is obtained when used in combination with the convolutional operations. Extensive experiments show that attention augmentation leads to systematic performance improvements in image classiﬁcation and object detection for a variety of existing architectures.
3.1.6 Vectorized Self-Attention
Zhao et al. [75] note that a traditional convolution operator performs feature aggregation and transformation jointly (by applying a ﬁlter and then passing it through a nonlinearity). In contrast, they propose to perform feature aggregation separately with self-attention followed by transformation using an element-wise perceptron layer (Fig. 7). To this end, they propose two alternate strategies for feature aggregation: (a) pairwise self-attention and (b) patch-wise self-attention. The pairwise self-attention is permutation and cardinality invariant operation, while the patch-wise self-attention does not have such invariance properties (similar to the convolution operator).
Both pairwise and patch-wise self-attentions are implemented as a vector attention [75] that learns weights for both the spatial and channel dimensions. This provides an alternate approach for attention that is conventionally performed using scalar weights (by taking a dot-product). The pairwise self-attention is a set operator that computes a vector attention keeping in view the relationships of a particular feature with its neighbors in a given local neighborhood. In contrast, patch-wise self-attention is a generalization of the convolution operator (not a set operator) and looks at all the feature vectors in the local neighbourhood when deriving the attention vectors. Authors show that with considerably fewer parameters, self-attention networks (SAN) can beat

7

Fig. 7: Vectorized self-attention block in SAN. The vector-based self-attention can be implemented as a pairwise or a patchwise operation. The input and output of the block is a feature tensor with C channels. The left branch calculates the attention weights α = γ(δ(x)) while the right branch transforms features using a linear mapping β. r1 and r2 denote the factors by which both branches reduce channel dimension for efﬁcient processing. Figure is from [75].
comparable baselines from ResNet family on the ImageNet dataset. One key property of their approach is its robustness against adversarial perturbations [76], [77] and generalization to unseen transformations in the data. This behaviour is due to the dynamic nature of attention that makes it difﬁcult for the adversary to calculate useful fooling directions.
3.1.7 Vision Transformer
Vision Transformer (ViT) [11] is the ﬁrst work to showcase how Transformers can ‘altogether’ replace standard convolutions in deep neural networks on large-scale computer vision datasets. They applied the original Transformer model (with minimal changes compared to the version used for NLP tasks) on a sequence of image ’patches’. The Transformer model was pre-trained on a large propriety dataset of images collected by Google and later ﬁne-tuned to downstream recognition benchmarks e.g., ImageNet. This is an important step since pre-training on a medium-range dataset would not give state-of-the-art results with a ViT. This is because the CNNs encode prior knowledge about the image domain (inductive biases e.g., translation equivariance) that reduces the need of data as compared to Transformers which must discover such knowledge rules from very large-scale datasets. To this end, a 300 million image JFT dataset [39] was used for pre-training that helped boost the performance to the level of state of the art CNN models. Notably, compared to the iGPT [19] model that also applied Transformers to full-sized images but performs training as a generative task, ViT pre-trains the model with a supervised classiﬁcation task (although a self-supervision variant is also explored which results in a less performance).
The main architecture of the model (Fig. 8) is very similar to language Transformers. Instead of a 1D sequence of language embeddings, 2D images patches are ﬂattened in a vector form and fed to the Transformer as a sequence. These vectorized patches are then projected to a patch embedding using a linear layer and position embedding is attached with it to encode location information. Importantly, a [cls] token (stands for classiﬁcation) is appended at the beginning

Fig. 8: An overview of Vision Transformer (on the left) and the details of Transformer encoder (on the right). The architecture resembles Transformers used in the NLP domain and the image patches are simply fed to the model after ﬂattening. After training, the feature obtained from the ﬁrst token position is used for classiﬁcation. Image obtained from [11].
of the input to the Transformer. The output representation corresponding to the ﬁrst position is then used as the global image representation for the image classiﬁcation task.
3.1.8 Data-Efﬁcient Image Transformers
The Data-efﬁcient image Transformer (DeiT) [12] is the ﬁrst result in large-scale image classiﬁcation, without utilizing any external large-scale dataset (e.g., JFT in [11]), which demonstrates the potential of Transformers compared to carefully tuned CNN designs. Since the Transformer architecture does not assume prior knowledge about the image structure as opposed to CNN design, it typically leads to longer training times, and larger datasets are required to train Transformer models. However, DeiT demonstrates how Transformers can be learned on mid-sized datasets (e.g., 1.2 million examples compared to hundreds of millions used in ViT [11]) in relatively shorter training episodes. Besides using augmentation and regularization procedures common in CNNs, the main contribution is a novel native distillation approach for Transformers.
The distillation process [78] uses a CNN as a teacher model (RegNetY-16GF [79]) whose outputs are used to train the Transformer model. The outputs from the CNN aids the Transformer in efﬁciently ﬁguring out useful representations for input images. A distillation token is appended with the input patch embeddings and the class token. The selfattention layers operate on these tokens to learn their interdependencies and output the learned class, patch, and distillation tokens. The network is trained with a cross-entropy loss deﬁned on the output class token and a distillation loss to match the distillation token with the teacher output. Both soft and hard label choices were explored for distillation, where the hard distillation was found to perform better. Interestingly, the learned class and distillation tokens do not exhibit a high correlation indicating their complementary nature. The learned representations compare favorably well against top-performing CNN architectures such as EfﬁcientNet [80] and also generalize well for a number of downstream recognition tasks.

8

3.1.9 CLIP: Contrastive Language–Image Pre-training
CLIP [81] is a contrastive approach to learn image representations from text, with a learning objective which maximizes similarity of correct text-image pairs embeddings in a large batch size. Speciﬁcally, given a batch of N image-text pairs, CLIP learns a multi-modal embedding space, by jointly training an image-encoder and a text-encoder, such that the cosine similarity of the valid N image-text pairs is maximized, while the remaining N 2 −N pairs is minimized. The authors consider ResNet-50 [59] and Vision Transformer (ViT) [82] for encoding images. The modiﬁed Transformer model [1] as in [5] is employed for encoding text. CLIP is trained on a large corpus of 400 million image-text pairs and demonstrates excellent zero-shot transfer capabilities. At inference, the names of classes are used as input to the textencoder, and similarity of the encoded image is computed with all encoded texts (classes) to ﬁnd the image-text pair with highest match. The CLIP achieves an astounding zeroshot classiﬁcation accuracy of 75% on ImageNet, without using an supervision from ImageNet training set. The authors further demonstrate zero-shot transfer capabilities of the CLIP model on 30 different computer vision benchmarks. Note that CLIP with ResNet took 18 days to train on 592 V100 GPUs while CLIP with ViT took 12 days on 256 V100 GPUs. This highlights the computational cost of CLIP.
3.2 Transformers for Object Detection
Similar to image classiﬁcation, Transformer models are applied to a set of image features obtained from a backbone CNN model to predict precise object bounding boxes and their corresponding class labels. Below, the ﬁrst approach [13] tackles the detection problem, for the ﬁrst time, using Transformer networks and the second approach [14] mainly extends [13] to a multi-scale architecture and focuses on improving computational efﬁciency.
3.2.1 Detection Transformer - DETR
In order to apply Transformer model, DETR [13] treats object detection as a set prediction problem and proposes a set loss function. This means that given a set of image features, predict the set of object bounding boxes. The ﬁrst contribution (the Transformer model) enables the prediction of a set of objects (in a single shot) and allows modeling their relationships. The second contribution (the set loss) allows bipartite matching between predictions and groundtruth boxes. The main advantage of DETR is that it removes the dependence on hand-crafted modules and operations, such as the RPN (region proposal network) and NMS (nonmaximal suppression) commonly used in object detection [83]–[87]. In this manner, the dependence on prior knowledge and careful engineering design is relaxed for complex structured tasks like object detection.
Given spatial feature maps from the CNN backbone, the encoder ﬁrst ﬂattens the spatial dimensions into a single dimension, as illustrated in Fig. 9. This gives a sequence of features d × n, where d is the feature dimension and n = h×w with h, w being the height and width of the spatial feature maps. These features are then encoded and decoded using multi-head self-attention modules as proposed in [1]. The main difference in the decoding stage is that all

Fig. 9: An overview of Detection Transformer (DETR) [13]. DETR treats the object detection task as a set prediction problem and uses the Transformer network to encode relationships between set elements. Then a bipartite set loss is used to uniquely match the box predictions with the ground-truth boxes (shown on the right two columns). In case of no match, a ’no object’ class prediction is selected. Its simple design with minimal problemspeciﬁc modiﬁcations can beat a carefully built and popular Faster R-CNN model. Figure from [13].
boxes are predicted in parallel while [1] uses an RNN to predict sequence elements one by one. Since the encoder and decoder are permutation invariant, learned positional encodings are used as the object queries by the decoder to generate different boxes. Note that the spatial structure in a CNN detector (e.g., Faster R-CNN) automatically encodes the positional information. DETR obtains performance comparable to the popular Faster R-CNN model [83] which is an impressive feat given its simple design. The DETR has also been extended to interesting applications in other domains, e.g., Cell-DETR [88] extends it for instance segmentation of biological cells. A dedicated attention branch is added to obtain instance-wise segmentations in addition box predictions that are enhanced with a CNN decoder to generate accurate instance masks.
3.2.2 Deformable - DETR
The above-mentioned DETR [13] successfully combines convolutional networks with Transformers [1] to remove handcrafted design requirements and achieves an end-to-end trainable object detection pipeline. However, it struggles to detect small objects and suffers from slow convergence and a relatively high computational cost [14]. DETR maps images to features space before using the Transformer for the relation modeling. Thus, the computational cost of selfattention grows quadratically with the spatial size of the feature map i.e., O(H2W 2C), where H and W represent the height and width of the feature map. This inherently puts a limitation on the use of multi-scale hierarchical features [89] in DETR training framework which is ultimately important to detect small objects. Furthermore, at the beginning of training, the attention module simply projects uniform attention to all the locations of the feature map and requires a large number of training epochs to tune attention weights to converge to meaningfully sparse locations. This approach contributes to a slow convergence rate of DETR. To mitigate the above-mentioned issues, [14] proposed a deformable attention module to process the feature maps. Inspired from deformable convolutions [90], deformable attention module [14] only attends to sparse set of elements from the whole feature map regardless of its spatial size. This further allows cross-scale aggregation of feature maps with the help of multi-scale attention modules without increasing the computational cost signiﬁcantly. Deformable DETR not only performs better but its training time also remains 10× lower than the original DETR model [14].

9

Fig. 10: Axial attention module [91] that sequentially applies multi-head axial attention operations along height and width axes. Image from [91].

3.3 Transformers for Segmentation
A dense prediction task like image segmentation into semantic labels and object instances requires modeling rich interactions between pixels. Here, we explain an axial selfattention operation [91] that seeks to reduce the complexity of self-attention and a cross-modal approach [15] that can segment regions corresponding to a given language expression.
3.3.1 Axial-Attention for Panoptic Segmentation
Panoptic segmentation [92] aims at jointly solving the otherwise distinct tasks of semantic segmentation and instance segmentation by assigning each pixel of the image a semantic label and an instance id. Global context can provide useful cues to deal with such a complex visual understanding task. Self-attention is effective at modeling long-range contextual information, albeit applying it to large inputs for a dense prediction task like panoptic segmentation is prohibitively expensive. A naive solution is to apply selfattention either to downsampled inputs or to limited regions around each pixel [64]. Even after introducing these constraints, the self-attention still has quadratic complexity and sacriﬁces the global context, respectively.
To mitigate the aforementioned issues, Wang et al. [91] propose the position-sensitive axial-attention where the 2D self-attention mechanism is reformulated as two 1D axialattention layers that are applied to height-axis and widthaxis sequentially (see Fig. 10). The axial-attention is highly compute efﬁcient and enables models to capture the fullimage context. The effectiveness of axial-attention is demonstrated by achieving state-of-the-art performance for the panoptic segmentation task on COCO [70], Mapillary Vistas [93], and Cityscapes [68] benchmarks and for the image classiﬁcation problem on ImageNet dataset [61].
3.3.2 CMSA: Cross-Modal Self-Attention
Cross-modal Self-attention (CMSA) [15] encodes long-range multi-modal dependencies between linguistic and visual domain features for referring image segmentation task. The referring image segmentation problem aims to segment entities in an image that are referred to by a language expression, as shown in Fig. 11. To this end, a set of crossmodal features is obtained by concatenating image features with each word embedding and the spatial coordinate features. The self-attention operates on this rich feature and generates attention over the image corresponding to each word in the sentence. The segmentation network performs self-attention at multiple spatial levels and uses a gated multi-level fusion module to reﬁne segmentation masks

Fig. 11: Cross-Modal Self-Attention: Red arrows show the selfattention over words, the green arrows show self-attention over image regions (a H×W grid) and the cross-modal attention is shown in blue. As shown in the right box, the overall framework can ﬁnd segmentation masks corresponding to words referred in the given text description. Figure based on [15].
via information exchange across multi-resolution features. A binary CE loss is used to train the overall model that achieves good improvements on UNC [94], G-Ref [95] and ReferIt [96] datasets.
3.4 Transformers for Image Generation
Image generation tasks are interesting from the perspective of generative modeling and because the representations learned in an unsupervised manner can later be used for down-stream tasks. Here, we summarize different Transformer-based architectures for image generation tasks [97]–[100]. We also cover a structured generation task where scene objects are populated given a room layout [23].
3.4.1 Image Transformer
Parmar et al. [97] develop an image generation model that can sequentially predict each pixel of an output image given its previously generated pixels. Their approach models the joint distribution of the image pixels by factorizing it as a product of pixel-wise conditional distributions. Previously developed auto-regressive models for this task, such as the PixelCNN [101], suffer from a limited receptive ﬁeld which hinders in modeling long term relationships in an image e.g., part relationships or occlusions. Using self-attention phenomenon, Image Transformers [97] enhance the receptive ﬁeld of neural networks without incurring a high computational cost (e.g., effective receptive ﬁeld up to 256 pixels was demonstrated to be achieved as compared to 25 pixels of PixelCNN [101]). The generative pipeline was also tested on conditional generation tasks e.g., image super-resolution, image completion, and denoising.
The core methodology has two main highlights (see Fig. 12), (a) the way key, query, and value triplets are used in images, and (b) the use of self-attention with a relatively high number of positions as compared to sentences in the language (where self-attention has been previously demonstrated to work successfully). For the ﬁrst part, the feature representations of previously generated pixels were used to generate ‘value’ and ‘key’ embeddings, while the current pixel’s feature embedding was used as a ‘query’.

10

Fig. 13: The architecture of generator and discriminator of TransGAN model, built using the encoder of the transformer model [1]. Figure is from [100].

Fig. 12: (a) Self-attention block in Image Transformer [97]. Given one channel for a pixel q, the block attends to the memory of previous synthesized pixels (mi), followed by a feedforward sub-network. Positional encodings pi are added in the ﬁrst layer. (b) The operation performed in Local Self-Attention (example of a 2D case is shown). The image is partitioned into a grid of spatial blocks known as query blocks. In the selfattention operation, each pixel in a query block attends to all pixels in the memory block (shown in cyan rectangle). White grid locations show masked inputs that have zero-contribution towards the self-attention.
Positional embeddings were used in the ﬁrst layer to encode location information. To solve the second problem, local attention (1D and 2D variants) was used only in the local neighborhood around the query position. For practical reasons, a ﬁxed memory block was deﬁned for each respective query, instead of dynamically calculating a different memory neighborhood for each pixel. A maximum likelihood loss was applied for training the generative model.
3.4.2 Image GPT
Motivated by the success of Transformer models in the language domain, image GPT (iGPT) [98] demonstrated that such models can be directly used for image generation tasks, and to learn strong features for downstream vision tasks (e.g., image classiﬁcation). Speciﬁcally, iGPT trains GPT v2 model [5] on ﬂattened image sequences (1D pixel arrays) and shows that it can generate plausible image outputs without any external supervision. The generated samples depict the model’s ability to understand spatial relationships between pixels and high-level attributes such as object classes, texture, and scale. Notably, the design does not use any image-speciﬁc knowledge in the design (e.g., the 2D position embeddings used in Image Transformer [97]).
The features learned with iGPT’s unsupervised training mechanism compete impressively against other unsupervised approaches, achieving state-of-the-art performance on CIFAR-10/100 [102] and STL [103] datasets while performing close to the best results of SimCLR (a contrastive learning approach) [104] on ImageNet dataset. This is an

astounding result, since the iGPT architecture is exactly the same as used for language modeling tasks, and therefore it does not incorporate any prior domain-speciﬁc knowledge. Notably, the competing unsupervised CNN based solutions widely adopt such priors in the form of architectural design, attention mechanisms, loss functions, and regularization [105]–[109]. However, on the downside, iGPT has a high compute cost e.g., iGPT-L version has roughly 36× high training cost compared to MoCo [107] which is a state of the art self-supervised feature learning approach. For this reason, the training was generally limited to low-resolution of ≤ 64 × 64, while convolutional architectures can effectively learn from high-resolution inputs.
3.4.3 High-Resolution Image Synthesis
Transformers typically incur a high computational cost when applied on high-dimensional sequences. To overcome this limitation, Esser et al. [99] proposed to include inductive biases (commonly used in the CNNs) alongside Transformers to improve their efﬁciency. Speciﬁcally, local connectivity and spatial invariance biases inbuilt in the CNN structure are leveraged by learning a rich dictionary of visual patterns. The dictionary is learned using a Generative Adversarial approach [46] that seeks to encode perceptually sound image patches. A Transformer is then used to learn the long-range interactions between the dictionary items to generate the outputs. In turn, they develop a conditional image generation model capable of producing very highresolution images (up to megapixel range) using Transformers. This is the ﬁrst work that demonstrates the application of Transformers to generate such high-resolution images.
3.4.4 TransGAN: Transformers based GAN model
Generative Adverserial Networks (GANs) [46] with CNNs as default backbone have been very successful for visually appealing image synthesis [110]–[112]. TransGAN [100] builds a strong GAN model, free of any convolution operation, with both generator and discriminator based upon the Transformer model [1]. As shown in Fig. 13, the the architecture of both generator and discriminator is based upon the encoder in original Transformer model [1]. For memory efﬁciency, the generator contains multiple stages, with up-sampling modules in-between, which gradually increase the resolution of feature maps (input sequence length) while reducing the embedding dimension. The discriminator of TransGAN takes ﬂattened image-patches as

11

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Fig. 14: Images generated by DALL·E [20] from the following text prompts. (a) An armchair in the shape of an avocado. (b) A photo

of San Francisco’s golden gate bridge. Given a part of the image (in green box), DALL·E performs the image completion. (c) An emoji

of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants. (d) An extreme close-up view of a capybara sitting in a ﬁeld.

(e) A cross-section view of a pomegranate. (f) A penguin made of watermelon. (g) The exact same cat on the top as a sketch on the bottom.

tokens similar to [82]. Authors introduce different training techniques including data augmentation, training with an auxiliary task and injecting locality to self-attention to scaleup their model for high quality image synthesis [99]. The TransGAN model achieves state-of-the-art results in terms of Inception Score and Fre´chet Inception Distance (FID) on STL-10 and performs favorably compared with their CNNbased GAN counterparts on other datasets.
3.4.5 SceneFormer
In the previous works on image generation [97]–[99], image outputs are generally predicted directly by the model. In contrast, [23] learns to generate parameters of 3D objects to be placed in a given scene. Speciﬁcally, SceneFormer [23] studies the 3D room layout conditioned scene generation task. Given the empty room shape, this approach can propose new object conﬁgurations in the room while maintaining realism. Remarkably, the model does not use any appearance information and only learns to generate new scenes by modeling the inter-object relationships using self-attention in Transformers. Similar to how a Transformer operates on a sentence, it is applied to a sequence of objects to predict the next suitable object in a scene. Speciﬁcally, the size, pose, location, and category of the next object is predicted by the Transformer model. A start token indicates the initiation of inference and the number of output token indicate the objects generated by the model in a sequence. The authors also explore generating new scenes given a textual description of the room layout. The independence from the appearance makes the approach efﬁcient, enabling interactive scene generation.
3.5 Transformers for Text-to-Image Synthesis
The task of generating realistic images from text is interesting and practically valuable (e.g., for artistic content creation), but at the same time highly challenging. Prior textto-image synthesis approaches [113]–[116] are mostly based on GANs [46]. Although these methods produce moderate results, they are far from being photo-realistic. To this end, Ramesh et al. [20] propose DALL·E which is a Transformer model capable of generating high-ﬁdelity images from a given text description. The model is named DALL·E using a portmanteau of the Spanish artist Salvador Dal´ı and the Pixar’s blockbuster movie WALL·E.
DALL·E model has 12 billion parameters and it is trained on a large set of text-image pairs taken from the internet. Before training, images are ﬁrst resized to 256×256 resolution, and subsequently compressed to a 32×32 grid of

latent codes using a pre-trained discrete variational autoencoder [117], [118]. DALL·E takes as input a single stream of 1280 tokens (256 for the text and 1024 for the image), and trained to generate all other tokens autoregressively (one after another). It provides ﬂexibility to generate images either from scratch (Fig. 14a) or by extending existing images (Fig. 14b), while staying faithful to the text caption.
The authors demonstrate the effectiveness of DALL·E by creating images from text describing a wide variety of real and ﬁctional things. While generating images purely from textural captions, DALL·E shows impressive performance at controlling multiple objects and their attributes (Fig. 14c), rendering certain viewpoint (Fig. 14d), capturing object’s internal structure (Fig. 14e), and combining unrelated objects (Fig. 14f). Furthermore, DALL·E can perform image-toimage translation (Fig. 14g) guided by the input text.
3.6 Transformers for Low-level Vision
Transformer models have also been proposed for low-level vision tasks including image super-resolution, denoising, deraining, and colorization. Speciﬁcally, Transformer network for super-resolution [16] uses attention mechanisms to search relevant textures from reference images and transfer them to low-resolution images to generate super-resolved outputs. Similarly, the work of [19] shows how to exploit the potential of pre-training and transfer learning with a shared Transformer based backbone to address multiple image restoration tasks (e.g., denoising, deraining, and super-resolution) with dedicated task-heads. The colorization transformer [24] proposes a progressive design for image colorization to achieve high-resolution outputs. Next, we provide details of these aforementioned image restoration Transformer models.
3.6.1 Transformers for Super-Resolution
Image super-resolution (SR) aims to generate a highresolution (HR) image from its low-resolution (LR) version. Recent years have seen major performance breakthroughs for SR due to convolutional neural networks (CNNs). Principally, the quality of super-resolved images generated by CNNs is dependent on the choice of optimization objective. On one hand, SR methods [119]–[123] that are based on pixel-wise loss functions (e.g., L1, MSE, etc.) yield impressive results in terms of image ﬁdelity metrics such as PSNR and SSIM. However, they struggle to recover ﬁne texture details and often produce images that are overlysmooth and perceptually less pleasant. On the other hand,

12

Fig. 15: Diagram of the texture Transformer module. Q (query), K (key) and V (value) represent texture features extracted from a (bicubic upsampled) low-resolution image, a sequentially down/upsampled reference image, and an original reference image, respectively. The relevance embedding aims to estimate similarity between low-resolution and reference images. H and S respectively denote hard and soft attentions computed from relevance embedding. T indicates high-resolution texture features that are then transferred to the features F of lowresolution image. Figure is from [16].
perceptual SR approaches [44], [124]–[127], in addition to per-pixel loss, employ adversarial loss [46] and perceptual loss [128] based on deep features extracted from pre-trained CNNs. While these methods generate images that are sharp, visually pleasant, and perceptually plausible, they show a substantial decrease in reconstruction accuracy measured in PSNR/SSIM. Moreover, the perceptual SR algorithms have a tendency to hallucinate fake textures and cause artifacts. The above mentioned SR approaches follow two distinct (but conﬂicting) research directions: one maximizing the reconstruction accuracy and the other maximizing the perceptual quality, but never both.
In order to alleviate the trade-off between perceptual reproduction and accurate reproduction, Yang et al. [16] propose a Transformer network (TTSR) for super-resolution. During training, TTSR uses paired LR-HR images, as well as reference (Ref) images with similar content as of LR images. TTSR learns to search relevant regions in the Ref image and transfers rich textures to help super-resolving the input LR image. The texture Transformer module of TTSR method, shown in Fig. 15, consists of four core components: (1) Learnable texture extractor takes as input LR↑, Ref↓↑, and Ref images, and generates texture features query (Q), key (K), and value (V), respectively. Here, ↑ denotes bicubic upsampling operation, and ↓↑ represents bicubic down-sampling followed by an upsampling operation. (2) Relevance embedding ﬁrst unfolds Q and K into patches and then computes the similarity of each patch in Q with each patch in K in order to generate hard and soft attention maps. (3) Hard-attention transfers HR texture features from V to (LR features) Q using the hard attention map. (4) Soft-attention further enhances relevant features while suppressing less relevant ones by using the soft-attention map.

Fig. 16: The overall architecture of Image Processing Transformer (IPT) used for denoising, deraining and super-resolution tasks. IPT employs multiple heads and tails to address each image restoration task separately, and a shared Transformer encoder-decoder. The multi-head extract visual features from the corrupted input images, that are then divided into patches and linearly ﬂatten before being passed to the Transformer encoder. Next, the encoder outputs along with the task-speciﬁc embeddings (indicating the desired task to be learned) are processed by the Transformer decoder followed by the multitails that yield the restored images. Figure is from [19].
3.6.2 Transformers for Image Processing Tasks
State-of-the-art algorithms developed for high-level computer vision tasks such as object detection and semantic segmentation often employ backbone networks that are pretrained on large-scale datasets e.g., ImageNet. In contrast, algorithms for low-level vision tasks such as image denoising, super-resolution, and deraining are directly trained on task-speciﬁc data, thereby suffer from the following limitations. First, the number of images available in task-speciﬁc datasets is small (e.g., the commonly used DIV2K dataset for image super-resolution contains only 2000 images). Second, the model trained for one image processing task does not adapt well to other related tasks.
Chen et al. [19] proposed a pre-trained model based on Transformer architecture, named as Image Processing Transformer (IPT). It is capable of performing various image restoration tasks such as super-resolution, denoising, and deraining. As shown in Fig. 16, the overall architecture of IPT consists of multi-heads and multi-tails to deal with different tasks separately, and a shared encoder-decoder Transformer body. Since exploiting Transformers at full potential requires training on large-scale data, Chen et al. [19] take the clean (ground-truth) images from the ImageNet benchmark and synthesize their degraded versions for different tasks. For example, bicubic interpolation is used for generating low-resolution images, additive white Gaussian noise is added to prepare noisy data, and hand-crafted rain streaks are applied to obtain rainy images. In total, 10 million images are used to pre-train the IPT model. During training, each task-speciﬁc head takes as input a degraded image and generates visual features. These feature maps are divided into small crops and subsequently ﬂattened before feeding them to the Transformer encoder. The architecture of the encoder is the same as that of the original Transformer model [1]. The outputs of the encoder along with the taskspeciﬁc embeddings are given as input to the Transformer decoder. The features from the decoder output are reshaped and passed to the multi-tail that yields restored images. The

13

Fig. 17: Colorization Transformer is a probabilistic model that breaks down the image colorization problem into three subtasks and trains separate Transformer models for each. Figure is from [24].

pre-training (VLP) on large-scale multi-modal datasets to learn generic representations that effectively encode crossmodality relationships (e.g., grounding semantic attributes of a person in a given image). These representations can then be transferred to downstream tasks, often obtaining state of the art results. Such models generally apply the vanilla multi-layer Transformer [1] with multi-modal inputs and don’t introduce fundamental changes to the core attention block. However, their main distinction is in the conﬁguration of Transformers and the loss functions, based on which we categorize them into two categories: single-stream and multi-stream Transformers. The single-stream designs feed the multi-modal inputs to a single Transformer while the multi-stream designs ﬁrst use independent Transformers for each modality and later learn cross-modal representations using another Transformer (see Fig. 18). We explain seminal multi-modal Transformers below.

IPT model is optimized with L1 loss. Experimental results show that the pre-trained IPT model, when ﬁne-tuned for a speciﬁc low-level vision task, can provide signiﬁcant performance gains over the state-of-the-art methods [122], [129], [130].
3.6.3 Colorization Transformer
Given a grayscale image, colorization seeks to produce the corresponding colorized sample. It is a one-to-many task as for a given grayscale input, there exist many possibilities in the colorized output space. The challenging nature of this task requires probabilistic models capable of producing multiple colorized output samples. Colorization Transformer [24] is a probabilistic model based on conditional attention mechanism [131]. It divides the image colorization task into three sub-problems (Fig. 17) and proposes to solve each task sequentially by a different Transformer network. The authors ﬁrst train a Transformer network to map a low-resolution grey-scale image to a 3-bit low-resolution colored image. Low-resolution images in turn allow training of larger models. The 3-bit low-resolution colored image is then upsampled to an 8-bit RGB sample by another Transformer network in the second stage of training. Finally, a third stage Transformer is trained to increase the spatial resolution of the 8-bit RGB sample produced by the secondstage Transformer. Self-attention used in the colorization Transformer is based on row/column attention layers introduced in [131]. These layers capture the interaction between each pixel of an input image while being computationally less costly. The row-wise attention layer applies selfattention to all pixels in a given row, while the column-wise attention layer considers pixels only in a given column of an image. This work [24] is the ﬁrst successful application of Transformers trained to colorize grey-scale images at high (256×256) resolution.
3.7 Transformers for Multi-Modal Tasks
Transformer models have also been extensively used for vision-language tasks such as visual question answering (VQA) [135], visual commonsense reasoning (VSR) [136], cross-modal retrieval [137] and image captioning [138]. Several works in this direction target effective vision-language

3.7.1 ViLBERT: Vision and Language BERT
Vision and Language BERT was the ﬁrst extension of the BERT model to the multi-modal domain. The goal was to learn representations that can jointly model images and natural language. For this purpose, ViLBERT developed a two-stream architecture where each stream is dedicated to model the vision or language inputs (Fig. 18-h). The architecture of both parallel streams is a series of Transformer blocks similar to the BERT model. Subsequently, coattentional Transformer layers are applied to learn crossmodal relationships. The co-attentional framework is very simple. Query, key, and value matrices are computed for each modality in the standard way [1] and then key-value pairs for one modality are passed on to the other modality’s attention head.
ViLBERT applies VLP on a set of proxy tasks deﬁned on the Conceptual Concepts dataset (with 3.3M images with weak captions) and later ﬁne-tune the model on downstream tasks such as VQA. The pre-training phase operates in a self-supervised manner, i.e., pretext tasks are created without manual labeling on the large-scale unlabelled dataset. These pretext tasks include predicting whether the text and image inputs are related and predicting the semantics of masked image regions and textual inputs (e.g., similar to reconstructing masked words in text in the BERT model [3]). This way, the model learns the inherent structure in the data during pre-training and also models cross-domain associations. With evaluations on several tasks, [17] demonstrated that a two-stream model can perform better than a single-stream model that uses shared parameters to model both language and vision domains [17].
3.7.2 LXMERT
Similar to ViLBERT [133], Learning Cross-Modality Encoder Representations from Transformers (LXMERT) [21] also uses a two-stream architecture based on BERT framework. The main difference lies in the object-relationship encoder that is used to model the visual features instead of simple imagelevel features used in ViLBERT. The information in two streams is then fused across modalities using cross-attention blocks similar to [133].
Compared to two pre-texts tasks used for VLP in [133], LXMERT uses ﬁve pre-training tasks including masked ob-

14

Fig. 18: An overview of Transformer models used for multi-modal tasks in computer vision. The Transformer designs in this category can be grouped into single-stream (UNITER [35], OSCAR [36], VideoBERT [17], Unicoder-VL [132], VisualBERT [55] and VL-BERT [22]) and dual-stream architectures (LXMERT [21], ViLBERT [133] and PEMT [134]). A key distinction between models is the choice of loss functions. While most of the multi-modal methods are focused on images as visual data, VideoBERT [17] and PEMT [134] are designed to work on video streams and leverage unique modalities e.g., audio signals in videos [134].

ject and language prediction, cross-modality matching, and visual question answering (Fig. 18-g). The pre-trained model is ﬁne-tuned on the VQA task, however, a high similarity between pre-training and ﬁne-tuned tasks raises questions on the generalizability of the learned representations to new tasks. To this end, the authors conducted generalization experiments on Visual Reasoning for Real (NLVR) task [139] demonstrating impressive improvements on novel tasks.
3.7.3 VisualBERT
Different from two-stream networks like ViLBERT [133] and LXMERT [21], VisualBERT [55] uses a single stack of Transformers to model both the domains (images and text). The input sequence of text (e.g., caption) and the visual features corresponding to the object proposals are fed to the Transformer that automatically discovers relations between the two domains. Notably, VisualBERT architecture is somewhat similar to VideoBERT [17] (explained in Sec. 3.8), but instead of only focusing on cooking videos, VisualBERT evaluates on various visual-linguistic tasks (e.g., VCR, NLVR, VQA, and visual grounding).
The VisualBERT model ﬁrst applies task-agnostic pretraining using two objectives (Fig. 18-e). The ﬁrst objective

simply attempts to predict missing text tokens using the image features and remaining textual tokens. The second objective attempts to differentiate between the true and false caption of a given image. After task-agnostic pre-training, the authors propose to perform task-speciﬁc pre-training to bridge the domain gap before the ﬁnal ﬁne-tuning to the downstream task.
3.7.4 VL-BERT
Su et al. [22] propose a multi-modal pre-training approach to learn features that are generalizable to multi-modal downstream tasks such as Visual Commonsense Reasoning and Visual Question Answering. This endeavor requires adequately aligning the visual and linguistic cues so that an effective composite representation is learned. To the end, [22] builds on the BERT model and inputs both the visual and language features. The language features correspond to the token in the input sentence and the visual features correspond to the region of interest (RoI) from the input image (obtained via a standard Faster R-CNN). Speciﬁcally, the model is pre-trained on both the visual-lingual dataset (Conceptual Captions [140]) as well as the language-only datasets (e.g., Wikipedia). The loss function is identical to

15

BERT, where the model is trained to predict the masked out words or visual ROIs (Fig. 18-f). In contrary to other works such as UNITER [35], VL-BERT claims that the visuallinguistic matching tasks are not useful during pre-training, which is in contrast to evidence from later efforts [132]. Their results on several multi-modal tasks show their beneﬁt over the language-only pre-training (e.g., in BERT).
3.7.5 Unicoder-VL
Universal Encoder for Vision and Language (Unicoder-VL) [132] learns multi-modal representations using large-scale image-caption datasets. The language and image inputs are fed to a single Transformer model (with multiple successive encoders) to learn joint embeddings. To this end, it uses masked word prediction, masked object classiﬁcation, and visual-linguistic matching as self-supervision tasks during pre-training (Fig. 18-d). Notably, the visuallinguistic matching is carried out only at the global level (i.e., image-sentence alignment). The model is evaluated on downstream tasks of image-text retrieval, zero-shot learning, and visual commonsense reasoning where it performs better than the previous models such as ViLBERT [133] and VisualBERT [55]. This shows the signiﬁcance of rich selfsupervised tasks and advocates for a uniﬁed Transformer architecture to learn multi-modal feature representations in a common framework.
3.7.6 Uniﬁed VLP
The Uniﬁed Vision-Language Pre-training (VLP) [141] model uses a single Transformer network for both encoding and decoding stages. This stands in contrast to BERT inspired VLP models [17], [22], [55], [142] which use independent encoder and decoder networks. Joint modeling of encoding and decoding stages allows the Uniﬁed VLP model to perform well for both image captioning and visualquestion answering tasks, when ﬁne-tuned on these individual tasks. The intuition for shared modeling of encoding and decoding stage stems from the need to better share crosstask information during pre-training. The uniﬁed model consists of a stack of 12 Transformer blocks, each with a selfattention layer followed by a feed-forward module. The selfsupervised objectives used for pre-training include masked vision-language predictions. Here, the authors explore two variants i.e., bidirectional and sequence-to-sequence prediction of masked works where different context encodings are used for both types of objectives. The proposed approach is evaluated on COCO Captions, Flick 30K Captions and VQA 2.0 and obtains encouraging results compared to previous methods on image captioning and VQA [143].
3.7.7 UNITER
Universal image-text representation (UNITER) [35] is also a multi-modal feature learning approach via pre-training on four large-scale visual-linguistic datasets (MS-COCO [70], Visual Genome [144], Conceptual Captions [140] and SBU Captions [145]). The learned representations have been shown to transfer well on downstream tasks such as VQA, Multi-modal retrieval, Visual Commonsense reasoning, and NLVR. In order to emphasize on learning the relationships between visual and language domains, they speciﬁcally

design pre-training tasks to predict masked visual or text region conditioned on the other domain input, and align language and visual inputs on both the global (image-text) and local (word-region) levels (Fig. 18-a). These tasks are beside the conventional masked language modeling task used in BERT and explicitly include ﬁne-grained wordregion alignment alongside conditional masking of inputs that were not considered in the earlier works such as VLBERT [22], Visual-BERT [55], Vilbert [133] and UnicoderVL [132]. Common to the other approaches, they adopt the Transformer architecture proposed in BERT that operates on both the visual and language embeddings. In contrast to applying independent Transformers to the language and visual inputs (as in ViLBERT [133] and LXMERT [21]), UNITER adopts a single Transformer applied to the textual and image inputs like [22], [55], [132].
3.7.8 Oscar: Object-Semantics Aligned Pre-Training
VisualBert [55], Uniter [35], VL-BERT [22], VilBERT [133], Unicoder-VL [132] models for VLP concatenate image and text features and leave it on to the self-attention to automatically discover cross-modal relationships. This can complicate the visual grounding of semantic concepts in an image. To address this problem, Oscar [36] ﬁrst uses an object detector to obtain object tags (labels), subsequently using these tags as a mechanism to align relevant visual features with the semantic domain information (Fig. 18-b). The motivation is that the textual content generally pertains to major objects in the image, therefore by explicitly adding those image labels in the input, visual features can be better attended. Similar to BERT [3], Oscar uses a Masked Token Loss for VLP. Speciﬁcally, different tokens in the textual input and image tags are randomly masked and the model’s job is to predict the missing token. This forces it to learn the relationship of the missing token with the contextual information given as visual and semantic features. Further, it also uses a contrastive loss that discriminates between the original and noisy/fake image-tag pairs. The representations thus learned are ﬁne-tuned on VQA, cross-modality retrieval, natural language reasoning, and image captioning tasks to obtain better performances compared to VLP methods that do not use object tags.
3.7.9 Vokenization
Tan and Bansal [146] introduce the concept of ‘vokens’ (images related to language tokens extracted from sentences). The vokens (visualized tokens) provide visual supervision to the language model to learn better features. The motivation is that humans learn languages by correlating visual information with semantic concepts. In a similar spirit to other self-supervised language representation learning methods [3], [133], they learn representations by deﬁning an auxiliary task of voken-prediction task.
Since the existing datasets encode limited visually grounded tokens, they propose a vokenization method to map language tokens to visual vokens, as illustrated in Fig. 19. The approach uses language-based retrieval for such a mapping and transfers a model trained on a small labeled dataset (MS-COCO) to a large dataset (Wikipedia). Furthermore, it was ensured that the sentence-wide context

16
is desirable in various uni-modal and multi-modal learning tasks such as activity recognition [67], [150]–[153]. Below, we explain recent approaches that seek to resolve this challenge using the expressivity of Transformer networks.

Fig. 19: Visualized tokens (Vokens) [146]: A language model is visually supervised using closely related images that leads to better feature representations from the pretrained model. Figure from [146].
is considered to obtain the token-voken mapping. The resulting model trained using generated tokens outperform the state of the art BERT model on a diverse set of NLP tasks. In this sense, the proposed model does not evaluate vision tasks, however, uses vision as a useful grounding cue to train the language model, hence we include it in the multimodal representation learning group.
3.7.10 Vision-and-Language Navigation
This task aims to predict a navigation plan on a map based on the vision and language inputs. Self-attention based Transformer networks were used earlier in [147], [148] for the visual and language navigation (VLN). These works ﬁrst pre-trained a cross-modal Transformer network using selfsupervision on vision and language pairs and subsequently ﬁne-tune on the speciﬁc VLN tasks. While these works learn attention between image region and language, Chen et al. [149] propose to learn cross-modal attention between language inputs and spatial topological maps. The topological maps represent an agent’s environment as a graph whose nodes denote places and the edges denote their connectivity. Given the topological map and natural language inputs, a VLN task using the Transformer model bears resemblance to sequence prediction in NLP. Speciﬁcally, at each time instance, the cross-modal Transformer predicts a single node of the topological map in the navigation plan. The individual language and map encodings are ﬁrst processed using uni-modal encoders and later a cross-modal encoder (similar to LXMERT [21]) is applied to aggregate information across modalities. To denote positions in the map, a learned trajectory position encoding is appended with the map features. Based on this Transformer setup, [149] reports a full navigation system that can freely explore the environment and intelligently plan its actions.
3.8 Video Understanding
Audio-visual data in the form of videos is abundantly available. However, the prevalent approaches generally learn representations on short-length videos (up to a few seconds long), that allow them to encode only short-range dependencies [1], [29]. Long-range dependency modeling

3.8.1 VideoBERT: Joint Video and Language Modeling
The VideoBERT [17] model leverages Transformer networks and the strength of self-supervised learning to learn effective multi-modal representations. Speciﬁcally, VideoBERT uses the prediction of masked visual and linguistic tokens as a pretext task in self-supervised learning (Fig. 18-c). This allows modeling high-level semantics and long-range temporal dependencies, important for video understanding tasks. Given a video, they convert speech to text using off-the-shelf speech recognition systems and apply vector quantization (clustering) to obtain visual features from pretrained video classiﬁcation models. The BERT model is then directly applied to these concatenated sequences of language and visual tokens to learn their joint distribution.
The model can be trained with only-text, video-only, and video+text domains. The resulting model showcases interesting capabilities for cross-modality predictions such as video generation from a given textual input (e.g., captions or cooking recipe) and (video-based) future forecasting given a video token. The video+text model uses a visual-linguistic alignment task to learn cross-modality relationships. The deﬁnition of this pre-text task is simple, given the latent state of the [cls] token, the task is to predict whether the sentence is temporally aligned with the sequence of visual tokens. Further, the learned representations are shown to be very useful for downstream tasks such as action classiﬁcation, zero-shot classiﬁcation, and video captioning.
3.8.2 Masked Transformer
Zhou et al. [154] study the dense video captioning problem using Transformers. This problem setting requires generating language descriptions for all events occurring in a video. The previous works on this problem generally operate sequentially i.e., ﬁrst detect events and then generate captions in separate sub-blocks. The proposed uniﬁed Transformer network learns a single model to tackle both tasks jointly, thereby seamlessly integrating the multi-modal tasks of event detection and captioning. First, a video encoder is used to obtain frame-wise representations followed by two decoder blocks focused on proposing the video events and the captions. Since untrimmed videos are considered, a masking network is used in the captioning decoder to focus on describing a single event proposal. Remarkably, [154] was the ﬁrst approach to target dense video captioning using non-recurrent models and used self-attention in the encoder(applied on CNN derived features) to model broad range context between video frames. Experiments on ActivityNet Captions [155] and YouCookII [156] datasets showed good improvements over previous recurrent network and two-stage based approaches.
3.8.3 Parameter Efﬁcient Multi-Modal Transformers
Lee et al. [134] note that the multi-modal representation learning approaches like VideoBERT [17] and ViLBERT [133]

17

generally keep the language processing part ﬁxed to a pretrained model (e.g., BERT [3]) to reduce training complexity. Alternatively, for the ﬁrst time in the literature, they propose to learn an end-to-end multi-modal bidirectional Transformer model called PEMT on audio-visual data from unlabeled videos. First, short-term (e.g., 1-3 seconds) video dynamics are encoded using CNNs, followed by a modalityspeciﬁc Transformer (audio/visual) that can model longterm dependencies (e.g., 30 seconds). A multi-modal Transformer is then applied to the modality-speciﬁc Transformer outputs to exchange information across visual-linguistic domains. However, learning such a model in a naive form would incur huge memory requirements. To reduce parametric complexity, the parameters are shared across layers within each Transformer based on a low-rank approximation that leads to as high as 80% parameter reduction.
The Transformer is trained using a contrastive learning approach based on a content-aware negative sampling (Fig. 18-i). Speciﬁcally, the model uses the features obtained from CNNs learned during the training phase to select negative samples that are visually similar to the positive instances. This work also compares various fusion strategies adopted in earlier works such as early (VideoBERT [17] and VL-BERT [22]), mid-level (ViL-BERT [133] and LXMERT [21]) and late fusion mechanisms and shows that the midlevel fusion is the optimal choice. The proposed model is pre-trained on Kinetics-700 [150] dataset and later ﬁnetuned on downstream video classiﬁcation tasks such as short video classiﬁcation on UCF101 [157], audio classiﬁcation on ESC50 [158] and long-term action recognition on Charades [159] and Kinetics-Sounds [57] datasets.
3.8.4 Video Action Transformer
Girdhar et al. [18] use a variant of Transformer architecture to aggregate contextual cues in a video relevant to a particular person. They demonstrate the usefulness of such contextual information for action classiﬁcation and localization. Initially, the model uses a Faster-RCNN [83] style processing where a backbone model generate features that are forwarded to the Region Proposal Network to obtain object proposals. Then RoI pooling is applied to generate object-speciﬁc features. Multi-head self-attention [1] is then applied on top of the object features as a cascade of self-attention layers. In each Transformer unit, a particular person feature is treated as the ‘query’ (Q), while the features from the neighboring video clip are used as ‘key’ (K) and ‘value’ (V). The location information is explicitly encoded in the input feature map from which K, V and Q are derived, thus incorporating the positional information in the self-attention. For a given 400×400×64 video clip, the key and value tensors are of size 16×25×25×128, while the query is 128 dimensional vector. Although this work uses only RGB stream, the use of additional modalities like optical ﬂow and audio signal (as in competing video analysis works) would further increase the computational complexity. Further, the Transformer model was found to be sub-optimal for action localization, perhaps due to its tendency to incorporate global information. Therefore, an important research question is how to achieve the right trade-off between the global and local context for problems

Fig. 20: Video Transformer Network (VTN) [160]: Features are extracted from the individual frames using a 2D CNN (f ) and fed to a Transformer encoder with positional embeddings (PE). An MLP is used to classify videos using the output classiﬁcation token embedding. Image courtesy [160].
that demand precise delineation (e.g., action localization and segmentation).
3.8.5 Video Transformer Network
The traditional CNN based methods in video classiﬁcation generally perform 3D spatio-temporal processing over limited intervals to understand videos. Neimark et al. [160] propose Video Transformer Network (VTN) that ﬁrst obtain frame-wise features using 2D CNN and apply a Transformer encoder (Longformer [161]) on top to learn temporal relationships (Fig. 20). Longformer is an attractive choice to process long sequences (with an arbitrary length n) due to its O(n) complexity. The classiﬁcation token [CLS] is passed through a fully connected layer to recognize actions or events. The advantage of using Transformer encoder on top of spatial features is two fold: (a) it allows processing a complete video in a single pass, and (b) considerably improves training and inference efﬁciency by avoiding the expensive 3D convolutions. This makes VTN particularly suitable for modeling long videos where interactions between entities are spread throughout the video length. Their experiments on Kinetics-400 dataset [67] with various backbones (ResNet [59], ViT [11] and DeiT [12]) shows competitive performance.
3.8.6 Video Instance Segmentation Transformer
The Video Instance Segmentation Transformer (VisTR) [153] extends DETR [13] for video object instance segmentation (VIS) task. Local features are obtained using a backbone CNN on a collection of video frames. An encoder and a decoder Transformer is used similar to DETR to frame the instance segmentation problem as a sequence to sequence prediction task. The input frame-level features are concatenated to form clip representations and the Transformer outputs instance predictions in a order that is consistent across frames. This integrates the object detection and tracking with in a single uniﬁed architecture. The predicted outputs

18

(a) Spatial Self-Attention (b) Temporal Self-Attention
Fig. 21: Spatial/Temporal Attention for Skeleton Data Representations. Relationships between body-joints and inter-frame dependencies are modeled using two dedicated self-attention modules. Figure is from [164].
are matched with the ground-truth using bipartitie matching. Similar to Mask R-CNN [85], a separate head is used to predict the instance mask based on self-attention and 3D convolutions. The overall results are competitive among the single model approaches on YouTube VIS dataset [162], but performs somewhat lower compared to more complex CNN-based models such as MaskProp [163].
3.8.7 Skeleton-Based Action Recognition
Human action recognition based on skeleton representation requires models that can understand relationships between different joints of a body in a given frame as well as between different frames of a video. Plizzari et al. [164] proposed a two-stream Transformer network to model such relationships. They introduced spatial self-attention (SSA) for relation modeling between different body-joints (Fig. 21a), while temporal self-attention (TSA) to capture long-range inter-frame dependencies (Fig. 21b). They ﬁrst used a small residual network to extract features from skeleton data and then used SSA and TSA modules to process those feature maps. SSA models the relations between different body parts by ﬁnding the correlation between each pair of joints independently, while TSA focuses on how features of a certain joint change between frames along the temporal dimension. Joints can be thought of as bag-of-words and the purpose of SSA is to discover relationships among the surrounding joints in the same way as the Transformer relates different words in a phrase. On the other hand, TSA ﬁnds long-range relations between frames, similarly to how relations among phrases are built in NLP. The two streamed spatial-temporal Transformer network achieve state-of-theart results on NTU-RGB+D 60 [165] and NTU-RGB+D 120 [166] datasets.
3.9 Transformers in Low-shot Learning
In the few-shot learning settings, a support set is provided at the inference to adapt to a novel set of categories. Transformer models have been used to learn set-to-set mappings on this support set [26] or learn the spatial relationships between a given input query and support set images [25]. In terms of absolute performance, the patch-wise spatial self-attention between query and support set images excels compared to an image level association learned in [26]. However, the patch-wise attention computation is computationally expensive. We elaborate on these approaches below.

3.9.1 Cross-Transformer
Doersch et al. [25] explore the utility of self-supervision and Transformer architectures for cases where distribution mismatch exists between training and evaluation phases. They speciﬁcally consider the few-shot ﬁne-grained classiﬁcation problem, where a model is ﬁrst trained on a set of base classes and later during the evaluation, it must adapt to novel classes using their few labeled examples (support set).
Cross-Transformer is evaluated on Meta-dataset [167], which is a huge dataset comprising of 10 distinct datasets (including ImageNet, MS-COCO, etc.). The dataset encapsulates the challenging scenario where a learner must adapt to new classes and novel domains during evaluation. The Transformer architecture in this case is used to relate a given query image with the few-examples available in the support set. To this end, the Transformer ﬁnds spatially similar regions in the query and support set images, and the corresponding features are then used to obtain class decisions for the query. The queries in the Transformer architecture are derived from the grid features obtained using the query image. Similarly, grid features from the support images are used to construct keys and values which are in turn used to derive attended outputs. This approach, besides a contrastive self-supervision based training mechanism, leads to the best performance on the challenging Meta-dataset.
3.9.2 FEAT: Few-Shot Embedding Adaptation
Ye et al. [26] propose to adapt the few-shot embeddings learned on the base classes to the few-shot target classes during inference using a Transformer module. This leads to task-speciﬁc embeddings that perform better on the discriminative tasks such as few-shot classiﬁcation. While many other set-to-set functions are also evaluated, such as Graph convolutional networks [168], Bidirectional LSTMs [29] and DeepSets [169], the best performance is achieved with the Transformer-based mapping. This is attributed to the better contextualization, task interpolation and extrapolation capability of Transformers and their permutation invariance while maintaining a relatively lower parameter complexity. The Transformer architecture used in this work follows the standard approach [1]. The embeddings are adapted using a contrastive loss function for preserving discriminative properties (Fig. 22). The resulting model achieves strong performance on inductive, transductive, and generalized FSL tasks.
3.10 Transformers for Clustering
Clustering is a fundamental operation in unsupervised learning that aims to discover structure in the data by grouping similar data points together. It has numerous applications such as data visualization and interpretation, anomaly detection, and open-set categorization. Neural networks have been developed for set prediction problems [169], [170], however, the setpoints are processed individually which can lose information about inter-point relationships. Recent works employ Transformers that operate on set inputs called the Set Transformers (ST) [171] for amortized clustering. Amortized clustering is a challenging problem that seeks to learn a parametric function that can map an

19

Fig. 22: An overview of Few-shot Embedding Adaptation with Transformer (FEAT [26]). Compared to the conventional instance embedding methods in FSL that keep the embedding function same for all tasks (a), FEAT uses a set-to-set function to adapt the embedding function to each FSL task (b). It evaluates several set-to-set functions and found the Transformer module to be the most suitable choice for FSL. Figure from [26].

three problems in the 3D domain namely, object classiﬁcation, semantic segmentation, and object part segmentation. The main contribution is a point Transformer layer that applies self-attention in the local neighborhood of 3D points.
The proposed point Transformer layer builds on vectorized self-attention network (SAN) [75] where attention weights are represented with vectors. Furthermore, a positional encoding δ is added both to the attention vector and transformed features (value vectors) to represent location information. The point Transformer layer is sandwiched between two linear layers to create a point Transformer block that is stacked multiple times in the developed network architecture. Their design also included transition down/up blocks to reduce/increase the number of points in the input (in a typical encoding-decoding pipeline style). The resulting architecture delivers state-of-the-art performance on the 3D classiﬁcation and segmentation tasks.

input set of points to their corresponding cluster centers. Lee et al. [171] propose to learn such a mapping function using a Transformer architecture comprising of multi-head self-attention blocks [1].
The Transformer model is permutation invariant by design and allows encoding both pair-wise and higher-order relationships between the input points. However, a full Transformer would lead to a high computational cost of O(n2) in each self-attention layer, where n is the number of points in the set. ST reduces this cost to O(mn) by using an Induced Self-Attention Block that uses a low-rank projection (H ∈ Rm) to allow operating on large sets. The model was trained to learn optimal parameters that maximize the likelihood of a mixture of Gaussians (MoGs). Thus MoG parameters are estimated by the ST given a set of data points. Beyond amortized clustering, ST was also evaluated on related set-transformation tasks including counting unique elements in an input set, set anomaly detection, and pointcloud classiﬁcation. More recently, [172] improves [171] by taking a sequential approach to cluster generation, thereby allowing assignment to a variable number of clusters.
3.11 Transformers for 3D Analysis
Given the irregular (variable number of points) and permutation invariant nature of 3D point cloud representations, Transformers provide a nice mechanism to encode rich relationships between the individual data points. To this end [173], [174] are motivated by the capability of Transformers to learn set-functions. Speciﬁcally, [173] introduced a Point Transformer which uses vector attention that learns weights for each channel, while [174] suggest an alternate design where local 3D structure is explicitly encoded. The nonlocal nature of Transformers is exploited in [37] towards an accurate human pose and mesh reconstruction algorithm. We discuss these approaches below.

Fig. 23: Point Transformer layer [173] based on vectorized selfattention [75]. δ denotes a position encoding, ψ, φ, α are pointwise transformations and γ is a mapping function. Figure is from [173].
3.11.2 Point-Cloud Transformer
The Point Cloud Transformer (PCT) [174] is a parallel work to [173] and motivated by the permutation invariance property of Transformers. However, compared to [173], it is more directly based on the conventional Transformer architecture [1] and does not involve vector attention. The key modiﬁcations include a 3D coordinate-based position encoding, an offset attention module, and a neighbor embedding that encodes local 3D structure in point-clouds. Speciﬁcally, the offset attention layer calculates the difference between the self-attended features and the input features using elementwise subtraction. The local neighbor embedding simply ﬁnds self-attention relationships among a group of points instead of individual 3D points. Explicitly incorporating local neighbourhood information makes this a more efﬁcient architecture compared to [173]. The experiments are reported on 3D shape classiﬁcation, normal estimation and segmentation tasks on ModelNet40 [175] and ShapeNet [176] datasets.

3.11.1 Point Transformer
Zhao et al. [173] study the self-attention based Transformer architecture for 3D point cloud processing. Self-attention being a set-operator is ideally suited for processing point clouds, a 3D data representation that demands invariance to number of points and their permutations. The authors study

3.11.3 Pose and Mesh Reconstruction
The Mesh Transformer (METRO) [37] model targets 3D human pose and mesh reconstruction from a single 2D image. A key challenge here is to faithfully learn the non-local interactions between body-joints and mesh vertices (e.g., hand and foot). The expressivity of Transformer network

20

Fig. 24: Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passed through multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from [37].

is used to jointly model vertex to vertex relationships in a mesh as well as the vertex to body-joint relationships. The self-attention mechanism can attend to any combination of vertices in the mesh, thereby encoding non-local relationships.
The multi-layer Transformer architecture sequentially performs dimensionality reduction to map the 2D image to 3D mesh. Position encoding is performed using the 3D coordinates (x,y,z) of each vertex and each body-joint. Similar to masked language modeling in NLP, METRO uses masked vertex modeling (MVM) which randomly masks some percentage of input queries (see Fig. 24). The Transformer is tasked with regressing all the joints and vertices which helps encode inter-dependencies between them. METRO obtains state-of-the-art results on human mesh reconstruction on two publicly available datasets (Human3.6M [177] and 3DPW [178]). Since the approach does not depends on a parametric mesh model, it generalizes well to other reconstruction tasks such as 3D hand reconstruction [179]. Overall, this is the ﬁrst effort to employ Transformers for 3D human reconstruction tasks and leads to fairly good results.
4 OPEN PROBLEMS & FUTURE DIRECTIONS
Despite excellent performance from Transformer models and their interesting salient features (Table 1), there exist several challenges associated with their applicability to practical settings (Table 2). The most important bottlenecks include requirement for large-amounts of training data and associated high computational costs. There have also been some challenges to visualize and interpret Transformer models. In this section, we provide an overview of these challenges, mention some of the recent efforts to address those limitations and highlight the open research questions.
4.1 High Computational Cost
As discussed in Sec. 1, Transformer models have high parametric complexity. This results in high training and inference cost, both in terms of computational time and resources required for processing. As an example, the BERT

[3] basic model (with 109 million parameters) took around 1.89 peta-ﬂop days1 for training, while the latest GPT3 [6] model (175 billion parameters) took around 3640 peta-ﬂop days for training (a staggering ∼1925x increase). This comes with a huge price tag, e.g., according to one estimate [180], GPT3 training might have cost OpenAI around 4.6 million USD. Additionally, these large-scale models require aggressive compression techniques (e.g., distillation) to make their inference feasible for real-world settings.
In the language domain, recent works focus on reducing the high complexity of Transformer models (basically arising from the self-attention mechanism [1] where a token’s representation is updated by considering all tokens from the previous layer). For example, [161], [185] explore selective or sparse attention to previous layer tokens which updating each next layer token. Linformer [33] reduces complexity of standard self-attention operation from O(n2) to O(n) (both in time and memory requirements). The main idea is to show that a low-rank matrix is sufﬁcient to model the selfattention mechanism. The Reformer model [186] employed locally-sensitive hashing (LSH) to minimize the complexity of self-attention from O(n2) to O(nlog(n)). In similar pursuit, the recent Lambda Networks propose to model context as a linear function which helps reduce complexity of selfattention [187].
Vyas et al. [188] developed an efﬁcient cluster attention approach to deal with large input sequences that approximates the original self-attention. They propose a cluster attention approach that groups queries into clusters and then computes attention between cluster centers (instead of attention between all the queries that leads to quadratic complexity). The main idea is that the queries close in the Euclidean space should have similar attention distributions. With a ﬁxed number of clusters, this intuition helps reduce the quadratic complexity to linear complexity of O(nc) with respect to the input sequence length n (where c is the number of clusters). We refer readers to [31] for a nice
1. A peta-ﬂop day is measure of computation and equals to performing 1015 neural net operations per second for one complete day.

Task Image Classiﬁcation
Object Detection
Low Shot Learning
Image Colorization
Action Recognition Super-resolution

Method ViT [11] DeiT [12] CLIP [81] DETR [13]
D-DETR [14] CT [25]

Design Highlights (focus on differences with the standard form)
Directly adopted NLP Transformer Encoder for images, Mechanism to linearly embed image patches with positional embedding suitable for the Encoder.
Transformer as s student while CNN as a teacher, Distillation tokens to produce estimated labels from teacher, Attention between class and distillation tokens.
Jointly train image and text encoders on image-text pairs, to maximize similarity of valid pairs and minimize otherwise
Linear projection layer to reduce CNN feature dimension, Spatial positional embedding added to each multi-head self-attention layer of both encoder and decoder. Object queries (output positional encoding) added to each multihead self-attention layer of decoder.
Deformable Transformer consists of deformable attention layers to introduce sparse priors in Transformers, Multiscale attention module.
Self-supervised pretraining, Queryaligned class prototypes that provide spatial correspondence between the support-set images and query image.

ColTran [24] ST-TR [164] TTSR [16]

Conditional Row/column multi-head attention layers, Progressive multi-scale colorization scheme.
Spatial and Temporal self-attention to operates on graph data such as joints in skeletons.
Texture enhancing Transformer module, Relevance embeddings to compute the relevance between the low-resolution and reference image.

Multi-Model Learning

Oscar [36]

Transformer layer to jointly process triplet representation of image-text [words, tags, features], Masked tokens to represent text data.

3D Classiﬁcation/Segmentation

PT [173]

3D Mesh Reconstruction

METRO [37]

Vision and Language Navigation

Chen et al. [149]

Referring Image Segmentation

CMSA [15]

Video Classiﬁcation

Lee et al. [134]

Point Transformer block, Transition down block to reduce cardinality of the point set, Transition up for dense prediction tasks.
Progressive dimensionality reduction across Transformer layers, Positional Encoding with 3D joint and 3D vertex coordinates, Masked vertex/joint modeling.
Uni-modal encoders on language and map inputs followed by a cross-modal transformer, Trajectory position encodings in the map encoder.
Multimodal feature, Cross-modal selfattention on multiple levels and their fusion using learned gates.
Operates on real-valued audio-visual signals instead of tokens, Contrastive learning for pre-training, End-to-end multimodal transformer learning.

Input Data Type 2D Image
2D Image
2D Images & texts 2D Image
2D Image
2D Image
2D Image Skeleton 2D Image
2D Image
CAD models, 3D object part segmentation 2D Image
Instruction text + RGBD panorama +
Topological Environment Map 2D Image + Language
expression Audio-Visual

Label Type Class labels
Class labels
Image-text pairs
Class labels
Class labels
Pretraining without labels and few-shot learning with
Class labels 2D Image
Action Classes
2D Image
Captions, Class labels, Object
tags
Object and shape
categories 3D Mesh + Human Pose
Navigation Plan
Segmentation mask
Activity labels

21
Loss
Cross-entropy
Cross-entropy, Distillation loss
based on KL-divergence
Symmetric cross-entropy
Hungarian loss based on bipartite matching between
predicted and ground truths Hungarian loss
Normalized Cross-entropy
Negative log-likelihood of the images Cross-entropy
Reconstruction loss, Perceptual loss deﬁned on
pretrained VGG19 features. Negative
log-likelihood of masked tokens, Contrastive binary
cross-entropy Cross-entropy
L1 loss on mesh vertices and joints in 3D
and 2D projection. Cross-entropy over nodes and [stop] action
Binary cross-entropy
loss Contrastive InfoNCE loss and Binary cross-entropy

TABLE 1: A summary of key design choices adopted in different variants of transformers for a representative set of computer vision applications. The main changes relate to speciﬁc loss function choices, architectural modiﬁcations, different position embeddings and variations in input data modalities.

22

Task
Image Classiﬁca-
tion

Method
ViT [11] ICLR’21

DeiT [12] arXiv’20

Low-Shot Learning

CT [25] NeurIPS’20

Object Detection

DETR [13] ECCV’20

D-DETR [14] ICLR’21

Image Coloriza-
tion
Action Recogni-
tion

ColTran [24] ICLR’21
ST-TR [164] arXiv’20

SuperResolution

TTSR [16] CVPR’20

MultiModel Learning

ViLBERT [133] NeurIPS’19

Oscar [36] ECCV’20

3D Analysis

UNITER [35] ECCV’20
Point Transformer [173] arXiv’20

METRO [37] arXiv’20

Metric Dataset Performance

Highlights

Limitations

Top-1 Acc. ImageNet 88.55

a) First application of Transformer (global self-attention) directly on image patches, b) Convolution-free network architecture, c) Outperforms CNN models such as ResNet.

a) Requires training on large-scale data e.g., 300-Million images, b) Requires careful transfer learning to the new task, c) Requires large model with 632-Million parameters to achieve SOTA results.

Top-1 Acc. ImageNet 83.10

Top-1 Acc.

ImageNet COCO

62.25 60.35

a) Successfully trains Transformer on ImageNet only, b) Introduces attention-based distillation method. c) Produces competitive performance with small (86-Million parameters) Transformers.

a) Requires access to pretrained CNN based teacher model thus performance depends on the quality of the teacher model.

a) Self-supervised pre-training mechanism that does not need manual labels, b) Dynamic inference using Transformer achieving stat-of-the-art results.

Proposed algorithm is limited in its capacity to perform on datasets that lack spatial details such as texture.

AP

COCO

44.9

a) Use of Transformer allows end- a) Performs poorly on small objects,

to-end training pipeline for object b) Requires long training time to

detection, b) Removes the need for converge.

hand-crafted post-processing steps.

AP

COCO

43.8

a) Achieves better performance on Obtain SOTA results with 52.3 AP

small objects than DETR [13], b) but with two stage detector design

Faster convergence than DETR [13] and test time augmentations.

FID

ImageNet 19.71

a) First successful application of a) Lacks end-to-end training, b)

Transformer to image colorization, limited to images of size 256×256.

b) Achieves SOTA FID score.

Top-1 Acc. NTU 60/120

94.0/84.7

a) Successfully applies Transformer to model relations between body joints both in spatial and temporal domain, b) Achieves SOTA results.

PSNR/ SSIM

CUFED5 Sun80 Urban100 Manga109

27.1 / 0.8 30.0 / 0.81 25.9 / 0.78 30.1 / 0.91

a) Achieves state-of-the-art superresolution by using attention, b) Novel Transformer inspired architectures that can process multi-scale features.

Acc./ mAP (R@1)

VQA [135]/ Retrieval [181]

70.6/ 58.2

a) Proposed Transformer architecture can combine text and visual information to understand intertask dependencies, b) Achieves pretraining on unlabelled dataset.

Acc./ mAP (R@1)

VQA [182]/ COCO

80.37/57.5

a) Exploit novel supervisory signal via object tags to achieve text and image alignment, b) Achieves stateof-the-art results.

Acc./

VQA [135]/

Learns ﬁne-grained relation align-

Avg.

Flickr30K 72.47/83.72 ment between text and images

(R@1/5/10) [183]

Top-1 Acc. ModelNet40 92.8

IoU

[175]

85.9

MPJPE

77.1

PA-MPJPE 3DPW

47.9

MPVE

[178]

88.2

a) Transformer based attention capable to process unordered and unstructured point sets, b) Permutation invariant architecture.
a) Does not depend on parametric mesh models so easily extendable to different objects, b) Achieves SOTA results using Transformers.

Proposed Transformers do not process joints directly rather operate on features extracted by a CNN, thus the overall model is based on handcrafted design.
a) Proposed Transformer does not process images directly but features extracted by a convolution based network, b) Model with large number of trainable parameters, and c) Compute intensive.
a) Requires large amount of data for pre-training, b) Requires ﬁne tuning to the new task.
Requires extra supervision through pre-trained object detectors thus performance is dependent on the quality of object detectors.
Requires large multi-task datasets for Transformer training which lead to high computational cost.
a) Only moderate improvements over previous SOTA, b) Large number of trainable parameters around 6× higher than PointNet++ [184].
Dependent on hand-crafted network design.

TABLE 2: A summary of advantages and limitations of different Transformers based methods in different Tasks. (CT: Cross Transformers, AP: Average Precision, mAP: mean AP, IoU: Intersection over Union, FID: Fre´chet inception distance, MPJPE: Mean Per Joint Position Error, MPVE: Mean Per Vertex Error).

23

literature survey on efﬁcient Transformers in NLP. Similar to the NLP domain, computer vision models
also suffer from the high computational cost of Transformer models. For example, image generators that are based on sequence-based Transformers (e.g., iGPT) have a high compute cost limiting their applicability to highresolution inputs. In future, it is interesting to explore how such models can be extended to high-dimensional cases e.g., using a multi-scale transformer design with a somewhat local context modeling. By inducing inductive biases based on our understanding of the visual learning tasks (e.g., spatial relationships in the local neighbourhood), the high computational cost can be reduced. Similarly, using sparse attention maps modeled with low-rank factorization in the matrices can also help towards reducing the computational cost [160].
4.2 Large Data Requirements
Since Transformer architectures do not inherently encode inductive biases (prior knowledge) to deal with visual data, they typically require large amounts of training data during pre-training to ﬁgure out the underlying modalityspeciﬁc rules. For example, a CNN has inbuilt translation invariance, weight sharing, and partial scale invariance due to pooling operations or multi-scale processing blocks. However, a Transformer network needs to ﬁgure out these image-speciﬁc properties on its own by looking at a large number of examples. Similarly, relationships between video frames need to be discovered automatically by the selfattention mechanism by looking at a large database of video sequences. This results in longer training times, a signiﬁcant increase in computational requirements, and large datasets for processing. For example, the ViT [11] model requires hundreds of millions of image examples to obtain a decent performance on the ImageNet benchmark dataset. The question of learning a Transformer in a data-efﬁcient manner is an open research problem and recent works report encouraging steps towards its resolution. For example, DeiT [12] uses a distillation approach to achieve data efﬁciency while T2T (Tokens-to-Token) ViT [189] models local structure by combining spatially close tokens together, thus leading to competitive performance when trained only on ImageNet from scratch (without pre-training).

a parallel branch in the Transformers to improve person re-identiﬁcation. A recent work [189] rearranges the spatially close tokens to better model relationships in spatially proximal locations. One may argue that the architectures like Transformer models should remain generic to be directly applicable across domains, we notice that the high computational and time cost for pre-training such models demands novel design strategies to make their training more affordable on vision problems.
4.4 Interpretability of Transformers
Given the strong performance of Transformer architectures, it is interesting and critical to interpret their decisions, e.g., by visualizing relevant regions in an image for a given classiﬁcation decision. The main challenge is that the attention originating in each layer, gets inter-mixed in the subsequent layers in a complex manner, making it difﬁcult to visualize the relative contribution of input tokens towards ﬁnal predictions. This is an open problem, however, some recent works [191]–[193] target enhanced interpretability of Transformers and report encouraging results. Attention rollout and attention ﬂow methods were proposed in [192] to estimate the accurate attentions. However, this method functions in an ad-hoc manner and makes simplistic assumptions e.g., input tokens are linearly combined using attention weights across the layers. Chefer et al. [193] note that the attention scores obtained directly via the selfattention process (encoding relationships between tokens) or reassignments in [192] do not provide an optimal solution. As an alternative, they propose to assign and propagate relevancy scores in the Transformer network such that the sum of relevancy is constant throughout the network. Their design can handle both the positive and negative attributions experienced in the self-attention layer. The proposed framework has an added advantage of being able to provide class-speciﬁc visualizations. Despite these seminal works, visualizing and interpreting Transformers is an unsolved problem and methods are needed to obtain spatially precise activation-speciﬁc visualizations. Further progress in this direction can help in better understanding the Transformer models, diagnosing any erroneous behaviors and biases in the decision process. It can also help us design novel architectures that can help us avoid any biases.

4.3 Vision Tailored Transformer Designs
We note that most of the existing works focused on vision tasks tend to directly apply Transformer models on computer vision problems. These include architectures designed for image recognition [11], video understanding [17] and especially multi-modal processing [133]. Although the initial results from these simple applications are quite encouraging and motivate us to look further into the strengths of selfattention and self-supervised learning, current architectures may still remain better tailored for language problems (with a sequence structure) and need further intuitions to make them more efﬁcient for visual inputs. For example, vector attention from [75] is a nice work in this direction which attempts to speciﬁcally tailor self-attention operation for visual inputs via learning channel-wise attentions. Similarly, [190] uses a Jigsaw puzzle based self-supervision loss as

4.5 Hardware Efﬁcient Designs
Large-scale Transformer networks can have intensive power and computation requirements, hindering their deployment on edge devices and resource-constrained environments such as internet-of-things (IoT) platforms. Some recent efforts have been reported to compress and accelerate NLP models on embedded systems such as FPGAs [194]. Li et al. [194] used an enhanced block-circulant matrix-based representation to compress NLP models and proposed a new Field Programmable Gate Array (FPGA) architecture design to efﬁciently manage resources for high throughput and low latency. They could achieve 27x, 3x and 81x improvements in performance (throughput measured in FPS), reduced power consumption, and energy efﬁciency relative a CPU for RoBERTa model [7]. Towards this goal, [195] proposed

24

to design Hardware-Aware Transformers (HAT) using neural architecture search strategies [196]–[198]. Speciﬁcally, a SuperTransformer model is ﬁrst trained for performance approximation which can estimate a model’s performance without fully training it. This model comprises the largest possible model in the search space while sharing weights between common parts. Eventually, an evolutionary search is performed considering the hardware latency constraints to ﬁnd a suitable SubTransformer model for a target hardware platform (e.g., IoT device, GPU, CPU). However, such hardware efﬁcient designs are currently lacking for the vision Transformers to enable their seamless deployment in resource-constrained devices. Further, the search cost of the evolutionary algorithms remains signiﬁcant with the associated impact of CO2 emissions on the environment.
4.6 Leveraging Rich Multi-modal Annotations
In cases, where training data is available with dense labels in multiple domains (e.g., language and vision [17], [199]), an interesting question to consider is whether the pre-training process leveraging rich labels on a small dataset speedup its learning. This question has been explored in Virtex [200], a model that seeks to learn strong visual representations using dense textual annotations (e.g., image captions). Since, the captions encode information about objects present in an image, their relationships, actions and attributes, they can provide better supervision to learn more generalizable and transferable representations. Particularly, they showed that a model trained with a visual backbone followed by a bidirectional language model (forward and backward Transformers) [3] to predict captions, can learn strong features on MS-COCO dataset in an unsupervised manner. When these features are transferred to the ImageNet model, they perform better or equally-well compared to the unsupervised/supervised features learned directly on the ImageNet dataset. Since, Transformer models can process multiple modalities in a uniﬁed architecture, it will be interesting to explore how densely annotated datasets can reduce the data requirement of Transformers and if dense-annotations allow transferring well to novel unseen conditions in one particular modality at inference.
5 CONCLUSION
Attention has played a key role in delivering efﬁcient and accurate computer vision systems, while simultaneously providing insights into the function of deep neural networks. This survey reviews the self-attention approaches and speciﬁcally focuses on the Transformer and bidirectional encoding architectures that are built on the principle of self-attention. We ﬁrst cover fundamental concepts pertaining to self-attention architectures and later provide an in-depth analysis of competing approaches for a broad range of computer vision applications. Speciﬁcally, we include state of the art self-attention models for image recognition, object detection, semantic and instance segmentation, video analysis and classiﬁcation, visual question answering, visual commonsense reasoning, image captioning, visionlanguage navigation, clustering, few-shot learning, and 3D data analysis. We systematically highlight the key strengths

and limitations of the existing methods and particularly elaborate on the important future research directions. With its speciﬁc focus on computer vision tasks, this survey provides a unique view of the recent progress in self-attention and Transformer-based methods. We hope this effort will drive further interest in the vision community to leverage the potential of Transformer models and improve on their current limitations e.g., reducing their carbon footprint.
ACKNOWLEDGMENTS
The authors would like to thank Tim Prangemeier (TU Darmstadt), Lu-
owei Zhou (Microsoft Research), Jason Corso (University of Michigan),
Pichao Wang (Alibaba Group), Yuqing Wang (Meituan) and Manoj
Kumar (Google Brain) for their helpful feedback on the survey. We
would also like to thank Mohamed Afham for his help with a ﬁgure.
REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017. 1, 2, 3, 4, 6, 8, 10, 12, 13, 16, 17, 18, 19, 20
[2] M. Ott, S. Edunov, D. Grangier, and M. Auli, “Scaling neural machine translation,” in WMT, 2018. 1
[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pretraining of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018. 1, 2, 3, 4, 13, 15, 17, 20, 24
[4] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” tech. rep., OpenAI, 2018. 1, 4
[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” tech. rep., OpenAI, 2019. 1, 8, 10
[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020. 1, 20
[7] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019. 1, 2, 23
[8] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer,” arXiv preprint arXiv:1910.10683, 2019. 1
[9] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv preprint arXiv:2006.16668, 2020. 1
[10] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efﬁcient sparsity,” arXiv preprint arXiv:2101.03961. 1, 3
[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” arXiv preprint arXiv:2010.11929, 2020. 1, 3, 5, 6, 7, 17, 21, 22, 23
[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Je´gou, “Training data-efﬁcient image transformers & distillation through attention,” arXiv preprint arXiv:2012.12877, 2020. 1, 5, 7, 17, 21, 22, 23
[13] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” arXiv preprint arXiv:2005.12872, 2020. 1, 3, 8, 17, 21, 22
[14] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR: Deformable transformers for end-to-end object detection,” arXiv preprint arXiv:2010.04159, 2020. 1, 8, 21, 22
[15] L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal selfattention network for referring image segmentation,” in CVPR, 2019. 1, 9, 21

25

[16] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture transformer network for image super-resolution,” in CVPR, 2020. 1, 11, 12, 21, 22
[17] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid, “VideoBERT: A joint model for video and language representation learning,” in ICCV, 2019. 1, 13, 14, 15, 16, 17, 23, 24
[18] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video action transformer network,” in CVPR, 2019. 1, 3, 17
[19] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao, “Pre-trained image processing transformer,” arXiv preprint arXiv:2012.00364, 2020. 1, 7, 11, 12
[20] A. Ramesh, M. Pavlov, G. Goh, and S. Gray, “DALL·E: Creating images from text,” tech. rep., OpenAI, 2021. 1, 3, 11
[21] H. Tan and M. Bansal, “LXMERT: Learning cross-modality encoder representations from transformers,” in EMNLP-IJCNLP, 2019. 1, 13, 14, 15, 16, 17
[22] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “VL-BERT: Pre-training of generic visual-linguistic representations,” arXiv preprint arXiv:1908.08530, 2019. 1, 3, 4, 14, 15, 17
[23] X. Wang, C. Yeshwanth, and M. Nießner, “SceneFormer: Indoor scene generation with transformers,” arXiv preprint arXiv:2012.09793, 2020. 1, 9, 11
[24] M. Kumar, D. Weissenborn, and N. Kalchbrenner, “Colorization transformer,” in ICLR, 2021. 1, 11, 13, 21, 22
[25] C. Doersch, A. Gupta, and A. Zisserman, “CrossTransformers: spatially-aware few-shot transfer,” NeurIPS, 2020. 1, 18, 21, 22
[26] H.-J. Ye, H. Hu, D.-C. Zhan, and F. Sha, “Few-shot learning via embedding adaptation with set-to-set functions,” in CVPR, 2020. 1, 18, 19
[27] Y. Bengio, I. Goodfellow, and A. Courville, Deep learning. MIT press, 2017. 1
[28] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, 2015. 1
[29] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, 1997. 1, 16, 18
[30] D. Hu, “An introductory survey on attention mechanisms in nlp problems,” in IntelliSys, 2019. 2
[31] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient transformers: A survey,” arXiv preprint arXiv:2009.06732, 2020. 2, 20
[32] S. Chaudhari, G. Polatkan, R. Ramanath, and V. Mithal, “An attentive survey of attention models,” arXiv preprint arXiv:1904.02874, 2019. 2
[33] S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Selfattention with linear complexity,” arXiv preprint arXiv:2006.04768, 2020. 2, 20
[34] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Selfattention generative adversarial networks,” in International conference on machine learning, pp. 7354–7363, PMLR, 2019. 2
[35] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu, “UNITER: Universal image-text representation learning,” in ECCV, 2020. 3, 4, 14, 15, 22
[36] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al., “Oscar: Object-semantics aligned pretraining for vision-language tasks,” in ECCV, 2020. 3, 14, 15, 21, 22
[37] K. Lin, L. Wang, and Z. Liu, “End-to-end human pose and mesh reconstruction with transformers,” arXiv preprint arXiv:2012.09760, 2020. 3, 19, 20, 21, 22
[38] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation learning by predicting image rotations,” arXiv preprint arXiv:1803.07728, 2018. 3, 4
[39] “Revisiting the unreasonable effectiveness of data.” https://ai. googleblog.com/2017/07/revisiting-unreasonable-effectiveness. html. Accessed: 2020-12-31. 3, 7
[40] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep neural networks: A survey,” TPAMI, 2020. 3
[41] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang, “Self-supervised learning: Generative or contrastive,” arXiv preprint arXiv:2006.08218, 2020. 3
[42] “Aaai 2020 keynotes turing award winners event.” https://www. youtube.com/watch?v=UX8OubxsY8w. Accessed: 2020-12-31. 3
[43] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in ECCV, 2016. 4
[44] C. Ledig, L. Theis, F. Husza´r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al., “Photorealistic single image super-resolution using a generative adversarial network,” in CVPR, 2017. 4, 12

[45] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context encoders: Feature learning by inpainting,” in CVPR, 2016. 4
[46] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in NeurIPS, 2014. 4, 10, 11, 12
[47] D. Lin, K. Fu, Y. Wang, G. Xu, and X. Sun, “MARTA GANs: Unsupervised representation learning for remote sensing image classiﬁcation,” GRSL, 2017. 4
[48] U. Ahsan, R. Madhok, and I. Essa, “Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition,” in WACV, 2019. 4
[49] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations by solving jigsaw puzzles,” in ECCV, 2016. 4
[50] D. Kim, D. Cho, D. Yoo, and I. S. Kweon, “Learning image representations by completing damaged jigsaw puzzles,” WACV, 2018. 4
[51] L. Jing, X. Yang, J. Liu, and Y. Tian, “Self-supervised spatiotemporal feature learning via video rotation prediction,” arXiv preprint arXiv:1811.11387, 2018. 4
[52] H.-Y. Lee, J.-B. Huang, M. Singh, and M.-H. Yang, “Unsupervised representation learning by sorting sequences,” in ICCV, 2017. 4
[53] I. Misra, C. L. Zitnick, and M. Hebert, “Shufﬂe and learn: unsupervised learning using temporal order veriﬁcation,” in ECCV, 2016. 4
[54] D. Wei, J. J. Lim, A. Zisserman, and W. T. Freeman, “Learning and using the arrow of time,” in CVPR, 2018. 4
[55] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, “VisualBERT: A simple and performant baseline for vision and language,” in Arxiv preprint arXiv:1908.03557, 2019. 4, 14, 15
[56] B. Korbar, D. Tran, and L. Torresani, “Cooperative learning of audio and video models from self-supervised synchronization,” in NeurIPS, 2018. 4
[57] R. Arandjelovic and A. Zisserman, “Look, listen and learn,” in ICCV, 2017. 4, 17
[58] N. Sayed, B. Brattoli, and B. Ommer, “Cross and learn: Crossmodal self-supervision,” in GCPR, 2018. 4
[59] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016. 4, 8, 17
[60] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint arXiv:1607.06450, 2016. 4
[61] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A large-scale hierarchical image database,” in CVPR, 2009. 5, 9
[62] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image recognition,” in ICCV, 2019. 5, 6
[63] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “CCNet: Criss-cross attention for semantic segmentation,” in ICCV, 2019. 5, 6
[64] N. Parmar, P. Ramachandran, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens, “Stand-alone self-attention in vision models,” in NeurIPS, 2019. 5, 6, 9
[65] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image denoising,” in CVPR, 2005. 5, 6
[66] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,” in CVPR, 2018. 5, 6
[67] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, et al., “The kinetics human action video dataset,” arXiv preprint arXiv:1705.06950, 2017. 5, 16, 17
[68] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in CVPR, 2016. 5, 9
[69] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Scene parsing through ade20k dataset,” in CVPR, 2017. 5
[70] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla´r, and C. L. Zitnick, “Microsoft COCO: Common objects in context,” in ECCV, 2014. 5, 9, 15
[71] X. Liang, K. Gong, X. Shen, and L. Lin, “Look into person: Joint body parsing & pose estimation network and a new benchmark,” TPAMI, 2018. 5
[72] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic object classes in video: A high-deﬁnition ground truth database,” Pattern Recognition Letters, 2009. 5
[73] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention augmented convolutional networks,” in ICCV, 2019. 6

26

[74] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” arXiv preprint arXiv:1803.02155, 2018. 6
[75] H. Zhao, J. Jia, and V. Koltun, “Exploring self-attention for image recognition,” in CVPR, 2020. 6, 7, 19, 23
[76] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties of neural networks,” arXiv preprint arXiv:1312.6199, 2013. 7
[77] M. M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, and F. Porikli, “Cross-domain transferability of adversarial perturbations,” in NeurIPS, 2019. 7
[78] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015. 7
[79] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dolla´r, “Designing network design spaces,” in CVPR, 2020. 7
[80] M. Tan and Q. V. Le, “EfﬁcientNet: Rethinking model scaling for convolutional neural networks,” arXiv preprint arXiv:1905.11946, 2019. 7
[81] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable visual models from natural language supervision,” Image, vol. 2, p. T2, 2021. 8, 21
[82] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” 2020. 8, 11
[83] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal networks,” TPAMI, 2016. 8, 17
[84] R. Girshick, “Fast R-CNN,” in ICCV, 2015. 8 [85] K. He, G. Gkioxari, P. Dolla´r, and R. Girshick, “Mask R-CNN,” in
ICCV, 2017. 8, 18 [86] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only
look once: Uniﬁed, real-time object detection,” in CVPR, 2016. 8 [87] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg, “SSD: Single shot multibox detector,” in ECCV, 2016. 8 [88] T. Prangemeier, C. Reich, and H. Koeppl, “Attention-based transformers for instance segmentation of cells in microstructures,” in 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 700–707, IEEE, 2020. 8 [89] T.-Y. Lin, P. Dolla´r, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in CVPR, 2017. 8 [90] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable convolutional networks,” in ICCV, 2017. 8 [91] H. Wang, Y. Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen, “Axial-DeepLab: Stand-alone axial-attention for panoptic segmentation,” arXiv preprint arXiv:2003.07853, 2020. 9 [92] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dolla´r, “Panoptic segmentation,” in CVPR, 2019. 9 [93] G. Neuhold, T. Ollmann, S. Rota Bulo, and P. Kontschieder, “The mapillary vistas dataset for semantic understanding of street scenes,” in ICCV, 2017. 9 [94] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling context in referring expressions,” in ECCV, 2016. 9 [95] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, “Generation and comprehension of unambiguous object descriptions,” in CVPR, 2016. 9 [96] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring to objects in photographs of natural scenes,” in EMNLP, 2014. 9 [97] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer, A. Ku, and D. Tran, “Image transformer,” in ICML, 2018. 9, 10, 11 [98] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever, “Generative pretraining from pixels,” in ICML, 2020. 9, 10, 11 [99] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution image synthesis,” arXiv preprint arXiv:2012.09841, 2020. 9, 10, 11 [100] Y. Jiang, S. Chang, and Z. Wang, “Transgan: Two transformers can make one strong gan,” 2021. 9, 10 [101] A. Van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves, et al., “Conditional image generation with pixelcnn decoders,” in NeurIPS, 2016. 9 [102] A. Krizhevsky, “Learning multiple layers of features from tiny images,” tech. rep., 2009. 10

[103] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks in unsupervised feature learning,” in AISTATS, 2011. 10
[104] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for contrastive learning of visual representations,” arXiv preprint arXiv:2002.05709, 2020. 10
[105] P. Bachman, R. D. Hjelm, and W. Buchwalter, “Learning representations by maximizing mutual information across views,” in NeurIPS, 2019. 10
[106] O. J. He´naff, A. Srinivas, J. De Fauw, A. Razavi, C. Doersch, S. Eslami, and A. v. d. Oord, “Data-efﬁcient image recognition with contrastive predictive coding,” arXiv preprint arXiv:1905.09272, 2019. 10
[107] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual representation learning,” in CVPR, 2020. 10
[108] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” arXiv preprint arXiv:1906.05849, 2019. 10
[109] S. Khan, H. Rahmani, S. A. A. Shah, and M. Bennamoun, “A guide to convolutional neural networks for computer vision,” Synthesis Lectures on Computer Vision, 2018. 10
[110] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” arXiv preprint arXiv:1511.06434, 2015. 10
[111] C. Gao, Y. Chen, S. Liu, Z. Tan, and S. Yan, “Adversarialnas: Adversarial neural architecture search for gans,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5680–5689, 2020. 10
[112] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, “Analyzing and improving the image quality of stylegan,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8110–8119, 2020.
[113] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, “Generative adversarial text to image synthesis,” in ICML, 2016. 11
[114] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas, “StackGAN: Text to photo-realistic image synthesis with stacked generative adversarial networks,” in ICCV, 2017. 11
[115] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas, “StackGAN++: Realistic image synthesis with stacked generative adversarial networks,” TPAMI, 2018. 11
[116] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He, “AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks,” in CVPR, 2018. 11
[117] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. 11
[118] A. Razavi, A. van den Oord, and O. Vinyals, “Generating diverse high-ﬁdelity images with vq-vae-2,” in NeurISP, 2019. 11
[119] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep residual networks for single image super-resolution,” in CVPRW, 2017. 11
[120] Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive residual network,” in CVPR, 2017. 11
[121] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang, “Image super-resolution via dual-state recurrent networks,” in CVPR, 2018. 11
[122] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-resolution using very deep residual channel attention networks,” in ECCV, 2018. 11, 13
[123] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense network for image restoration,” TPAMI, 2020. 11
[124] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. Change Loy, “ESRGAN: enhanced super-resolution generative adversarial networks,” in ECCVW, 2018. 12
[125] S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “SRFEAT: Single image super-resolution with feature discrimination,” in ECCV, 2018. 12
[126] M. S. Sajjadi, B. Scholkopf, and M. Hirsch, “EnhanceNet: Single image super-resolution through automated texture synthesis,” in ICCV, 2017. 12
[127] C. Ledig, L. Theis, F. Husza´r, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al., “Photorealistic single image super-resolution using a generative adversarial network,” in CVPR, 2017. 12
[128] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for realtime style transfer and super-resolution,” in ECCV, 2016. 12

27

[129] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order attention network for single image super-resolution,” in CVPR, 2019. 13
[130] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao, and H. Shen, “Single image super-resolution via a holistic attention network,” in ECCV, 2020. 13
[131] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, “Axial attention in multidimensional transformers,” arXiv preprint arXiv:1912.12180, 2019. 13
[132] G. Li, N. Duan, Y. Fang, M. Gong, D. Jiang, and M. Zhou, “Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training.,” in AAAI, 2020. 14, 15
[133] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks,” in NeurIPS, 2019. 13, 14, 15, 16, 17, 22, 23
[134] S. Lee, Y. Yu, G. Kim, T. Breuel, J. Kautz, and Y. Song, “Parameter efﬁcient multimodal transformers for video representation learning,” arXiv preprint arXiv:2012.04124, 2020. 14, 16, 21
[135] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh, “VQA: Visual question answering,” in ICCV, 2015. 13, 22
[136] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, “From recognition to cognition: Visual commonsense reasoning,” in CVPR, 2019. 13
[137] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for image-text matching,” in ECCV, 2018. 13
[138] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in CVPR, 2015. 13
[139] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi, “A corpus for reasoning about natural language grounded in photographs,” arXiv preprint arXiv:1811.00491, 2018. 14
[140] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning,” in ACL, 2018. 14, 15
[141] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao, “Uniﬁed vision-language pre-training for image captioning and vqa,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34, pp. 13041–13049, 2020. 15
[142] C. Sun, F. Baradel, K. Murphy, and C. Schmid, “Learning video representations using contrastive bidirectional transformer,” arXiv preprint arXiv:1906.05743, 2019. 15
[143] C. Alberti, J. Ling, M. Collins, and D. Reitter, “Fusion of detected objects in text for visual question answering,” arXiv preprint arXiv:1908.05054, 2019. 15
[144] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al., “Visual genome: Connecting language and vision using crowdsourced dense image annotations,” IJCV, 2017. 15
[145] V. Ordonez, G. Kulkarni, and T. L. Berg, “Im2text: Describing images using 1 million captioned photographs,” in NeurIPS, 2011. 15
[146] H. Tan and M. Bansal, “Vokenization: Improving language understanding with contextualized, visual-grounded supervision,” arXiv preprint arXiv:2010.06775, 2020. 15, 16
[147] W. Hao, C. Li, X. Li, L. Carin, and J. Gao, “Towards learning a generic agent for vision-and-language navigation via pretraining,” in CVPR, 2020. 16
[148] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra, “Improving vision-and-language navigation with image-text pairs from the web,” arXiv preprint arXiv:2004.14973, 2020. 16
[149] K. Chen, J. K. Chen, J. Chuang, M. Va´zquez, and S. Savarese, “Topological planning with transformers for vision-andlanguage navigation,” arXiv preprint arXiv:2012.05292, 2020. 16, 21
[150] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on the kinetics-700 human action dataset,” arXiv preprint arXiv:1907.06987, 2019. 16, 17
[151] S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, “COOT: Cooperative hierarchical transformer for video-text representation learning,” arXiv preprint arXiv:2011.00597, 2020. 16
[152] H. Seong, J. Hyun, and E. Kim, “Video multitask transformer network,” in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0–0, 2019. 16
[153] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia, “End-to-end video instance segmentation with transformers,” arXiv preprint arXiv:2011.14503, 2020. 16, 17

[154] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, “End-to-end dense video captioning with masked transformer,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8739–8748, 2018. 16
[155] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, “Dense-captioning events in videos,” in Proceedings of the IEEE international conference on computer vision, pp. 706–715, 2017. 16
[156] L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of procedures from web instructional videos,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 32, 2018. 16
[157] K. Soomro, A. R. Zamir, and M. Shah, “UCF101: A dataset of 101 human actions classes from videos in the wild,” arXiv preprint arXiv:1212.0402, 2012. 17
[158] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in ICASSP, 2017. 17
[159] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta, “Hollywood in homes: Crowdsourcing data collection for activity understanding,” in ECCV, 2016. 17
[160] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video transformer network,” arXiv preprint arXiv:2102.00719, 2021. 17, 23
[161] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The longdocument transformer,” arXiv preprint arXiv:2004.05150, 2020. 17, 20
[162] L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5188–5197, 2019. 18
[163] G. Bertasius and L. Torresani, “Classifying, segmenting, and tracking object instances in video with mask propagation,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9739–9748, 2020. 18
[164] C. Plizzari, M. Cannici, and M. Matteucci, “Spatial temporal transformer network for skeleton-based action recognition,” arXiv preprint arXiv:2008.07404, 2020. 18, 21, 22
[165] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, “NTU RGB+D: A large scale dataset for 3d human activity analysis,” in CVPR, 2016. 18
[166] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C. Kot, “NTU RGB+D 120: A large-scale benchmark for 3d human activity understanding,” TPAMI, 2019. 18
[167] E. Triantaﬁllou, T. Zhu, V. Dumoulin, P. Lamblin, U. Evci, K. Xu, R. Goroshin, C. Gelada, K. Swersky, P.-A. Manzagol, et al., “Metadataset: A dataset of datasets for learning to learn from few examples,” arXiv preprint arXiv:1903.03096, 2019. 18
[168] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph convolutional networks,” arXiv preprint arXiv:1609.02907, 2016. 18
[169] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola, “Deep sets,” in NeurIPS, 2017. 18
[170] H. Edwards and A. Storkey, “Towards a neural statistician,” arXiv preprint arXiv:1606.02185, 2016. 18
[171] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh, “Set transformer: A framework for attention-based permutationinvariant neural networks,” in ICML, 2019. 18, 19
[172] J. Lee, Y. Lee, and Y. W. Teh, “Deep amortized clustering,” arXiv preprint arXiv:1909.13433, 2019. 19
[173] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun, “Point transformer,” arXiv preprint arXiv:2012.09164, 2020. 19, 21, 22
[174] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, “PCT: Point cloud transformer,” arXiv preprint arXiv:2012.09688, 2020. 19
[175] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, “3D ShapeNets: A deep representation for volumetric shapes,” in CVPR, 2015. 19, 22
[176] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu, “ShapeNet: An information-rich 3d model repository,” arXiv preprint arXiv:1512.03012, 2015. 19
[177] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments,” TPAMI, 2013. 20
[178] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll, “Recovering accurate 3d human pose in the wild using imus and a moving camera,” in ECCV, 2018. 20, 22

28
[179] C. Zimmermann, D. Ceylan, J. Yang, B. Russell, M. Argus, and T. Brox, “FreiHAND: A dataset for markerless capture of hand pose and shape from single rgb images,” in ICCV, 2019. 20
[180] “OpenAI’s GPT-3 language model: A technical overview.” https: //lambdalabs.com/blog/demystifying-gpt-3/. Accessed: 202012-31. 20
[181] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,” TACL, 2014. 22
[182] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in vqa matter: Elevating the role of image understanding in visual question answering,” in CVPR, 2017. 22
[183] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, “Flickr30k entities: Collecting regionto-phrase correspondences for richer image-to-sentence models,” in ICCV, 2015. 22
[184] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “PointNet++: Deep hierarchical feature learning on point sets in a metric space,” NeurIPS, 2017. 22
[185] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences with sparse transformers,” arXiv preprint arXiv:1904.10509, 2019. 20
[186] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efﬁcient transformer,” in ICLR, 2020. 20
[187] I. Bello, “Lambdanetworks: Modeling long-range interactions without attention,” in International Conference on Learning Representations, 2021. 20
[188] A. Vyas, A. Katharopoulos, and F. Fleuret, “Fast transformers with clustered attention,” NeurIPS, 2020. 20
[189] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and S. Yan, “Tokens-to-token vit: Training vision transformers from scratch on imagenet,” arXiv preprint arXiv:2101.11986, 2021. 23
[190] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang, “Transreid: Transformer-based object re-identiﬁcation,” arXiv preprint arXiv:2102.04378, 2021. 23
[191] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, “Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned,” arXiv preprint arXiv:1905.09418, 2019. 23
[192] S. Abnar and W. Zuidema, “Quantifying attention ﬂow in transformers,” arXiv preprint arXiv:2005.00928, 2020. 23
[193] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond attention visualization,” arXiv preprint arXiv:2012.09838, 2020. 23
[194] B. Li, S. Pandey, H. Fang, Y. Lyv, J. Li, J. Chen, M. Xie, L. Wan, H. Liu, and C. Ding, “FTRANS: energy-efﬁcient acceleration of transformers using fpga,” in ISLPED, 2020. 23
[195] H. Wang, Z. Wu, Z. Liu, H. Cai, L. Zhu, C. Gan, and S. Han, “HAT: Hardware-aware transformers for efﬁcient natural language processing,” arXiv preprint arXiv:2005.14187, 2020. 23
[196] G. Bender, P.-J. Kindermans, B. Zoph, V. Vasudevan, and Q. Le, “Understanding and simplifying one-shot architecture search,” in ICML, 2018. 24
[197] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y. Wei, and J. Sun, “Single path one-shot neural architecture search with uniform sampling,” arXiv preprint arXiv:1904.00420, 2019. 24
[198] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efﬁcient neural architecture search via parameter sharing,” arXiv preprint arXiv:1802.03268, 2018. 24
[199] L. Zhou, Y. Kalantidis, X. Chen, J. J. Corso, and M. Rohrbach, “Grounded video description,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6578– 6587, 2019. 24
[200] K. Desai and J. Johnson, “VirTex: Learning visual representations from textual annotations,” arXiv preprint arXiv:2006.06666, 2020. 24

